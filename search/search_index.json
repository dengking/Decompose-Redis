{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u5de5\u7a0b Redis is simple but powerful, it can be a great learning material. I decompose it into modules and conquer each. It can be divide into modules as below \u5173\u4e8eredis redis Redis cn \u6587\u6863 Redis data structure Redis network module Redis event library Redis memory management","title":"Home"},{"location":"#_1","text":"Redis is simple but powerful, it can be a great learning material. I decompose it into modules and conquer each. It can be divide into modules as below","title":"\u5173\u4e8e\u672c\u5de5\u7a0b"},{"location":"#redis","text":"redis Redis cn \u6587\u6863","title":"\u5173\u4e8eredis"},{"location":"#redis#data#structure","text":"","title":"Redis data structure"},{"location":"#redis#network#module","text":"","title":"Redis network module"},{"location":"#redis#event#library","text":"","title":"Redis event library"},{"location":"#redis#memory#management","text":"","title":"Redis memory management"},{"location":"TODO/","text":"TODO Cache replacement policies \u5728\u9605\u8bfbredis\u7684\u6e90\u4ee3\u7801\u7684\u65f6\u5019\uff0c \u53d1\u73b0\u4e86\u5982\u4e0b\u7684\u5b9a\u4e49\uff1a typedef struct redisObject { unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; /* lru time (relative to server.lruclock) */ int refcount; void *ptr; } robj; \u5176\u4e2d\u7684LRU\u5f15\u8d77\u4e86\u6211\u7684\u6ce8\u610f\uff0c\u9042Google\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u4e86\u5982\u4e0b\u5185\u5bb9\uff1a - Cache replacement policies Redis Persistence https://redis.io/topics/persistence Redis Sentinel Documentation clients service discovery https://en.wikipedia.org/wiki/Service_discovery https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/ https://stackoverflow.com/questions/37148836/what-is-service-discovery-and-why-do-you-need-it redis pub/sub How to use Pub/sub with hiredis in C++? Add code example for pub/sub pub/sub sample redis source code c\u6ca1\u6709destructor\uff0c\u6211\u770bredis\u7684source code\u4e2d\u5b58\u5728\u7740\u5927\u91cf\u7684\u6307\u9488\uff0c\u90a3\u4e48\u5f53redis\u8fdb\u7a0b\u9000\u51fa\u6267\u884c\u7684\u65f6\u5019\uff0c\u662f\u5426\u9700\u8981\u6267\u884c\u6e05\u7406\u5de5\u4f5c\u5462\uff1f\u8fd8\u662f\u8bf4\u5b8c\u5168\u4f9d\u9760OS\u6765\u6267\u884c\u8fd9\u4e9b\u6e05\u7406\uff1b redis\u4f7f\u7528\u5f15\u7528\u8ba1\u6570\u6765\u5b9e\u73b0\u81ea\u52a8\u5185\u5b58\u7ba1\u7406 redis\u4fe1\u53f7\u5904\u7406 redis-server\u5411\u81ea\u5df1\u53d1\u9001\u4fe1\u53f7\u662f\u5426\u4f7f\u7528\u7684\u662fraise\uff1f \u53c2\u52a0APUE10.9 stream oriented \u51fa\u81ea\uff1a https://redis.io/topics/protocol Why TCP called stream oriented protocol? byte stream vs character stream consistent hashing https://redis.io/topics/cluster-tutorial https://en.wikipedia.org/wiki/Consistent_hashing redis data consistency https://www.cnblogs.com/bigben0123/p/9115597.html https://stackoverflow.com/questions/13681565/does-redis-support-strong-consistency http://antirez.com/news/36 https://docs.redislabs.com/latest/rs/concepts/data-access/consistency-durability/ https://quabase.sei.cmu.edu/mediawiki/index.php/Redis_Consistency_Features How Twitter Uses Redis To Scale - 105TB RAM, 39MM QPS, 10,000+ Instances http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html Learn Redis the hard way (in production) https://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/","title":"TODO"},{"location":"TODO/#todo","text":"","title":"TODO"},{"location":"TODO/#cache#replacement#policies","text":"\u5728\u9605\u8bfbredis\u7684\u6e90\u4ee3\u7801\u7684\u65f6\u5019\uff0c \u53d1\u73b0\u4e86\u5982\u4e0b\u7684\u5b9a\u4e49\uff1a typedef struct redisObject { unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; /* lru time (relative to server.lruclock) */ int refcount; void *ptr; } robj; \u5176\u4e2d\u7684LRU\u5f15\u8d77\u4e86\u6211\u7684\u6ce8\u610f\uff0c\u9042Google\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u4e86\u5982\u4e0b\u5185\u5bb9\uff1a","title":"Cache replacement policies"},{"location":"TODO/#-#cache#replacement#policies","text":"","title":"- Cache replacement policies"},{"location":"TODO/#redis#persistence","text":"https://redis.io/topics/persistence","title":"Redis Persistence"},{"location":"TODO/#redis#sentinel#documentation","text":"","title":"Redis Sentinel Documentation"},{"location":"TODO/#clients#service#discovery","text":"https://en.wikipedia.org/wiki/Service_discovery https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/ https://stackoverflow.com/questions/37148836/what-is-service-discovery-and-why-do-you-need-it","title":"clients service discovery"},{"location":"TODO/#redis#pubsub","text":"How to use Pub/sub with hiredis in C++? Add code example for pub/sub pub/sub sample","title":"redis pub/sub"},{"location":"TODO/#redis#source#code","text":"c\u6ca1\u6709destructor\uff0c\u6211\u770bredis\u7684source code\u4e2d\u5b58\u5728\u7740\u5927\u91cf\u7684\u6307\u9488\uff0c\u90a3\u4e48\u5f53redis\u8fdb\u7a0b\u9000\u51fa\u6267\u884c\u7684\u65f6\u5019\uff0c\u662f\u5426\u9700\u8981\u6267\u884c\u6e05\u7406\u5de5\u4f5c\u5462\uff1f\u8fd8\u662f\u8bf4\u5b8c\u5168\u4f9d\u9760OS\u6765\u6267\u884c\u8fd9\u4e9b\u6e05\u7406\uff1b redis\u4f7f\u7528\u5f15\u7528\u8ba1\u6570\u6765\u5b9e\u73b0\u81ea\u52a8\u5185\u5b58\u7ba1\u7406","title":"redis source code"},{"location":"TODO/#redis","text":"redis-server\u5411\u81ea\u5df1\u53d1\u9001\u4fe1\u53f7\u662f\u5426\u4f7f\u7528\u7684\u662fraise\uff1f \u53c2\u52a0APUE10.9","title":"redis\u4fe1\u53f7\u5904\u7406"},{"location":"TODO/#stream#oriented","text":"\u51fa\u81ea\uff1a https://redis.io/topics/protocol Why TCP called stream oriented protocol?","title":"stream oriented"},{"location":"TODO/#byte#stream#vs#character#stream","text":"","title":"byte stream vs character stream"},{"location":"TODO/#consistent#hashing","text":"https://redis.io/topics/cluster-tutorial https://en.wikipedia.org/wiki/Consistent_hashing","title":"consistent hashing"},{"location":"TODO/#redis#data#consistency","text":"https://www.cnblogs.com/bigben0123/p/9115597.html https://stackoverflow.com/questions/13681565/does-redis-support-strong-consistency http://antirez.com/news/36 https://docs.redislabs.com/latest/rs/concepts/data-access/consistency-durability/ https://quabase.sei.cmu.edu/mediawiki/index.php/Redis_Consistency_Features","title":"redis data consistency"},{"location":"TODO/#how#twitter#uses#redis#to#scale#-#105tb#ram#39mm#qps#10000#instances","text":"http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html","title":"How Twitter Uses Redis To Scale - 105TB RAM, 39MM QPS, 10,000+ Instances"},{"location":"TODO/#learn#redis#the#hard#way#in#production","text":"https://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/","title":"Learn Redis the hard way (in production)"},{"location":"Amazon-ElastiCache/","text":"Amazon ElastiCache Documentation # Amazon ElastiCache for Redis \u4e00\u3001\u5176\u4e2d\u7684\u4e00\u4e9b\u5185\u5bb9\u5bf9\u4e8e\u90e8\u7f72\u662f\u6709\u501f\u9274\u53c2\u8003\u4ef7\u503c\u7684\u3002 \u4e8c\u3001\u539f\u6587\u7684\u5185\u5bb9\uff0c \u4e3b\u8981\u662f\u57fa\u4e8eRedis\u7684\u4e24\u79cd\u90e8\u7f72\u65b9\u5f0f\u6765\u5c55\u5f00\u8ba8\u8bba\u7684: \u90e8\u7f72\u65b9\u5f0f shard\u6570\u91cf Redis (cluster mode disabled) clusters \u4e00\u4e2ashard Redis (cluster mode enabled) clusters up to 500 shards Choosing regions and availability zones AWS Cloud computing resources are housed in highly available data center facilities. To provide additional scalability and reliability, these data center facilities are located in different physical locations. These locations are categorized by regions and Availability Zones . NOTE: \u7269\u7406\u9694\u79bb","title":"Introduction"},{"location":"Amazon-ElastiCache/#amazon#elasticache#documentation#amazon#elasticache#for#redis","text":"\u4e00\u3001\u5176\u4e2d\u7684\u4e00\u4e9b\u5185\u5bb9\u5bf9\u4e8e\u90e8\u7f72\u662f\u6709\u501f\u9274\u53c2\u8003\u4ef7\u503c\u7684\u3002 \u4e8c\u3001\u539f\u6587\u7684\u5185\u5bb9\uff0c \u4e3b\u8981\u662f\u57fa\u4e8eRedis\u7684\u4e24\u79cd\u90e8\u7f72\u65b9\u5f0f\u6765\u5c55\u5f00\u8ba8\u8bba\u7684: \u90e8\u7f72\u65b9\u5f0f shard\u6570\u91cf Redis (cluster mode disabled) clusters \u4e00\u4e2ashard Redis (cluster mode enabled) clusters up to 500 shards","title":"Amazon ElastiCache Documentation # Amazon ElastiCache for Redis"},{"location":"Amazon-ElastiCache/#choosing#regions#and#availability#zones","text":"AWS Cloud computing resources are housed in highly available data center facilities. To provide additional scalability and reliability, these data center facilities are located in different physical locations. These locations are categorized by regions and Availability Zones . NOTE: \u7269\u7406\u9694\u79bb","title":"Choosing regions and availability zones"},{"location":"Amazon-ElastiCache/High-availability-using-replication-groups/","text":"amazon Replication: Redis (Cluster Mode Disabled) vs. Redis (Cluster Mode Enabled) Beginning with Redis version 3.2, you have the ability to create one of two distinct types of Redis clusters (API/CLI: replication groups). A Redis (cluster mode disabled) cluster always has a single shard (API/CLI: node group) with up to 5 read replica nodes. A Redis (cluster mode enabled) cluster has up to 90 shards with 1 to 5 read replica nodes in each. Redis (cluster mode disabled) and Redis (cluster mode enabled) clusters The following table summarizes important differences between Redis (cluster mode disabled) and Redis (cluster mode enabled) clusters. Comparing Redis (Cluster Mode Disabled) and Redis (Cluster Mode Enabled) Clusters Feature Redis (cluster mode disabled) Redis (cluster mode enabled) Modifiable Yes. Supports adding and deleting replica nodes, and scaling up node type. Limited. For more information, see Upgrading Engine Versions and Scaling Clusters in Redis (Cluster Mode Enabled) . Data Partitioning No Yes Shards 1 1 to 90 Read replicas 0 to 5 Important If you have no replicas and the node fails, you experience total data loss. 0 to 5 per shard. Important If you have no replicas and a node fails, you experience loss of all data in that shard. Multi-AZ with Automatic Failover Yes, with at least 1 replica. Optional. On by default. Yes. Required. Snapshots (Backups) Yes, creating a single .rdb file. Yes, creating a unique .rdb file for each shard. Restore Yes, using a single .rdb file from a Redis (cluster mode disabled) cluster. Yes, using .rdb files from either a Redis (cluster mode disabled) or a Redis (cluster mode enabled) cluster. Supported by All Redis versions Redis 3.2 and following Engine upgradeable Yes, with some limits. For more information, see Upgrading Engine Versions . Yes, with some limits. For more information, see Upgrading Engine Versions . Encryption Versions 3.2.6 and 4.0.10 and later. Versions 3.2.6 and 4.0.10 and later. HIPAA Compliant Version 3.2.6 and 4.0.10 and later. Version 3.2.6 and 4.0.10 and later. PCI DSS Compliant Version 3.2.6 and 4.0.10 and later. Version 3.2.6 and 4.0.10 and later. Online resharding N/A Version 3.2.10 and later.","title":"Introduction"},{"location":"Amazon-ElastiCache/High-availability-using-replication-groups/#amazon#replication#redis#cluster#mode#disabled#vs#redis#cluster#mode#enabled","text":"Beginning with Redis version 3.2, you have the ability to create one of two distinct types of Redis clusters (API/CLI: replication groups). A Redis (cluster mode disabled) cluster always has a single shard (API/CLI: node group) with up to 5 read replica nodes. A Redis (cluster mode enabled) cluster has up to 90 shards with 1 to 5 read replica nodes in each. Redis (cluster mode disabled) and Redis (cluster mode enabled) clusters The following table summarizes important differences between Redis (cluster mode disabled) and Redis (cluster mode enabled) clusters. Comparing Redis (Cluster Mode Disabled) and Redis (Cluster Mode Enabled) Clusters Feature Redis (cluster mode disabled) Redis (cluster mode enabled) Modifiable Yes. Supports adding and deleting replica nodes, and scaling up node type. Limited. For more information, see Upgrading Engine Versions and Scaling Clusters in Redis (Cluster Mode Enabled) . Data Partitioning No Yes Shards 1 1 to 90 Read replicas 0 to 5 Important If you have no replicas and the node fails, you experience total data loss. 0 to 5 per shard. Important If you have no replicas and a node fails, you experience loss of all data in that shard. Multi-AZ with Automatic Failover Yes, with at least 1 replica. Optional. On by default. Yes. Required. Snapshots (Backups) Yes, creating a single .rdb file. Yes, creating a unique .rdb file for each shard. Restore Yes, using a single .rdb file from a Redis (cluster mode disabled) cluster. Yes, using .rdb files from either a Redis (cluster mode disabled) or a Redis (cluster mode enabled) cluster. Supported by All Redis versions Redis 3.2 and following Engine upgradeable Yes, with some limits. For more information, see Upgrading Engine Versions . Yes, with some limits. For more information, see Upgrading Engine Versions . Encryption Versions 3.2.6 and 4.0.10 and later. Versions 3.2.6 and 4.0.10 and later. HIPAA Compliant Version 3.2.6 and 4.0.10 and later. Version 3.2.6 and 4.0.10 and later. PCI DSS Compliant Version 3.2.6 and 4.0.10 and later. Version 3.2.6 and 4.0.10 and later. Online resharding N/A Version 3.2.10 and later.","title":"amazon Replication: Redis (Cluster Mode Disabled) vs. Redis (Cluster Mode Enabled)"},{"location":"Application/Redis-Cache/","text":"Redis cache \u53c2\u8003\u6587\u7ae0: 1\u3001toutiao \u963f\u91ccP8\u6280\u672f\u4e13\u5bb6\u7ec6\u7a76\u5206\u5e03\u5f0f\u7f13\u5b58\u95ee\u9898 cnblogs Redis\u7cfb\u5217\u5341\uff1a\u7f13\u5b58\u96ea\u5d29\u3001\u7f13\u5b58\u7a7f\u900f\u3001\u7f13\u5b58\u9884\u70ed\u3001\u7f13\u5b58\u66f4\u65b0\u3001\u7f13\u5b58\u964d\u7ea7 NOTE: \u4e24\u8005\u5176\u5b9e\u4e3a\u4e00\u7bc7 2\u3001 csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29 \u8fd9\u7bc7\u6587\u7ae0\u8bb2\u89e3\u5f97\u6bd4\u8f83\u597d 3\u3001developpaper Redis \u2013 cache avalanche, cache breakdown, cache penetration \u975e\u5e38\u7cbe\u7b80 4\u3001ideras Redis cache penetration & breakdown & avalanche \u95ee\u9898 Cache penetration / \u7f13\u5b58\u7a7f\u900f \u7f13\u5b58\u7a7f\u900f\u662f\u6307\u7528\u6237\u67e5\u8be2\u6570\u636e\uff0c\u5728\u6570\u636e\u5e93\u6ca1\u6709\uff0c\u81ea\u7136\u5728\u7f13\u5b58\u4e2d\u4e5f\u4e0d\u4f1a\u6709\u3002\u8fd9\u6837\u5c31\u5bfc\u81f4\u7528\u6237\u67e5\u8be2\u7684\u65f6\u5019\uff0c\u5728\u7f13\u5b58\u4e2d\u627e\u4e0d\u5230\uff0c\u6bcf\u6b21\u90fd\u8981\u53bb\u6570\u636e\u5e93\u518d\u67e5\u8be2\u4e00\u904d\uff0c\u7136\u540e\u8fd4\u56de\u7a7a\uff08\u76f8\u5f53\u4e8e\u8fdb\u884c\u4e86\u4e24\u6b21\u65e0\u7528\u7684\u67e5\u8be2\uff09\u3002\u8fd9\u6837\u8bf7\u6c42\u5c31\u7ed5\u8fc7\u7f13\u5b58\u76f4\u63a5\u67e5\u6570\u636e\u5e93\uff0c\u8fd9\u4e5f\u662f\u7ecf\u5e38\u63d0\u7684\u7f13\u5b58\u547d\u4e2d\u7387\u95ee\u9898\u3002 toutiao \u963f\u91ccP8\u6280\u672f\u4e13\u5bb6\u7ec6\u7a76\u5206\u5e03\u5f0f\u7f13\u5b58\u95ee\u9898 \u5982\u679c\u6b64\u65f6\u6709\u4eba\u6076\u610f\u7684\u653b\u51fb\u5462\uff1f\u53d1\u8d77\u51e0\u5341\u4ebf\u4e07\u6761redis\u548cmysql\u4e2d\u90fd\u4e0d\u5b58\u5728\u7684\u6570\u636e\uff0c\u8bf7\u6c42\u8bbf\u95ee\u4f60\u7684\u7f51\u7ad9\uff0c\u6570\u636e\u5e93\u4e0d\u5c31\u6302\u4e86\u3002 csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29 Cache breakdown / \u7f13\u5b58\u51fb\u7a7f csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29 Cache avalanche / \u7f13\u5b58\u96ea\u5d29 \u7f13\u5b58\u96ea\u5d29\u6307\u7684\u662f: \u6240\u6709\u7684\u7f13\u5b58\u90fd\u6d88\u5931\u4e86\uff0c\u53ef\u80fd\u539f\u56e0: 1\u3001\u7531\u4e8e\u539f\u6709\u7f13\u5b58\u5931\u6548\uff0c\u65b0\u7f13\u5b58\u672a\u5230\u671f\u95f4(\u4f8b\u5982\uff1a\u6211\u4eec\u8bbe\u7f6e\u7f13\u5b58\u65f6\u91c7\u7528\u4e86\u76f8\u540c\u7684\u8fc7\u671f\u65f6\u95f4\uff0c\u5728\u540c\u4e00\u65f6\u523b\u51fa\u73b0\u5927\u9762\u79ef\u7684\u7f13\u5b58\u8fc7\u671f)\uff0c\u6240\u6709\u539f\u672c\u5e94\u8be5\u8bbf\u95ee\u7f13\u5b58\u7684\u8bf7\u6c42\u90fd\u53bb\u67e5\u8be2\u6570\u636e\u5e93\u4e86\uff0c\u800c\u5bf9\u6570\u636e\u5e93CPU\u548c\u5185\u5b58\u9020\u6210\u5de8\u5927\u538b\u529b\uff0c\u4e25\u91cd\u7684\u4f1a\u9020\u6210\u6570\u636e\u5e93\u5b95\u673a\u3002\u4ece\u800c\u5f62\u6210\u4e00\u7cfb\u5217\u8fde\u9501\u53cd\u5e94\uff0c\u9020\u6210\u6574\u4e2a\u7cfb\u7edf\u5d29\u6e83\u3002 2\u3001Redis\u5b95\u673a\u4e86 \u89e3\u51b3\u601d\u8def \u770b\u4e86\u5404\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u5b9e\uff0c\u76ee\u6807\u90fd\u662f\u4e00\u81f4\u7684: \u907f\u514d\u5927\u6279\u91cf\u7684\u8bf7\u6c42\u76f4\u63a5\u8fdb\u5165\u5230\u6570\u636e\u5e93\uff0c\u800c\u9020\u6210\u6570\u636e\u5e93\u538b\u529b\u8fc7\u5927\u800c\u5b95\u673a\uff1b \u6240\u4ee5\u89e3\u51b3\u601d\u8def\u5c31\u662f: \u62e6\u622a\u8bf7\u6c42\uff1b \u5728\u4e0d\u540c\u7684\u573a\u666f\u4e2d\uff0c\u62e6\u622a\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u9700\u8981\u7ed3\u5408\u573a\u666f\u8fdb\u884c\u5206\u6790\u3002 developpaper Redis \u2013 cache avalanche, cache breakdown, cache penetration 2. Cache penetration(\u7a7f\u900f) concept Cache penetration refers to the continuous request for data that does not exist in the cache, then all requests fall into the database layer. In the case of high concurrency, it will directly affect the business of the whole system, and even lead to system crash process 1\u3001Get data from cache according to key; 2\u3001If the data is not empty, it will be returned directly; 3\u3001If the data is empty, query the database; 4\u3001If the data queried from the database is not empty, it will be put into the cache (set the expiration time) Repeat the third step continuously. For example, if you want to check the information of a product and pass in '- 1', all requests will be directly placed in the database solve 1\u3001Set the null object cache If the data retrieved from the database is null, it will also be cached. However, setting a shorter cache time can slow down the cache penetration to a certain extent 2\u3001Bulon filter In essence, bloom filter is a highly efficient probabilistic algorithm and data structure, which is mainly used to determine whether an element exists in a set. //Use You can add the true and correct key to the filter after adding it in advance. Each time you query again, first confirm whether the key to be queried is in the filter. If not, the key is illegal, and there is no need for subsequent query steps. 3. Cache avalanche(\u96ea\u5d29) concept Cache avalanche refers to that a large number of data in the cache are expired at the same time or the redis service is hung up, and the huge amount of query data causes the database pressure to be too large. process 1\u3001 The information of commodities is cached in the mall for 24 hours 2\u3001 At 0 o'clock the next day, there was a rush sale 3\u3001 At the beginning of the rush purchase, all caches are invalid, and all commodity information queries fall into the database at the same time, which may lead to excessive pressure solve 1\u3001You can cache data by category and add different cache times 2\u3001At the same time of caching, add a random number to the cache time, so that all caches will not fail at the same time 3\u3001For the problem of the redis service hanging up, we can realize the high availability master-slave architecture of redis, and implement the persistence of redis. When the redis is down, read the local cache data, and restore the redis service to load the persistent data 4. Buffer breakdown(\u51fb\u7a7f) concept Cache breakdown is similar to cache avalanche, but it is not a massive cache failure Cache breakdown means that a key value in the cache continuously receives a large number of requests, and at the moment when the key value fails, a large number of requests fall on the database, which may lead to excessive pressure on the database process 1\u3001 There is a cache corresponding to key1, key2, etc 2\u3001 Key1 is a hot commodity, constantly carrying the request 3\u3001 The expiration time of key1 is up, and the request falls on the database instantly, which may cause excessive pressure solve 1\u3001Add distributed locks or distributed queues In order to avoid a large number of concurrent requests falling on the underlying storage system, istributed locks or distributed queues are used to ensure the cache single thread write. In the lock method, first get the lock from the cache again (to prevent another thread from acquiring the lock first, and the lock has been written to the cache), and then the DB write cache is not checked. (of course, you can poll the cache until it times out in a thread that does not acquire a lock.) 2\u3001 Add timeout flag An attribute is added to the cached object to identify the timeout time. When the data is obtained, the internal marking time of the data is checked to determine whether the timeout is fast. If so, a thread is initiated asynchronously (concurrency is controlled) to actively update the cache. 3\u3001 In addition, there is a crude method. If the real-time performance of your hot data is relatively low, you can set the hot data not to expire in the hot period, and expire in the low peak period of access, such as in the early hours of every day.","title":"Introduction"},{"location":"Application/Redis-Cache/#redis#cache","text":"","title":"Redis cache"},{"location":"Application/Redis-Cache/#_1","text":"1\u3001toutiao \u963f\u91ccP8\u6280\u672f\u4e13\u5bb6\u7ec6\u7a76\u5206\u5e03\u5f0f\u7f13\u5b58\u95ee\u9898 cnblogs Redis\u7cfb\u5217\u5341\uff1a\u7f13\u5b58\u96ea\u5d29\u3001\u7f13\u5b58\u7a7f\u900f\u3001\u7f13\u5b58\u9884\u70ed\u3001\u7f13\u5b58\u66f4\u65b0\u3001\u7f13\u5b58\u964d\u7ea7 NOTE: \u4e24\u8005\u5176\u5b9e\u4e3a\u4e00\u7bc7 2\u3001 csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29 \u8fd9\u7bc7\u6587\u7ae0\u8bb2\u89e3\u5f97\u6bd4\u8f83\u597d 3\u3001developpaper Redis \u2013 cache avalanche, cache breakdown, cache penetration \u975e\u5e38\u7cbe\u7b80 4\u3001ideras Redis cache penetration & breakdown & avalanche","title":"\u53c2\u8003\u6587\u7ae0:"},{"location":"Application/Redis-Cache/#_2","text":"","title":"\u95ee\u9898"},{"location":"Application/Redis-Cache/#cache#penetration","text":"\u7f13\u5b58\u7a7f\u900f\u662f\u6307\u7528\u6237\u67e5\u8be2\u6570\u636e\uff0c\u5728\u6570\u636e\u5e93\u6ca1\u6709\uff0c\u81ea\u7136\u5728\u7f13\u5b58\u4e2d\u4e5f\u4e0d\u4f1a\u6709\u3002\u8fd9\u6837\u5c31\u5bfc\u81f4\u7528\u6237\u67e5\u8be2\u7684\u65f6\u5019\uff0c\u5728\u7f13\u5b58\u4e2d\u627e\u4e0d\u5230\uff0c\u6bcf\u6b21\u90fd\u8981\u53bb\u6570\u636e\u5e93\u518d\u67e5\u8be2\u4e00\u904d\uff0c\u7136\u540e\u8fd4\u56de\u7a7a\uff08\u76f8\u5f53\u4e8e\u8fdb\u884c\u4e86\u4e24\u6b21\u65e0\u7528\u7684\u67e5\u8be2\uff09\u3002\u8fd9\u6837\u8bf7\u6c42\u5c31\u7ed5\u8fc7\u7f13\u5b58\u76f4\u63a5\u67e5\u6570\u636e\u5e93\uff0c\u8fd9\u4e5f\u662f\u7ecf\u5e38\u63d0\u7684\u7f13\u5b58\u547d\u4e2d\u7387\u95ee\u9898\u3002 toutiao \u963f\u91ccP8\u6280\u672f\u4e13\u5bb6\u7ec6\u7a76\u5206\u5e03\u5f0f\u7f13\u5b58\u95ee\u9898 \u5982\u679c\u6b64\u65f6\u6709\u4eba\u6076\u610f\u7684\u653b\u51fb\u5462\uff1f\u53d1\u8d77\u51e0\u5341\u4ebf\u4e07\u6761redis\u548cmysql\u4e2d\u90fd\u4e0d\u5b58\u5728\u7684\u6570\u636e\uff0c\u8bf7\u6c42\u8bbf\u95ee\u4f60\u7684\u7f51\u7ad9\uff0c\u6570\u636e\u5e93\u4e0d\u5c31\u6302\u4e86\u3002 csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29","title":"Cache penetration /  \u7f13\u5b58\u7a7f\u900f"},{"location":"Application/Redis-Cache/#cache#breakdown","text":"csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29","title":"Cache breakdown / \u7f13\u5b58\u51fb\u7a7f"},{"location":"Application/Redis-Cache/#cache#avalanche","text":"\u7f13\u5b58\u96ea\u5d29\u6307\u7684\u662f: \u6240\u6709\u7684\u7f13\u5b58\u90fd\u6d88\u5931\u4e86\uff0c\u53ef\u80fd\u539f\u56e0: 1\u3001\u7531\u4e8e\u539f\u6709\u7f13\u5b58\u5931\u6548\uff0c\u65b0\u7f13\u5b58\u672a\u5230\u671f\u95f4(\u4f8b\u5982\uff1a\u6211\u4eec\u8bbe\u7f6e\u7f13\u5b58\u65f6\u91c7\u7528\u4e86\u76f8\u540c\u7684\u8fc7\u671f\u65f6\u95f4\uff0c\u5728\u540c\u4e00\u65f6\u523b\u51fa\u73b0\u5927\u9762\u79ef\u7684\u7f13\u5b58\u8fc7\u671f)\uff0c\u6240\u6709\u539f\u672c\u5e94\u8be5\u8bbf\u95ee\u7f13\u5b58\u7684\u8bf7\u6c42\u90fd\u53bb\u67e5\u8be2\u6570\u636e\u5e93\u4e86\uff0c\u800c\u5bf9\u6570\u636e\u5e93CPU\u548c\u5185\u5b58\u9020\u6210\u5de8\u5927\u538b\u529b\uff0c\u4e25\u91cd\u7684\u4f1a\u9020\u6210\u6570\u636e\u5e93\u5b95\u673a\u3002\u4ece\u800c\u5f62\u6210\u4e00\u7cfb\u5217\u8fde\u9501\u53cd\u5e94\uff0c\u9020\u6210\u6574\u4e2a\u7cfb\u7edf\u5d29\u6e83\u3002 2\u3001Redis\u5b95\u673a\u4e86","title":"Cache avalanche / \u7f13\u5b58\u96ea\u5d29"},{"location":"Application/Redis-Cache/#_3","text":"\u770b\u4e86\u5404\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u5b9e\uff0c\u76ee\u6807\u90fd\u662f\u4e00\u81f4\u7684: \u907f\u514d\u5927\u6279\u91cf\u7684\u8bf7\u6c42\u76f4\u63a5\u8fdb\u5165\u5230\u6570\u636e\u5e93\uff0c\u800c\u9020\u6210\u6570\u636e\u5e93\u538b\u529b\u8fc7\u5927\u800c\u5b95\u673a\uff1b \u6240\u4ee5\u89e3\u51b3\u601d\u8def\u5c31\u662f: \u62e6\u622a\u8bf7\u6c42\uff1b \u5728\u4e0d\u540c\u7684\u573a\u666f\u4e2d\uff0c\u62e6\u622a\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u9700\u8981\u7ed3\u5408\u573a\u666f\u8fdb\u884c\u5206\u6790\u3002","title":"\u89e3\u51b3\u601d\u8def"},{"location":"Application/Redis-Cache/#developpaper#redis#cache#avalanche#cache#breakdown#cache#penetration","text":"","title":"developpaper Redis \u2013 cache avalanche, cache breakdown, cache penetration"},{"location":"Application/Redis-Cache/#2#cache#penetration","text":"","title":"2. Cache penetration(\u7a7f\u900f)"},{"location":"Application/Redis-Cache/#concept","text":"Cache penetration refers to the continuous request for data that does not exist in the cache, then all requests fall into the database layer. In the case of high concurrency, it will directly affect the business of the whole system, and even lead to system crash","title":"concept"},{"location":"Application/Redis-Cache/#process","text":"1\u3001Get data from cache according to key; 2\u3001If the data is not empty, it will be returned directly; 3\u3001If the data is empty, query the database; 4\u3001If the data queried from the database is not empty, it will be put into the cache (set the expiration time) Repeat the third step continuously. For example, if you want to check the information of a product and pass in '- 1', all requests will be directly placed in the database","title":"process"},{"location":"Application/Redis-Cache/#solve","text":"1\u3001Set the null object cache If the data retrieved from the database is null, it will also be cached. However, setting a shorter cache time can slow down the cache penetration to a certain extent 2\u3001Bulon filter In essence, bloom filter is a highly efficient probabilistic algorithm and data structure, which is mainly used to determine whether an element exists in a set. //Use You can add the true and correct key to the filter after adding it in advance. Each time you query again, first confirm whether the key to be queried is in the filter. If not, the key is illegal, and there is no need for subsequent query steps.","title":"solve"},{"location":"Application/Redis-Cache/#3#cache#avalanche","text":"","title":"3. Cache avalanche(\u96ea\u5d29)"},{"location":"Application/Redis-Cache/#concept_1","text":"Cache avalanche refers to that a large number of data in the cache are expired at the same time or the redis service is hung up, and the huge amount of query data causes the database pressure to be too large.","title":"concept"},{"location":"Application/Redis-Cache/#process_1","text":"1\u3001 The information of commodities is cached in the mall for 24 hours 2\u3001 At 0 o'clock the next day, there was a rush sale 3\u3001 At the beginning of the rush purchase, all caches are invalid, and all commodity information queries fall into the database at the same time, which may lead to excessive pressure","title":"process"},{"location":"Application/Redis-Cache/#solve_1","text":"1\u3001You can cache data by category and add different cache times 2\u3001At the same time of caching, add a random number to the cache time, so that all caches will not fail at the same time 3\u3001For the problem of the redis service hanging up, we can realize the high availability master-slave architecture of redis, and implement the persistence of redis. When the redis is down, read the local cache data, and restore the redis service to load the persistent data","title":"solve"},{"location":"Application/Redis-Cache/#4#buffer#breakdown","text":"","title":"4. Buffer breakdown(\u51fb\u7a7f)"},{"location":"Application/Redis-Cache/#concept_2","text":"Cache breakdown is similar to cache avalanche, but it is not a massive cache failure Cache breakdown means that a key value in the cache continuously receives a large number of requests, and at the moment when the key value fails, a large number of requests fall on the database, which may lead to excessive pressure on the database","title":"concept"},{"location":"Application/Redis-Cache/#process_2","text":"1\u3001 There is a cache corresponding to key1, key2, etc 2\u3001 Key1 is a hot commodity, constantly carrying the request 3\u3001 The expiration time of key1 is up, and the request falls on the database instantly, which may cause excessive pressure","title":"process"},{"location":"Application/Redis-Cache/#solve_2","text":"1\u3001Add distributed locks or distributed queues In order to avoid a large number of concurrent requests falling on the underlying storage system, istributed locks or distributed queues are used to ensure the cache single thread write. In the lock method, first get the lock from the cache again (to prevent another thread from acquiring the lock first, and the lock has been written to the cache), and then the DB write cache is not checked. (of course, you can poll the cache until it times out in a thread that does not acquire a lock.) 2\u3001 Add timeout flag An attribute is added to the cached object to identify the timeout time. When the data is obtained, the internal marking time of the data is checked to determine whether the timeout is fast. If so, a thread is initiated asynchronously (concurrency is controlled) to actively update the cache. 3\u3001 In addition, there is a crude method. If the real-time performance of your hot data is relatively low, you can set the hot data not to expire in the hot period, and expire in the low peak period of access, such as in the early hours of every day.","title":"solve"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/","text":"csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29 \u4e00\u822c\u7528redis\u5e38\u89c4\u7684\u5199\u6cd5\uff1a 1\uff1a\u5ba2\u6237\u7aef\u53d1\u8d77\u8bf7\u6c42\u3002 2\uff1a\u5224\u65adredis\u4e2d\u662f\u5426\u6709\u6570\u636e\uff0c\u5982\u679c\u6709\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\uff0c\u6ca1\u6709\u5219\u8bf7\u6c42\u6570\u636e\u5e93,\u67e5\u51fa\u6570\u636e\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002 \u5e26\u6765\u7684\u95ee\u9898\u4e00\uff1a\u7f13\u5b58\u7a7f\u900f 1\uff1a\u5982\u679c\u6b64\u65f6\u6709\u4eba\u6076\u610f\u7684\u653b\u51fb\u5462\uff1f\u53d1\u8d77\u51e0\u5341\u4ebf\u4e07\u6761redis\u548cmysql\u4e2d\u90fd\u4e0d\u5b58\u5728\u7684\u6570\u636e\uff0c\u8bf7\u6c42\u8bbf\u95ee\u4f60\u7684\u7f51\u7ad9\uff0c\u6570\u636e\u5e93\u4e0d\u5c31\u6302\u4e86\u3002 \u89e3\u51b3\u529e\u6cd51\uff1a\u4f7f\u7528\u5e03\u9686\u8fc7\u6ee4\u5668 redis\u4e2d\u6ca1\u6709\u6570\u636e\uff0c\u8bf7\u6c42\u5e03\u9686\u8fc7\u6ee4\u5668\u62e6\u622a\u8bf7\u6c42mysql\u548credis\u4e2d\u4e0d\u5b58\u5728\u7684key\u7684\u8bf7\u6c42\u3002\u5982\u679c\u5e03\u9686\u8fc7\u6ee4\u5668\u4e2d\u5bf9\u5e94\u7684key\uff0c\u5728\u8bf7\u6c42\u6570\u636e\u5e93\uff0c\u6ca1\u6709\u5219\u8fd4\u56de\u4e00\u4e2a\u975e\u6cd5\u8bbf\u95ee \u89e3\u51b3\u529e\u6cd52\uff1a\u7f13\u5b58\u7a7a\u6570\u636e \u5728\u5ba2\u6237\u7aef\u8bf7\u6c42redis\u65f6\u53d1\u73b0\u6ca1\u6709key\uff0c\u63a5\u7740\u8bf7\u6c42mysql\u53d1\u73b0\u4e5f\u6ca1\u6709key\uff0c\u6b64\u65f6\u5c31\u628akey\u7f13\u5b58\u8d77\u6765\uff0cvalue\u8bbe\u7f6e\u4e3anull\u3002\uff08 \u7f3a\u70b9\u53ea\u9002\u5408\u5355\u4e00key\u591a\u6b21\u8bbf\u95ee\u6570\u636e\u5e93\u7684\u60c5\u51b5 \uff09 \u5e26\u6765\u7684\u95ee\u9898\u4e8c\uff1a\u7f13\u5b58\u51fb\u7a7f \u5ba2\u6237\u7aef\u8bbf\u95ee\u6570\u636e\u7684\u65f6\u5019\uff0credis\u4e2d\u6ca1\u6709\u6570\u636e\uff0cmysql\u4e2d\u6709\u6570\u636e\uff0c\u76f8\u5f53\u4e8e\u76f4\u63a5\u8df3\u8fc7\u4e86redis\u3002 \u4e3a\u4f55\u4f1a\u53d1\u751f \u7528\u6237\u8bbf\u95ee\u8fd9\u6761\u6570\u636e\u7684\u65f6\u5019\uff0c\u70ed\u70b9\u6570\u636e\u8fc7\u671f\u65f6\u95f4\u521a\u597d\u5230\u4e86\u3002 \u95ee\u9898 1\uff1a\u5982\u679c\u6b64\u65f6\u8fd9\u6761\u6570\u636e\u5f88\u70ed\u95e8\uff0c\u79d2\u7ea7\u6709\u51e0\u5341\u4ebf\u4e07\u6b21\u7684\u8bbf\u95ee\u91cf\uff0c\u6570\u636e\u5e93\u4e0d\u5c31\u6302\u4e86\u3002 \u89e3\u51b3 1\uff1a\u8bbe\u7f6e\u70ed\u70b9\u6570\u636e\u6c38\u8fdc\u4e0d\u8fc7\u671f\u3002 2\uff1a\u53d1\u73b0redis\u4e2d\u6ca1\u6709\u6570\u636e\uff0c\u52a0\u5165\u5206\u5e03\u5f0f\u9501\uff08\u62e6\u622a\u8bf7\u6c42\uff09\uff0c\u63a5\u7740\u67e5\u8be2redis\uff0c\u67e5\u8be2\u6570\u636e\u5e93\u540e\uff0c\u628a\u8fd9\u6761\u6570\u636e\u91cd\u65b0\u52a0\u5165\u5230\u7f13\u5b58\uff0c\u91ca\u653e\u9501\uff0c\u4e4b\u540e\u5269\u4f59\u7684\u8bf7\u6c42\uff0c\u8bf7\u6c42redis\u65f6\u5c31\u53ef\u4ee5\u67e5\u5230\u6570\u636e\u4e86 public Item_kill getItemKill ( int id ) { /** * \u5e03\u9686\u8fc7\u6ee4\u5668\u89e3\u51b3\u7f13\u5b58\u7a7f\u900f */ if ( ! bloomFilter . mightContain ( id )) { log . warn ( \"\u975e\u6cd5\u79d2\u6740\u5546\u54c1id\" + id ); return null ; } /** * \u4ece\u7f13\u5b58\u4e2d\u83b7\u53d6\u79d2\u6740\u5546\u54c1\u7684\u4fe1\u606f\uff0c\u5982\u679c\u6b64\u65f6\u70ed\u70b9\u6570\u636e\u6070\u597d\u8fc7\u671f\u4e86\u5462\uff1f */ Item_kill itemKill = ( Item_kill ) redisTemplate . opsForValue (). get ( CACHE_PRE + \"[\" + id + \"]\" ); if ( itemKill != null ) { return itemKill ; } /** * \u5b9a\u4e49\u552f\u4e00\u6807\u8bc6key */ String key = new StringBuffer (). append ( id ). toString (); RLock lock = redissonClient . getLock ( key ); boolean ok = false ; Item_kill item_kill = null ; try { ok = lock . tryLock ( 30 , 10 , TimeUnit . SECONDS ); if ( ok ){ /** * \u4ece\u7f13\u5b58\u4e2d\u83b7\u53d6\u79d2\u6740\u5546\u54c1\u7684\u4fe1\u606f */ itemKill = ( Item_kill ) redisTemplate . opsForValue (). get ( CACHE_PRE + \"[\" + id + \"]\" ); if ( itemKill != null ) { return itemKill ; } item_kill = itemKillMapper . selectByPrimaryKey ( id ); /** * \u5c06\u70ed\u70b9\u6570\u636e\u91cd\u65b0\u52a0\u5165\u5230\u7f13\u5b58\uff0c\u89e3\u51b3\u70ed\u70b9\u6570\u636e\u5931\u6548\u7684\u95ee\u9898 */ redisTemplate . opsForValue (). set ( CACHE_PRE + \"[\" + id + \"]\" , item_kill ); } } catch ( InterruptedException e ) { e . printStackTrace (); } finally { lock . unlock (); } return item_kill ; } 3\uff1a\u505a\u63a5\u53e3\u7684\u9650\u6d41\u964d\u7ea7\uff08\u4e3e\u4e2a\u4f8b\u5b50\uff09 NOTE: \u5176\u5b9e\u5c31\u662f\u62e6\u622a\u8bf7\u6c42 \u63a5\u53e3\u9650\u6d41\uff08\u8fd9\u6837\u7684\u8bdd\uff0c\u8fd9\u4e2a\u767b\u5f55\u7684\u63a5\u53e3\u5c31\u6700\u5927\u652f\u630120\u4e2a\u4eba\u540c\u65f6\u8bbf\u95ee\u4e86\uff09 @RestController public class logController { @PostMapping ( \"/login\" ) @Limit ( maxLimit = 20 ) public R login () { } } \u4e09\uff1a\u7f13\u5b58\u96ea\u5d29 \u4ecb\u7ecd\uff1a redis\u6302\u4e86\uff0c\u6240\u6709\u7684\u8bf7\u6c42\u90fd\u8fbe\u5230\u4e86\u6570\u636e\u5e93 \u89e3\u51b3 1\uff1a\u4f7f\u7528\u7f13\u5b58\u96c6\u7fa4\uff0c\u4fdd\u8bc1\u7f13\u5b58\u9ad8\u53ef\u7528 \u4f7f\u7528 Redis Sentinel \u548c Redis Cluster \u5b9e\u73b0\u9ad8\u53ef\u7528\u7f13\u5b58 2\uff1a\u4f7f\u7528Hystrix \u505a\u4e00\u4e9b\u9650\u6d41\u6216\u8005\u7194\u65ad\u7684\u515c\u5e95\u7b56\u7565","title":"Introduction"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#csdn","text":"","title":"csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#redis","text":"1\uff1a\u5ba2\u6237\u7aef\u53d1\u8d77\u8bf7\u6c42\u3002 2\uff1a\u5224\u65adredis\u4e2d\u662f\u5426\u6709\u6570\u636e\uff0c\u5982\u679c\u6709\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\uff0c\u6ca1\u6709\u5219\u8bf7\u6c42\u6570\u636e\u5e93,\u67e5\u51fa\u6570\u636e\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002","title":"\u4e00\u822c\u7528redis\u5e38\u89c4\u7684\u5199\u6cd5\uff1a"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#_1","text":"1\uff1a\u5982\u679c\u6b64\u65f6\u6709\u4eba\u6076\u610f\u7684\u653b\u51fb\u5462\uff1f\u53d1\u8d77\u51e0\u5341\u4ebf\u4e07\u6761redis\u548cmysql\u4e2d\u90fd\u4e0d\u5b58\u5728\u7684\u6570\u636e\uff0c\u8bf7\u6c42\u8bbf\u95ee\u4f60\u7684\u7f51\u7ad9\uff0c\u6570\u636e\u5e93\u4e0d\u5c31\u6302\u4e86\u3002","title":"\u5e26\u6765\u7684\u95ee\u9898\u4e00\uff1a\u7f13\u5b58\u7a7f\u900f"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#1","text":"redis\u4e2d\u6ca1\u6709\u6570\u636e\uff0c\u8bf7\u6c42\u5e03\u9686\u8fc7\u6ee4\u5668\u62e6\u622a\u8bf7\u6c42mysql\u548credis\u4e2d\u4e0d\u5b58\u5728\u7684key\u7684\u8bf7\u6c42\u3002\u5982\u679c\u5e03\u9686\u8fc7\u6ee4\u5668\u4e2d\u5bf9\u5e94\u7684key\uff0c\u5728\u8bf7\u6c42\u6570\u636e\u5e93\uff0c\u6ca1\u6709\u5219\u8fd4\u56de\u4e00\u4e2a\u975e\u6cd5\u8bbf\u95ee","title":"\u89e3\u51b3\u529e\u6cd51\uff1a\u4f7f\u7528\u5e03\u9686\u8fc7\u6ee4\u5668"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#2","text":"\u5728\u5ba2\u6237\u7aef\u8bf7\u6c42redis\u65f6\u53d1\u73b0\u6ca1\u6709key\uff0c\u63a5\u7740\u8bf7\u6c42mysql\u53d1\u73b0\u4e5f\u6ca1\u6709key\uff0c\u6b64\u65f6\u5c31\u628akey\u7f13\u5b58\u8d77\u6765\uff0cvalue\u8bbe\u7f6e\u4e3anull\u3002\uff08 \u7f3a\u70b9\u53ea\u9002\u5408\u5355\u4e00key\u591a\u6b21\u8bbf\u95ee\u6570\u636e\u5e93\u7684\u60c5\u51b5 \uff09","title":"\u89e3\u51b3\u529e\u6cd52\uff1a\u7f13\u5b58\u7a7a\u6570\u636e"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#_2","text":"\u5ba2\u6237\u7aef\u8bbf\u95ee\u6570\u636e\u7684\u65f6\u5019\uff0credis\u4e2d\u6ca1\u6709\u6570\u636e\uff0cmysql\u4e2d\u6709\u6570\u636e\uff0c\u76f8\u5f53\u4e8e\u76f4\u63a5\u8df3\u8fc7\u4e86redis\u3002","title":"\u5e26\u6765\u7684\u95ee\u9898\u4e8c\uff1a\u7f13\u5b58\u51fb\u7a7f"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#_3","text":"\u7528\u6237\u8bbf\u95ee\u8fd9\u6761\u6570\u636e\u7684\u65f6\u5019\uff0c\u70ed\u70b9\u6570\u636e\u8fc7\u671f\u65f6\u95f4\u521a\u597d\u5230\u4e86\u3002","title":"\u4e3a\u4f55\u4f1a\u53d1\u751f"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#_4","text":"1\uff1a\u5982\u679c\u6b64\u65f6\u8fd9\u6761\u6570\u636e\u5f88\u70ed\u95e8\uff0c\u79d2\u7ea7\u6709\u51e0\u5341\u4ebf\u4e07\u6b21\u7684\u8bbf\u95ee\u91cf\uff0c\u6570\u636e\u5e93\u4e0d\u5c31\u6302\u4e86\u3002","title":"\u95ee\u9898"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#_5","text":"1\uff1a\u8bbe\u7f6e\u70ed\u70b9\u6570\u636e\u6c38\u8fdc\u4e0d\u8fc7\u671f\u3002 2\uff1a\u53d1\u73b0redis\u4e2d\u6ca1\u6709\u6570\u636e\uff0c\u52a0\u5165\u5206\u5e03\u5f0f\u9501\uff08\u62e6\u622a\u8bf7\u6c42\uff09\uff0c\u63a5\u7740\u67e5\u8be2redis\uff0c\u67e5\u8be2\u6570\u636e\u5e93\u540e\uff0c\u628a\u8fd9\u6761\u6570\u636e\u91cd\u65b0\u52a0\u5165\u5230\u7f13\u5b58\uff0c\u91ca\u653e\u9501\uff0c\u4e4b\u540e\u5269\u4f59\u7684\u8bf7\u6c42\uff0c\u8bf7\u6c42redis\u65f6\u5c31\u53ef\u4ee5\u67e5\u5230\u6570\u636e\u4e86 public Item_kill getItemKill ( int id ) { /** * \u5e03\u9686\u8fc7\u6ee4\u5668\u89e3\u51b3\u7f13\u5b58\u7a7f\u900f */ if ( ! bloomFilter . mightContain ( id )) { log . warn ( \"\u975e\u6cd5\u79d2\u6740\u5546\u54c1id\" + id ); return null ; } /** * \u4ece\u7f13\u5b58\u4e2d\u83b7\u53d6\u79d2\u6740\u5546\u54c1\u7684\u4fe1\u606f\uff0c\u5982\u679c\u6b64\u65f6\u70ed\u70b9\u6570\u636e\u6070\u597d\u8fc7\u671f\u4e86\u5462\uff1f */ Item_kill itemKill = ( Item_kill ) redisTemplate . opsForValue (). get ( CACHE_PRE + \"[\" + id + \"]\" ); if ( itemKill != null ) { return itemKill ; } /** * \u5b9a\u4e49\u552f\u4e00\u6807\u8bc6key */ String key = new StringBuffer (). append ( id ). toString (); RLock lock = redissonClient . getLock ( key ); boolean ok = false ; Item_kill item_kill = null ; try { ok = lock . tryLock ( 30 , 10 , TimeUnit . SECONDS ); if ( ok ){ /** * \u4ece\u7f13\u5b58\u4e2d\u83b7\u53d6\u79d2\u6740\u5546\u54c1\u7684\u4fe1\u606f */ itemKill = ( Item_kill ) redisTemplate . opsForValue (). get ( CACHE_PRE + \"[\" + id + \"]\" ); if ( itemKill != null ) { return itemKill ; } item_kill = itemKillMapper . selectByPrimaryKey ( id ); /** * \u5c06\u70ed\u70b9\u6570\u636e\u91cd\u65b0\u52a0\u5165\u5230\u7f13\u5b58\uff0c\u89e3\u51b3\u70ed\u70b9\u6570\u636e\u5931\u6548\u7684\u95ee\u9898 */ redisTemplate . opsForValue (). set ( CACHE_PRE + \"[\" + id + \"]\" , item_kill ); } } catch ( InterruptedException e ) { e . printStackTrace (); } finally { lock . unlock (); } return item_kill ; } 3\uff1a\u505a\u63a5\u53e3\u7684\u9650\u6d41\u964d\u7ea7\uff08\u4e3e\u4e2a\u4f8b\u5b50\uff09 NOTE: \u5176\u5b9e\u5c31\u662f\u62e6\u622a\u8bf7\u6c42 \u63a5\u53e3\u9650\u6d41\uff08\u8fd9\u6837\u7684\u8bdd\uff0c\u8fd9\u4e2a\u767b\u5f55\u7684\u63a5\u53e3\u5c31\u6700\u5927\u652f\u630120\u4e2a\u4eba\u540c\u65f6\u8bbf\u95ee\u4e86\uff09 @RestController public class logController { @PostMapping ( \"/login\" ) @Limit ( maxLimit = 20 ) public R login () { } }","title":"\u89e3\u51b3"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#_6","text":"","title":"\u4e09\uff1a\u7f13\u5b58\u96ea\u5d29"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#_7","text":"redis\u6302\u4e86\uff0c\u6240\u6709\u7684\u8bf7\u6c42\u90fd\u8fbe\u5230\u4e86\u6570\u636e\u5e93","title":"\u4ecb\u7ecd\uff1a"},{"location":"Application/Redis-Cache/CSDN-%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF-%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/#_8","text":"1\uff1a\u4f7f\u7528\u7f13\u5b58\u96c6\u7fa4\uff0c\u4fdd\u8bc1\u7f13\u5b58\u9ad8\u53ef\u7528 \u4f7f\u7528 Redis Sentinel \u548c Redis Cluster \u5b9e\u73b0\u9ad8\u53ef\u7528\u7f13\u5b58 2\uff1a\u4f7f\u7528Hystrix \u505a\u4e00\u4e9b\u9650\u6d41\u6216\u8005\u7194\u65ad\u7684\u515c\u5e95\u7b56\u7565","title":"\u89e3\u51b3"},{"location":"Basic/","text":"Redis basic \u672c\u7ae0\u4ecb\u7ecdRedis\u7684\u57fa\u7840\u5185\u5bb9\uff1b","title":"Introduction"},{"location":"Basic/#redis#basic","text":"\u672c\u7ae0\u4ecb\u7ecdRedis\u7684\u57fa\u7840\u5185\u5bb9\uff1b","title":"Redis basic"},{"location":"Basic/Background-process/","text":"redis\u7684background process \u901a\u8fc7\u5168\u5c40\u68c0\u7d22 fork \u7cfb\u7edf\u8c03\u7528\uff0c\u4ee5\u4e0b\u662f\u7ed3\u679c\uff1a rdb background saving process rdb.c:rdbSaveBackground \u4f1afork\u4e00\u4e2aprocess\uff0c\u8fd9\u4e2aprocess\u6240\u6267\u884c\u7684\u5c31\u662fsave\uff0c struct redisServer \u7684 rdb_child_pid \u6210\u5458\u53d8\u91cf\u7528\u4e8e\u4fdd\u5b58\u8fd9\u4e2aprocess\u7684 pid \uff0c\u5728redis\u7684source code\u4e2d\u8fdb\u884c\u4e86\u5168\u5c40\u7684\u68c0\u7d22\uff0c\u53d1\u73b0\u4ee5\u4e0b\u5730\u65b9\u6d89\u53ca rdb.c:rdbSaveBackground \u7684\u8c03\u7528\uff1a server.c \u4e2d\u4f1a\u5b9a\u65f6\u89e6\u53d1background save replication.c \u4e2d startBgsaveForReplication \u4f1a\u8c03\u7528 rdbSaveBackground \u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a rdb.c:rdbSaveBackground \u53ea\u662f\u542f\u52a8background saving process\uff0c\u5b83\u5e76\u4e0dwait\u8fd9\u4e2aprocess\u7684finish\uff1b \u867d\u7136\u6709\u591a\u5904\u5bf9 rdb.c:rdbSaveBackground \u7684\u8c03\u7528\uff0c\u4f46\u662f\u4eceredis\u7684\u8bbe\u8ba1\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u540c\u4e00\u65f6\u95f4\u5b83\u4ec5\u4ec5\u5141\u8bb8\u4e00\u4e2abackground saving process\uff0c\u5e76\u4e0d\u5141\u8bb8\u540c\u65f6\u8fd0\u884c\u591a\u4e2abackground saving process\uff0c\u5b83\u7684 struct redisServer \u7684 rdb_child_pid \u6210\u5458\u53d8\u91cf\u662f\u4e00\u4e2a\u6807\u91cf\uff1b \u5176\u5b9e\u8fd9\u5c31\u5f15\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1abackground saving process\u6709\u591a\u79cd\u65b9\u5f0f\u88ab\u8c03\u7528\uff0c\u5e76\u4e14redis\u7684\u8bbe\u8ba1\u8fd8\u8981\u6c42\u552f\u4e00\u6027\uff0c\u4e92\u65a5\u6027\uff0c\u90a3\u4e48\u5b83\u5982\u4f55\u5f97\u77e5\u5f53\u524d\u6b63\u5728\u5141\u8bb8\u7684background saving process\u662f\u4e3a\u4f55\u800c\u8fd0\u884c\u7684\u5462\uff1f \u5176\u5b9e\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u4ece\u540e\u5f80\u524d\u6765\u8fdb\u884c\u63a8\uff0c\u6838\u5b9e\u5bf9background saving process\u7684wait\u5904\u7684\u4ee3\u7801\uff0c\u8be5\u5904\u4ee3\u7801\u4f4d\u4e8e server.c:serverCron \u4e2d\uff0c\u5b83\u6700\u7ec8\u4f1a\u8c03\u7528 rdb.c:backgroundSaveDoneHandler \uff0c\u5728 rdb.c:backgroundSaveDoneHandler \u4e2d\u4f1a\u8c03\u7528 replication.c:updateSlavesWaitingBgsave \u8fd9\u4e2a\u51fd\u6570\u7528\u4e8e\u66f4\u65b0\u7b49\u5f85background saving process\u5b8c\u6210\u7684slave\u3002 rdb backgroud saving process\u7684\u8bbe\u8ba1\u8fd8\u662f\u9075\u5faa\u7684event driven programming\uff0c\u5f53\u89e6\u53d1\u4e86rdb background saving process\u5f00\u59cb\u8fd0\u884c\u540e\uff0cmain process\u5e76\u4e0dblocking\uff0c\u800c\u662f\u8fdb\u884cevent loop\uff0c\u5728\u8fd9\u4e2aevent loop\u4e2d\uff0c\u9664\u4e86\u76d1\u89c6file event\uff0ctime event\uff0c\u8fd8\u76d1\u89c6background process\u7684\u72b6\u6001(\u901a\u8fc7poll\u7684\u65b9\u5f0f)\uff0c\u5982\u679cbackground process\u7684\u72b6\u6001\u53d8\u66f4\u4e86\uff0c\u4e5f\u4f1a\u89e6\u53d1\u54cd\u5e94\u7684handler\uff1b \u53c2\u89c1\u300a redis-code-analysis-handler.md \u300b\u3002 aof background saving process aof.c:rewriteAppendOnlyFileBackground \u4f1afork\u4e00\u4e2aprocess srcripting.c scripting.c:ldbStartSession sentinel.c sentinelRunPendingScripts server.c daemonize","title":"Introduction"},{"location":"Basic/Background-process/#redisbackground#process","text":"\u901a\u8fc7\u5168\u5c40\u68c0\u7d22 fork \u7cfb\u7edf\u8c03\u7528\uff0c\u4ee5\u4e0b\u662f\u7ed3\u679c\uff1a","title":"redis\u7684background process"},{"location":"Basic/Background-process/#rdb#background#saving#process","text":"rdb.c:rdbSaveBackground \u4f1afork\u4e00\u4e2aprocess\uff0c\u8fd9\u4e2aprocess\u6240\u6267\u884c\u7684\u5c31\u662fsave\uff0c struct redisServer \u7684 rdb_child_pid \u6210\u5458\u53d8\u91cf\u7528\u4e8e\u4fdd\u5b58\u8fd9\u4e2aprocess\u7684 pid \uff0c\u5728redis\u7684source code\u4e2d\u8fdb\u884c\u4e86\u5168\u5c40\u7684\u68c0\u7d22\uff0c\u53d1\u73b0\u4ee5\u4e0b\u5730\u65b9\u6d89\u53ca rdb.c:rdbSaveBackground \u7684\u8c03\u7528\uff1a server.c \u4e2d\u4f1a\u5b9a\u65f6\u89e6\u53d1background save replication.c \u4e2d startBgsaveForReplication \u4f1a\u8c03\u7528 rdbSaveBackground \u9700\u8981\u6ce8\u610f\u7684\u662f\uff1a rdb.c:rdbSaveBackground \u53ea\u662f\u542f\u52a8background saving process\uff0c\u5b83\u5e76\u4e0dwait\u8fd9\u4e2aprocess\u7684finish\uff1b \u867d\u7136\u6709\u591a\u5904\u5bf9 rdb.c:rdbSaveBackground \u7684\u8c03\u7528\uff0c\u4f46\u662f\u4eceredis\u7684\u8bbe\u8ba1\u89d2\u5ea6\u6765\u770b\uff0c\u5728\u540c\u4e00\u65f6\u95f4\u5b83\u4ec5\u4ec5\u5141\u8bb8\u4e00\u4e2abackground saving process\uff0c\u5e76\u4e0d\u5141\u8bb8\u540c\u65f6\u8fd0\u884c\u591a\u4e2abackground saving process\uff0c\u5b83\u7684 struct redisServer \u7684 rdb_child_pid \u6210\u5458\u53d8\u91cf\u662f\u4e00\u4e2a\u6807\u91cf\uff1b \u5176\u5b9e\u8fd9\u5c31\u5f15\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1abackground saving process\u6709\u591a\u79cd\u65b9\u5f0f\u88ab\u8c03\u7528\uff0c\u5e76\u4e14redis\u7684\u8bbe\u8ba1\u8fd8\u8981\u6c42\u552f\u4e00\u6027\uff0c\u4e92\u65a5\u6027\uff0c\u90a3\u4e48\u5b83\u5982\u4f55\u5f97\u77e5\u5f53\u524d\u6b63\u5728\u5141\u8bb8\u7684background saving process\u662f\u4e3a\u4f55\u800c\u8fd0\u884c\u7684\u5462\uff1f \u5176\u5b9e\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u4ece\u540e\u5f80\u524d\u6765\u8fdb\u884c\u63a8\uff0c\u6838\u5b9e\u5bf9background saving process\u7684wait\u5904\u7684\u4ee3\u7801\uff0c\u8be5\u5904\u4ee3\u7801\u4f4d\u4e8e server.c:serverCron \u4e2d\uff0c\u5b83\u6700\u7ec8\u4f1a\u8c03\u7528 rdb.c:backgroundSaveDoneHandler \uff0c\u5728 rdb.c:backgroundSaveDoneHandler \u4e2d\u4f1a\u8c03\u7528 replication.c:updateSlavesWaitingBgsave \u8fd9\u4e2a\u51fd\u6570\u7528\u4e8e\u66f4\u65b0\u7b49\u5f85background saving process\u5b8c\u6210\u7684slave\u3002 rdb backgroud saving process\u7684\u8bbe\u8ba1\u8fd8\u662f\u9075\u5faa\u7684event driven programming\uff0c\u5f53\u89e6\u53d1\u4e86rdb background saving process\u5f00\u59cb\u8fd0\u884c\u540e\uff0cmain process\u5e76\u4e0dblocking\uff0c\u800c\u662f\u8fdb\u884cevent loop\uff0c\u5728\u8fd9\u4e2aevent loop\u4e2d\uff0c\u9664\u4e86\u76d1\u89c6file event\uff0ctime event\uff0c\u8fd8\u76d1\u89c6background process\u7684\u72b6\u6001(\u901a\u8fc7poll\u7684\u65b9\u5f0f)\uff0c\u5982\u679cbackground process\u7684\u72b6\u6001\u53d8\u66f4\u4e86\uff0c\u4e5f\u4f1a\u89e6\u53d1\u54cd\u5e94\u7684handler\uff1b \u53c2\u89c1\u300a redis-code-analysis-handler.md \u300b\u3002","title":"rdb background saving process"},{"location":"Basic/Background-process/#aof#background#saving#process","text":"aof.c:rewriteAppendOnlyFileBackground \u4f1afork\u4e00\u4e2aprocess","title":"aof background saving process"},{"location":"Basic/Background-process/#srcriptingc","text":"scripting.c:ldbStartSession","title":"srcripting.c"},{"location":"Basic/Background-process/#sentinelc","text":"sentinelRunPendingScripts","title":"sentinel.c"},{"location":"Basic/Background-process/#serverc","text":"daemonize","title":"server.c"},{"location":"Basic/Data-structure/","text":"Data structure in Redis Redis\u662fdata structure server\uff0c\u672c\u7ae0\u5bf9Redis\u7684data structure\u8fdb\u884c\u4ecb\u7ecd\u3002","title":"Introduction"},{"location":"Basic/Data-structure/#data#structure#in#redis","text":"Redis\u662fdata structure server\uff0c\u672c\u7ae0\u5bf9Redis\u7684data structure\u8fdb\u884c\u4ecb\u7ecd\u3002","title":"Data structure in Redis"},{"location":"Basic/Data-structure/Dict/","text":"dict.h:struct dictEntry \u63cf\u8ff0\u952e\u503c\u5bf9 chain dict.h:struct dictht \u63cf\u8ff0hash table structure dict.h:dict \u63cf\u8ff0dict rehash","title":"Introduction"},{"location":"Basic/Data-structure/Dict/#dicthstruct#dictentry","text":"\u63cf\u8ff0\u952e\u503c\u5bf9","title":"dict.h:struct dictEntry"},{"location":"Basic/Data-structure/Dict/#chain","text":"","title":"chain"},{"location":"Basic/Data-structure/Dict/#dicthstruct#dictht","text":"\u63cf\u8ff0hash table structure","title":"dict.h:struct dictht"},{"location":"Basic/Data-structure/Dict/#dicthdict","text":"\u63cf\u8ff0dict","title":"dict.h:dict"},{"location":"Basic/Data-structure/Dict/#rehash","text":"","title":"rehash"},{"location":"Basic/Data-structure/Dict/Dict-usage/","text":"redis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0 \u7b2c\u56db\u7ae0 \u6570\u636e\u5e93 hash\u6570\u636e\u7c7b\u578b","title":"Dict-usage"},{"location":"Basic/Data-structure/Dict/Dict-usage/#redis","text":"","title":"redis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0"},{"location":"Basic/Data-structure/Dict/Dict-usage/#_1","text":"\u6570\u636e\u5e93 hash\u6570\u636e\u7c7b\u578b","title":"\u7b2c\u56db\u7ae0"},{"location":"Basic/Data-structure/Dict/Incremental-rehash/","text":"Incremental rehash Redis\u7684 dict \u91c7\u7528\u7684\u662fincremental rehash\u3002 \u5728\u300aRedis\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300bchapter \u300a4.5 \u6e10\u8fdb\u5f0frehash\u300b\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u4ecb\u7ecd\uff0c\u8fd9\u79cd\"\u5747\u644a\"\u7684\u601d\u60f3\u662f\u975e\u5e38\u503c\u5f97\u501f\u9274\u5b66\u4e60\u7684\uff0c\u6211\u5c06\u8fd9\u79cdtechnique\u603b\u7ed3\u4e3a: \"incremental-amortize-not-all-at-once\"\uff0c\u5373\"\u6e10\u8fdb\u5f0f\u5747\u644a\u800c\u4e0d\u662f\u4e00\u8e74\u800c\u5c31\"\u3002 TODO https://userpages.umbc.edu/~park/cs341.f18/projects/proj5.shtml","title":"Introduction"},{"location":"Basic/Data-structure/Dict/Incremental-rehash/#incremental#rehash","text":"Redis\u7684 dict \u91c7\u7528\u7684\u662fincremental rehash\u3002 \u5728\u300aRedis\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300bchapter \u300a4.5 \u6e10\u8fdb\u5f0frehash\u300b\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u4ecb\u7ecd\uff0c\u8fd9\u79cd\"\u5747\u644a\"\u7684\u601d\u60f3\u662f\u975e\u5e38\u503c\u5f97\u501f\u9274\u5b66\u4e60\u7684\uff0c\u6211\u5c06\u8fd9\u79cdtechnique\u603b\u7ed3\u4e3a: \"incremental-amortize-not-all-at-once\"\uff0c\u5373\"\u6e10\u8fdb\u5f0f\u5747\u644a\u800c\u4e0d\u662f\u4e00\u8e74\u800c\u5c31\"\u3002","title":"Incremental rehash"},{"location":"Basic/Data-structure/Dict/Incremental-rehash/#todo","text":"https://userpages.umbc.edu/~park/cs341.f18/projects/proj5.shtml","title":"TODO"},{"location":"Basic/Data-structure/Generic-double-linked-list/","text":"Generic double linked pointer typedef struct listNode { struct listNode * prev ; struct listNode * next ; void * value ; } listNode ; \u8282\u70b9\u7684\u6570\u636e\u7c7b\u578b\u662fpointer\uff0c\u56e0\u4e3aredis\u4e2d\u7684\u6240\u6709\u7684\u6570\u636e\u90fd\u662f\u6765\u81ea\u4e8e\u7f51\u7edc\uff0c\u90fd\u662f\u4ece\u63a5\u6536\u5230\u7684\u6570\u636enew\u51fa\u4e00\u7247\u7a7a\u95f4\u7684\uff1b \u5728 networking.c \u4e2d\u6709\u5982\u4e0b\u51fd\u6570\uff1a /* This function links the client to the global linked list of clients. * unlinkClient() does the opposite, among other things. */ void linkClient ( client * c ) { listAddNodeTail ( server . clients , c ); /* Note that we remember the linked list node where the client is stored, * this way removing the client in unlinkClient() will not require * a linear scan, but just a constant time operation. */ c -> client_list_node = listLast ( server . clients ); uint64_t id = htonu64 ( c -> id ); raxInsert ( server . clients_index ,( unsigned char * ) & id , sizeof ( id ), c , NULL ); } listAddNodeTail \u51fd\u6570\u7684\u539f\u578b\u5982\u4e0b\uff1a list * listAddNodeTail ( list * list , void * value ) \u663e\u7136\u5728 linkClient \u51fd\u6570\u4e2d\uff0c\u6d89\u53ca\u4e86\u4ece client * \u5230 void * \u7c7b\u578b\u7684\u8f6c\u6362","title":"Introduction"},{"location":"Basic/Data-structure/Generic-double-linked-list/#generic#double#linked#pointer","text":"typedef struct listNode { struct listNode * prev ; struct listNode * next ; void * value ; } listNode ; \u8282\u70b9\u7684\u6570\u636e\u7c7b\u578b\u662fpointer\uff0c\u56e0\u4e3aredis\u4e2d\u7684\u6240\u6709\u7684\u6570\u636e\u90fd\u662f\u6765\u81ea\u4e8e\u7f51\u7edc\uff0c\u90fd\u662f\u4ece\u63a5\u6536\u5230\u7684\u6570\u636enew\u51fa\u4e00\u7247\u7a7a\u95f4\u7684\uff1b \u5728 networking.c \u4e2d\u6709\u5982\u4e0b\u51fd\u6570\uff1a /* This function links the client to the global linked list of clients. * unlinkClient() does the opposite, among other things. */ void linkClient ( client * c ) { listAddNodeTail ( server . clients , c ); /* Note that we remember the linked list node where the client is stored, * this way removing the client in unlinkClient() will not require * a linear scan, but just a constant time operation. */ c -> client_list_node = listLast ( server . clients ); uint64_t id = htonu64 ( c -> id ); raxInsert ( server . clients_index ,( unsigned char * ) & id , sizeof ( id ), c , NULL ); } listAddNodeTail \u51fd\u6570\u7684\u539f\u578b\u5982\u4e0b\uff1a list * listAddNodeTail ( list * list , void * value ) \u663e\u7136\u5728 linkClient \u51fd\u6570\u4e2d\uff0c\u6d89\u53ca\u4e86\u4ece client * \u5230 void * \u7c7b\u578b\u7684\u8f6c\u6362","title":"Generic double linked pointer"},{"location":"Basic/Data-structure/Rax/","text":"rax redis\u7684rax\u5b9e\u73b0\u5176\u5b9e\u662f\u5373\u652f\u6301trie\u53c8\u652f\u6301radix tree\u7684\uff1b\u8ba4\u77e5\u5230\u8fd9\u4e00\u70b9\uff0c\u6211\u662f\u5148\u901a\u8fc7\u9605\u8bfb\u4e86trie\u7684\u5b9e\u73b0\u540e\uff0c\u518d\u9605\u8bfbredis rax\u7684\u5b9e\u73b0\u624d\u53d1\u73b0\u7684\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u4e0b\u662ftrie\u7684\u4e24\u79cd\u5b9e\u73b0\uff1a 1\u3001python trie\uff1a https://github.com/dengking/TheAlgorithms-Python/tree/master/data_structures/trie 2\u3001c trie\uff1a https://github.com/dengking/TheAlgorithms-C/tree/master/data_structures/trie python trie\u4e2d\uff0cnode\u7684\u5b9a\u4e49\u662f\u5982\u4e0b\u7684\uff1a class TrieNode : def __init__ ( self ): self . nodes = dict () # Mapping from char to TrieNode self . is_leaf = False \u5b83\u901a\u8fc7\u4f7f\u7528 dict \u6765\u5b9e\u73b0\u8282\u70b9\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5176\u5b9e\u53ef\u4ee5\u8ba4\u4e3a dict \u6240\u4fdd\u5b58\u7684\u662fedge\uff0c dict \u7684key\u5c31\u662fedge\u7684label\uff1b radish rax\u5728\u652f\u6301trie\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u4f7f\u7528\u7684\u5c31\u662f\u7c7b\u4f3c\u4e8e\u4e0a\u8ff0\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4ee5\u4e0b\u662f\u5b83\u7684\u6587\u6863\u4e2d\u7ed9\u51fa\u7684\uff1a /* Data layout is as follows: * * If node is not compressed we have 'size' bytes, one for each children * character, and 'size' raxNode pointers, point to each child node. * Note how the character is not stored in the children but in the * edge of the parents: * * [header iscompr=0][abc][a-ptr][b-ptr][c-ptr](value-ptr?) [abc][a-ptr][b-ptr][c-ptr] \u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e\u4e00\u4e2adict","title":"Introduction"},{"location":"Basic/Data-structure/Rax/#rax","text":"redis\u7684rax\u5b9e\u73b0\u5176\u5b9e\u662f\u5373\u652f\u6301trie\u53c8\u652f\u6301radix tree\u7684\uff1b\u8ba4\u77e5\u5230\u8fd9\u4e00\u70b9\uff0c\u6211\u662f\u5148\u901a\u8fc7\u9605\u8bfb\u4e86trie\u7684\u5b9e\u73b0\u540e\uff0c\u518d\u9605\u8bfbredis rax\u7684\u5b9e\u73b0\u624d\u53d1\u73b0\u7684\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u4e0b\u662ftrie\u7684\u4e24\u79cd\u5b9e\u73b0\uff1a 1\u3001python trie\uff1a https://github.com/dengking/TheAlgorithms-Python/tree/master/data_structures/trie 2\u3001c trie\uff1a https://github.com/dengking/TheAlgorithms-C/tree/master/data_structures/trie python trie\u4e2d\uff0cnode\u7684\u5b9a\u4e49\u662f\u5982\u4e0b\u7684\uff1a class TrieNode : def __init__ ( self ): self . nodes = dict () # Mapping from char to TrieNode self . is_leaf = False \u5b83\u901a\u8fc7\u4f7f\u7528 dict \u6765\u5b9e\u73b0\u8282\u70b9\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5176\u5b9e\u53ef\u4ee5\u8ba4\u4e3a dict \u6240\u4fdd\u5b58\u7684\u662fedge\uff0c dict \u7684key\u5c31\u662fedge\u7684label\uff1b radish rax\u5728\u652f\u6301trie\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u4f7f\u7528\u7684\u5c31\u662f\u7c7b\u4f3c\u4e8e\u4e0a\u8ff0\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4ee5\u4e0b\u662f\u5b83\u7684\u6587\u6863\u4e2d\u7ed9\u51fa\u7684\uff1a /* Data layout is as follows: * * If node is not compressed we have 'size' bytes, one for each children * character, and 'size' raxNode pointers, point to each child node. * Note how the character is not stored in the children but in the * edge of the parents: * * [header iscompr=0][abc][a-ptr][b-ptr][c-ptr](value-ptr?) [abc][a-ptr][b-ptr][c-ptr] \u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e\u4e00\u4e2adict","title":"rax"},{"location":"Basic/Data-structure/redis-doc-Redis-data-type/","text":"Redis doc redis An introduction to Redis data types and abstractions redis Data types","title":"Introduction"},{"location":"Basic/Data-structure/redis-doc-Redis-data-type/#redis#doc","text":"","title":"Redis doc"},{"location":"Basic/Data-structure/redis-doc-Redis-data-type/#redis#an#introduction#to#redis#data#types#and#abstractions","text":"","title":"redis An introduction to Redis data types and abstractions"},{"location":"Basic/Data-structure/redis-doc-Redis-data-type/#redis#data#types","text":"","title":"redis Data types"},{"location":"Basic/Event-library/","text":"Redis event library: ae narkive what does \"ae\" mean, short for? \u770b\u4e86\u5176\u4e2d\u7684\u56de\u7b54\uff0c\u6211\u89c9\u5f97\u53ef\u80fd\u6027\u66f4\u5927\u7684\u662f**async events** \u529f\u80fd/\u9700\u6c42 1\u3001\u9700\u8981\u76d1\u63a7\u3001\u8f6e\u8bad\u7684\u4e8b\u4ef6 2\u3001\u5f53\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0c\u9700\u8981\u6267\u884c\u7684callback 3\u3001\u80fd\u591fcross plateform(Unix-like OS) 4\u3001file event\u3001time event Implementation\u6982\u8ff0 1\u3001\u9700\u8981\u4f9d\u8d56\u4e8eOS\u63d0\u4f9b\u7684IO multiplex/polling system call\u6765\u540c\u65f6\u5bf9\u591a\u4e2afile descriptor\u8fdb\u884c\u76d1\u63a7\uff0c\u8fd9\u5728 Low-level-IO-multiplex \u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\u3002 2\u3001\u57fa\u4e8elow level IO multiplex\u4e2d\u63d0\u4f9b\u7684abstract interface\uff0cae\u57fa\u4e8e\u81ea\u8eab\u9700\u6c42\u5efa\u7acb\u8d77\u4e86\u6bd4\u8f83\u7075\u6d3b\u7684\u3001\u5f3a\u5927\u7684\u3001cross plateform\u7684event library\uff0c\u5e76\u63d0\u4f9b\u4e86high level interface\uff0cRedis\u7684\u5176\u4ed6\u90e8\u5206\u5c31\u662f\u4f7f\u7528\u7684high level interface\uff0c\u8fd9\u5728 High-level \u7ae0\u8282\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 3\u3001single thread\u540c\u65f6multiplex on time and file event Redis ae\u5145\u5206\u5229\u7528\u4e86\u5404\u79cdOS IO multiplex\u6240\u63d0\u4f9b\u7684\"multiplex on time and file event\"\u7279\u6027\uff0c\u5b9e\u73b0\u4e86multiplex on time and file event\uff0c\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1: a\u3001 voidcn \u628aredis\u6e90\u7801\u7684linux\u7f51\u7edc\u5e93\u63d0\u53d6\u51fa\u6765\uff0c\u81ea\u5df1\u5c01\u88c5\u6210\u901a\u7528\u5e93\u4f7f\u7528\uff08\u2605firecat\u63a8\u8350\u2605\uff09 Redis\u7f51\u7edc\u5e93\u662f\u4e00\u4e2a\u5355\u7ebf\u7a0bEPOLL\u6a21\u578b\uff0c\u4e5f\u5c31\u662f\u8bf4\u63a5\u6536\u8fde\u63a5\u548c\u5904\u7406\u8bfb\u5199\u8bf7\u6c42\u5305\u62ec**\u5b9a\u65f6\u5668\u4efb\u52a1**\u90fd\u88ab\u8fd9\u4e00\u4e2a\u7ebf\u7a0b\u5305\u63fd\uff0c\u771f\u7684\u662f\u53c8\u5f53\u7239\u53c8\u5f53\u5988\uff0c\u4f46\u662f\u6548\u7387\u4e00\u5b9a\u6bd4\u591a\u7ebf\u7a0b\u5dee\u5417\uff1f\u4e0d\u89c1\u5f97\u3002 \u5355\u7ebf\u7a0b\u7684\u597d\u5904\u6709\uff1a 1\uff1a\u907f\u514d\u7ebf\u7a0b\u5207\u6362\u5e26\u6765\u7684\u4e0a\u4e0b\u6587\u5207\u6362\u5f00\u9500\u3002 2\uff1a\u5355\u7ebf\u7a0b\u907f\u514d\u4e86\u9501\u7684\u4e89\u7528\u3002 3\uff1a\u5bf9\u4e8e\u4e00\u4e2a\u5185\u5b58\u578b\u6570\u636e\u5e93\uff0c\u5982\u679c\u4e0d\u8003\u8651\u6570\u636e\u6301\u4e45\u5316\uff0c\u4e5f\u5c31\u662f\u8bfb\u5199\u7269\u7406\u78c1\u76d8\uff0c\u4e0d\u4f1a\u6709\u963b\u585e\u64cd\u4f5c\uff0c\u5185\u5b58\u64cd\u4f5c\u662f\u975e\u5e38\u5feb\u7684\u3002 b\u3001 High-level \u7ae0\u8282\u4e2d\uff0c \u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u8bf4\u660e 5\u3001Redis ae\u662f\u5178\u578b\u7684\u91c7\u7528reactor pattern\uff0c\u53c2\u89c1: a\u3001zhihu I/O\u591a\u8def\u590d\u7528\u6280\u672f\uff08multiplexing\uff09\u662f\u4ec0\u4e48\uff1f Documentation \u5728\u300aredis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300b\u7684\u7b2c12\u7ae0\u4e8b\u4ef6\u4e2d\u5bf9redis\u7684event library\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 Redis official doc: redis.io Redis Internals documentation Redis Event Library Read event library to understand what an event library does and why its needed. Redis event library documents the implementation details of the event library used by Redis. NOTE: event library \u3001 Redis event library \u6536\u5f55\u4e8e Redis-event-library-office-doc \u7ae0\u8282\u4e2d redis-ae-application 1\u3001voidcn \u628aredis\u6e90\u7801\u7684linux\u7f51\u7edc\u5e93\u63d0\u53d6\u51fa\u6765\uff0c\u81ea\u5df1\u5c01\u88c5\u6210\u901a\u7528\u5e93\u4f7f\u7528\uff08\u2605firecat\u63a8\u8350\u2605\uff09","title":"Introduction"},{"location":"Basic/Event-library/#redis#event#library#ae","text":"","title":"Redis event library: ae"},{"location":"Basic/Event-library/#narkive#what#does#ae#mean#short#for","text":"\u770b\u4e86\u5176\u4e2d\u7684\u56de\u7b54\uff0c\u6211\u89c9\u5f97\u53ef\u80fd\u6027\u66f4\u5927\u7684\u662f**async events**","title":"narkive what does \"ae\" mean, short for?"},{"location":"Basic/Event-library/#_1","text":"1\u3001\u9700\u8981\u76d1\u63a7\u3001\u8f6e\u8bad\u7684\u4e8b\u4ef6 2\u3001\u5f53\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0c\u9700\u8981\u6267\u884c\u7684callback 3\u3001\u80fd\u591fcross plateform(Unix-like OS) 4\u3001file event\u3001time event","title":"\u529f\u80fd/\u9700\u6c42"},{"location":"Basic/Event-library/#implementation","text":"1\u3001\u9700\u8981\u4f9d\u8d56\u4e8eOS\u63d0\u4f9b\u7684IO multiplex/polling system call\u6765\u540c\u65f6\u5bf9\u591a\u4e2afile descriptor\u8fdb\u884c\u76d1\u63a7\uff0c\u8fd9\u5728 Low-level-IO-multiplex \u7ae0\u8282\u8fdb\u884c\u63cf\u8ff0\u3002 2\u3001\u57fa\u4e8elow level IO multiplex\u4e2d\u63d0\u4f9b\u7684abstract interface\uff0cae\u57fa\u4e8e\u81ea\u8eab\u9700\u6c42\u5efa\u7acb\u8d77\u4e86\u6bd4\u8f83\u7075\u6d3b\u7684\u3001\u5f3a\u5927\u7684\u3001cross plateform\u7684event library\uff0c\u5e76\u63d0\u4f9b\u4e86high level interface\uff0cRedis\u7684\u5176\u4ed6\u90e8\u5206\u5c31\u662f\u4f7f\u7528\u7684high level interface\uff0c\u8fd9\u5728 High-level \u7ae0\u8282\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 3\u3001single thread\u540c\u65f6multiplex on time and file event Redis ae\u5145\u5206\u5229\u7528\u4e86\u5404\u79cdOS IO multiplex\u6240\u63d0\u4f9b\u7684\"multiplex on time and file event\"\u7279\u6027\uff0c\u5b9e\u73b0\u4e86multiplex on time and file event\uff0c\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1: a\u3001 voidcn \u628aredis\u6e90\u7801\u7684linux\u7f51\u7edc\u5e93\u63d0\u53d6\u51fa\u6765\uff0c\u81ea\u5df1\u5c01\u88c5\u6210\u901a\u7528\u5e93\u4f7f\u7528\uff08\u2605firecat\u63a8\u8350\u2605\uff09 Redis\u7f51\u7edc\u5e93\u662f\u4e00\u4e2a\u5355\u7ebf\u7a0bEPOLL\u6a21\u578b\uff0c\u4e5f\u5c31\u662f\u8bf4\u63a5\u6536\u8fde\u63a5\u548c\u5904\u7406\u8bfb\u5199\u8bf7\u6c42\u5305\u62ec**\u5b9a\u65f6\u5668\u4efb\u52a1**\u90fd\u88ab\u8fd9\u4e00\u4e2a\u7ebf\u7a0b\u5305\u63fd\uff0c\u771f\u7684\u662f\u53c8\u5f53\u7239\u53c8\u5f53\u5988\uff0c\u4f46\u662f\u6548\u7387\u4e00\u5b9a\u6bd4\u591a\u7ebf\u7a0b\u5dee\u5417\uff1f\u4e0d\u89c1\u5f97\u3002 \u5355\u7ebf\u7a0b\u7684\u597d\u5904\u6709\uff1a 1\uff1a\u907f\u514d\u7ebf\u7a0b\u5207\u6362\u5e26\u6765\u7684\u4e0a\u4e0b\u6587\u5207\u6362\u5f00\u9500\u3002 2\uff1a\u5355\u7ebf\u7a0b\u907f\u514d\u4e86\u9501\u7684\u4e89\u7528\u3002 3\uff1a\u5bf9\u4e8e\u4e00\u4e2a\u5185\u5b58\u578b\u6570\u636e\u5e93\uff0c\u5982\u679c\u4e0d\u8003\u8651\u6570\u636e\u6301\u4e45\u5316\uff0c\u4e5f\u5c31\u662f\u8bfb\u5199\u7269\u7406\u78c1\u76d8\uff0c\u4e0d\u4f1a\u6709\u963b\u585e\u64cd\u4f5c\uff0c\u5185\u5b58\u64cd\u4f5c\u662f\u975e\u5e38\u5feb\u7684\u3002 b\u3001 High-level \u7ae0\u8282\u4e2d\uff0c \u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u8bf4\u660e 5\u3001Redis ae\u662f\u5178\u578b\u7684\u91c7\u7528reactor pattern\uff0c\u53c2\u89c1: a\u3001zhihu I/O\u591a\u8def\u590d\u7528\u6280\u672f\uff08multiplexing\uff09\u662f\u4ec0\u4e48\uff1f","title":"Implementation\u6982\u8ff0"},{"location":"Basic/Event-library/#documentation","text":"\u5728\u300aredis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300b\u7684\u7b2c12\u7ae0\u4e8b\u4ef6\u4e2d\u5bf9redis\u7684event library\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002","title":"Documentation"},{"location":"Basic/Event-library/#redis#official#doc#redisio#redis#internals#documentation","text":"","title":"Redis official doc: redis.io Redis Internals documentation"},{"location":"Basic/Event-library/#redis#event#library","text":"Read event library to understand what an event library does and why its needed. Redis event library documents the implementation details of the event library used by Redis. NOTE: event library \u3001 Redis event library \u6536\u5f55\u4e8e Redis-event-library-office-doc \u7ae0\u8282\u4e2d","title":"Redis Event Library"},{"location":"Basic/Event-library/#redis-ae-application","text":"1\u3001voidcn \u628aredis\u6e90\u7801\u7684linux\u7f51\u7edc\u5e93\u63d0\u53d6\u51fa\u6765\uff0c\u81ea\u5df1\u5c01\u88c5\u6210\u901a\u7528\u5e93\u4f7f\u7528\uff08\u2605firecat\u63a8\u8350\u2605\uff09","title":"redis-ae-application"},{"location":"Basic/Event-library/High-level/","text":"High level interface and implementation Source code ae.h ae.c Multiplex on time and file event \u53c2\u8003 aeProcessEvents \u7684source code\u53ef\u77e5: Note that we want call select() even if there are no file events to process as long as we want to process time events, in order to sleep until the next time event is ready to fire. \u67e5\u770b\u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff1a aeApiPoll \u662f\u5c06time even\u548cfile event\u6742\u7cc5\u8d77\u6765\u4e86\uff0c\u5b83\u80fd\u591f\u5728\u8fd9\u4e24\u79cd\u4e8b\u4ef6\u4e0a\u8fdb\u884cmultiplex\uff1b\u67e5\u770bAPUE\u768414.4.1 select and pselect Functions\u4e2d\u5173\u4e8eselect\u7cfb\u7edf\u8c03\u7528\u53ef\u77e5\uff0c\u8be5\u7cfb\u7edf\u8c03\u7528\u662f\u652f\u6301\u7528\u6237\u8bbe\u7f6e\u4e00\u4e2atimeout\u7684\uff1b\u7531\u4e8e\u6587\u4ef6\u4e8b\u4ef6\u662f\u7531OS\u8fdb\u884c\u7ba1\u7406\uff0c\u800c\u65f6\u95f4\u4e8b\u4ef6\u662f\u6709ae\u5e93\u81ea\u5df1\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u6240\u4ee5\u4e0b\u9762\u7684\u4ee3\u7801\u4f1a\u5148\u81ea\u5df1\u6765\u67e5\u627e\u51fa\u9700\u8981\u5904\u7406\u7684\u65f6\u95f4\u4e8b\u4ef6\u7684\u6700\u77ed\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u7136\u540e\u5c06\u8be5\u65f6\u95f4\u4f5c\u4e3aselect\u7cfb\u7edf\u8c03\u7528\u7684\u8d85\u65f6\u65f6\u95f4\uff1b ae data structure data structure struct aeFileEvent aeFileProc #define AE_FILE_EVENTS (1<<0) struct aeTimeEvent aeTimeProc #define AE_TIME_EVENTS (1<<1) Callback function type typedef void aeFileProc ( struct aeEventLoop * eventLoop , int fd , void * clientData , int mask ); typedef int aeTimeProc ( struct aeEventLoop * eventLoop , long long id , void * clientData ); typedef void aeEventFinalizerProc ( struct aeEventLoop * eventLoop , void * clientData ); typedef void aeBeforeSleepProc ( struct aeEventLoop * eventLoop ); struct aeFileEvent /* File event structure */ typedef struct aeFileEvent { int mask ; /* one of AE_(READABLE|WRITABLE|BARRIER) */ aeFileProc * rfileProc ; // \u51fd\u6570\u6307\u9488 aeFileProc * wfileProc ; // \u51fd\u6570\u6307\u9488 void * clientData ; } aeFileEvent ; 1\u3001 struct aeFileEvent \u63cf\u8ff0\u7684\u662f\u9700\u8981\u76d1\u63a7\u7684file event: 2\u3001\u6bcf\u4e2a struct aeFileEvent object\uff0c\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684file descriptor\u3001file\uff1b\u65e2\u7136\u662ffile\uff0c\u90a3\u4e48\u663e\u7136\u5bf9\u4e8efile event\uff0c\u5b83\u7684\u89e6\u53d1\u6761\u4ef6\u5c31\u662f: readable\u3001writeable\uff1b\u53ef\u4ee5\u901a\u8fc7\u6210\u5458 mask \u6765\u63a7\u5236\u5230\u5e95\u662f\u7531readable\u3001writeable\u6765\u89e6\u53d1 3\u3001\u663e\u7136\uff0c\u7531 struct aeFileEvent \u8bb0\u5f55\u5b83\u7684callback: rfileProc \u3001 wfileProc 4\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728 aeFileEvent \u4e2d\u5e76\u6ca1\u6709file descriptor\u6210\u5458\u53d8\u91cf\uff0c\u90a3\u5b83\u662f\u5982\u4f55\u548cfile descriptor\u8fdb\u884c\u7ba1\u7406\u7684\u5462\uff1f a\u3001ae\u662f\u5c06file descriptor\u4f5c\u4e3a\u7d22\u5f15\uff0c\u5c06 aeFileEvent \u5bf9\u8c61\u6309\u7167\u8be5\u7d22\u5f15\u5b58\u653e\u5982 struct aeEventLoop.events \u4e2d\uff1b b\u3001OS poll system call\u4f1a\u4fdd\u5b58file descriptor\uff0c\u5e76\u4e14\u5728\u89e6\u53d1\u7684\u65f6\u5019\uff0c\u4f1a\u4f20\u56de\u7528\u6237\u4f20\u5165\u7684file descriptor 5\u3001 mask \u7684\u529f\u80fd\u662f\u4ec0\u4e48\uff1f\u63cf\u8ff0\u7684\u662f\u76d1\u63a7\u7684\u662freadable\u3001writeable struct aeFiredEvent /* A fired event */ typedef struct aeFiredEvent { int fd ; int mask ; } aeFiredEvent ; 1\u3001\u63cf\u8ff0\u7684\u662f\u89e6\u53d1\u7684event 2\u3001\u53ef\u4ee5\u770b\u5230\uff0c\u5176\u4e2d\u5305\u542b\u4e86file descriptor struct aeEventLoop /* State of an event based program */ typedef struct aeEventLoop { int maxfd ; /* highest file descriptor currently registered */ int setsize ; /* max number of file descriptors tracked */ long long timeEventNextId ; time_t lastTime ; /* Used to detect system clock skew */ aeFileEvent * events ; /* Registered events */ // \u5728select \u4e2d\u4f1a\u4f7f\u7528\u5b83 aeFiredEvent * fired ; /* Fired events */ aeTimeEvent * timeEventHead ; int stop ; // \u662f\u5426\u505c\u6b62 void * apidata ; /* This is used for polling API specific data */ aeBeforeSleepProc * beforesleep ; aeBeforeSleepProc * aftersleep ; } aeEventLoop ; 1\u3001 ae \u66b4\u9732\u51fa\u7684 aeCreateFileEvent \u65b9\u6cd5\u5141\u8bb8\u7528\u6237\u6dfb\u52a0 FileEvent 2\u3001\u53ef\u4ee5\u770b\u5230\u5728 aeCreateFileEvent \u4e2d\u4f1a\u66f4\u65b0 maxfd \u6210\u5458\u53d8\u91cf\uff0c\u4ece\u800c\u4f7f maxfd \u59cb\u7ec8\u8bb0\u5f55\u6700\u5927\u7684file descriptor 3\u3001 apidata \u662f\u5178\u578b\u7684type erasure\uff0c\u7528\u4e8eC\u4e2d\u5b9e\u73b0generic programming High level interface aeMain void aeMain ( aeEventLoop * eventLoop ) { eventLoop -> stop = 0 ; while ( ! eventLoop -> stop ) { if ( eventLoop -> beforesleep != NULL ) eventLoop -> beforesleep ( eventLoop ); aeProcessEvents ( eventLoop , AE_ALL_EVENTS | AE_CALL_AFTER_SLEEP ); } } \u8fd9\u5c31\u662fevent loop\u3001main loop aeProcessEvents Process every pending time event , then every pending file event (that may be registered by time event callbacks just processed). Without special flags the function sleeps until some file event fires, or when the next time event occurs (if any). If flags is 0, the function does nothing and returns. If flags has AE_ALL_EVENTS set, all the kind of events are processed. If flags has AE_FILE_EVENTS set, file events are processed. If flags has AE_TIME_EVENTS set, time events are processed. If flags has AE_DONT_WAIT set, the function returns ASAP until all the events that's possible to process without to wait are processed( AE_DONT_WAIT \u7684\u542b\u4e49\u662fdo not wait\uff0c\u5b83\u7684\u542b\u4e49\u662f\u4f7fevent loop\u4e0d\u53bb\u7b49\u5f85). If flags has AE_CALL_AFTER_SLEEP set, the aftersleep callback is called. The function returns the number of events processed. int aeProcessEvents ( aeEventLoop * eventLoop , int flags ) { int processed = 0 , numevents ; /* Nothing to do? return ASAP */ if ( ! ( flags & AE_TIME_EVENTS ) && ! ( flags & AE_FILE_EVENTS )) return 0 ; /* Note that we want call select() even if there are no * file events to process as long as we want to process time * events, in order to sleep until the next time event is ready * to fire. */ // \u67e5\u770b\u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff1aaeApiPoll\u662f\u5c06time even\u548cfile event\u6742\u7cc5\u8d77\u6765\u4e86\uff0c\u5b83\u80fd\u591f\u5728\u8fd9\u4e24\u79cd\u4e8b\u4ef6\u4e0a\u8fdb\u884cmultiplex\uff1b\u67e5\u770bAPUE\u768414.4.1 select and pselect Functions\u4e2d\u5173\u4e8eselect\u7cfb\u7edf\u8c03\u7528\u53ef\u77e5\uff0c\u8be5\u7cfb\u7edf\u8c03\u7528\u662f\u652f\u6301\u7528\u6237\u8bbe\u7f6e\u4e00\u4e2atimeout\u7684\uff1b\u7531\u4e8e\u6587\u4ef6\u4e8b\u4ef6\u662f\u7531OS\u8fdb\u884c\u7ba1\u7406\uff0c\u800c\u65f6\u95f4\u4e8b\u4ef6\u662f\u6709ae\u5e93\u81ea\u5df1\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u6240\u4ee5\u4e0b\u9762\u7684\u4ee3\u7801\u4f1a\u5148\u81ea\u5df1\u6765\u67e5\u627e\u51fa\u9700\u8981\u5904\u7406\u7684\u65f6\u95f4\u4e8b\u4ef6\u7684\u6700\u77ed\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u7136\u540e\u5c06\u8be5\u65f6\u95f4\u4f5c\u4e3aselect\u7cfb\u7edf\u8c03\u7528\u7684\u8d85\u65f6\u65f6\u95f4\uff1b if ( eventLoop -> maxfd != -1 || (( flags & AE_TIME_EVENTS ) && ! ( flags & AE_DONT_WAIT ))) { // eventLoop->maxfd != -1 \u8868\u793a\u6709\u6587\u4ef6\u4e8b\u4ef6 // ((flags & AE_TIME_EVENTS) && !(flags & AE_DONT_WAIT)) \u8868\u793a\u8981\u5904\u7406\u65f6\u95f4\u4e8b\u4ef6 int j ; aeTimeEvent * shortest = NULL ; struct timeval tv , * tvp ; if ( flags & AE_TIME_EVENTS && ! ( flags & AE_DONT_WAIT )) shortest = aeSearchNearestTimer ( eventLoop ); if ( shortest ) { // \u6700\u8fd1\u9700\u8981\u5904\u7406\u7684\u4e8b\u4ef6 long now_sec , now_ms ; aeGetTime ( & now_sec , & now_ms ); tvp = & tv ; /* How many milliseconds we need to wait for the next * time event to fire? */ long long ms = ( shortest -> when_sec - now_sec ) * 1000 + shortest -> when_ms - now_ms ; if ( ms > 0 ) { tvp -> tv_sec = ms / 1000 ; tvp -> tv_usec = ( ms % 1000 ) * 1000 ; } else { tvp -> tv_sec = 0 ; tvp -> tv_usec = 0 ; } } else { // \u8868\u793a\u6ca1\u6709\u65f6\u95f4\u4e8b\u4ef6 /* If we have to check for events but need to return * ASAP because of AE_DONT_WAIT we need to set the timeout * to zero */ if ( flags & AE_DONT_WAIT ) { tv . tv_sec = tv . tv_usec = 0 ; tvp = & tv ; } else { /* Otherwise we can block */ tvp = NULL ; /* wait forever */ } } /* Call the multiplexing API, will return only on timeout or when * some event fires. */ numevents = aeApiPoll ( eventLoop , tvp ); /* After sleep callback. */ if ( eventLoop -> aftersleep != NULL && flags & AE_CALL_AFTER_SLEEP ) eventLoop -> aftersleep ( eventLoop ); for ( j = 0 ; j < numevents ; j ++ ) { aeFileEvent * fe = & eventLoop -> events [ eventLoop -> fired [ j ]. fd ]; int mask = eventLoop -> fired [ j ]. mask ; int fd = eventLoop -> fired [ j ]. fd ; int fired = 0 ; /* Number of events fired for current fd. */ /* Normally we execute the readable event first, and the writable * event laster. This is useful as sometimes we may be able * to serve the reply of a query immediately after processing the * query. * * However if AE_BARRIER is set in the mask, our application is * asking us to do the reverse: never fire the writable event * after the readable. In such a case, we invert the calls. * This is useful when, for instance, we want to do things * in the beforeSleep() hook, like fsynching a file to disk, * before replying to a client. */ int invert = fe -> mask & AE_BARRIER ; /* Note the \"fe->mask & mask & ...\" code: maybe an already * processed event removed an element that fired and we still * didn't processed, so we check if the event is still valid. * * Fire the readable event if the call sequence is not * inverted. */ if ( ! invert && fe -> mask & mask & AE_READABLE ) { fe -> rfileProc ( eventLoop , fd , fe -> clientData , mask ); fired ++ ; } /* Fire the writable event. */ if ( fe -> mask & mask & AE_WRITABLE ) { if ( ! fired || fe -> wfileProc != fe -> rfileProc ) { fe -> wfileProc ( eventLoop , fd , fe -> clientData , mask ); fired ++ ; } } /* If we have to invert the call, fire the readable event now * after the writable one. */ if ( invert && fe -> mask & mask & AE_READABLE ) { if ( ! fired || fe -> wfileProc != fe -> rfileProc ) { fe -> rfileProc ( eventLoop , fd , fe -> clientData , mask ); fired ++ ; } } processed ++ ; } } /* Check time events */ if ( flags & AE_TIME_EVENTS ) processed += processTimeEvents ( eventLoop ); return processed ; /* return the number of processed file/time events */ } \u770b\u4e86\u4e0a\u8ff0\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c aeApiPoll \u662f\u5c06time even\u548cfile event\u6742\u7cc5\u8d77\u6765\u4e86\uff0c\u5b83\u80fd\u591f\u5728\u8fd9\u4e24\u79cd\u4e8b\u4ef6\u4e0a\u8fdb\u884cmultiplex\u3002 aeCreateEventLoop aeEventLoop * aeCreateEventLoop ( int setsize ) { aeEventLoop * eventLoop ; int i ; if (( eventLoop = zmalloc ( sizeof ( * eventLoop ))) == NULL ) goto err ; eventLoop -> events = zmalloc ( sizeof ( aeFileEvent ) * setsize ); // \u9996\u5148\u5c31\u4e00\u6b21\u6027\u4e3a\u6240\u6709\u7684aeFileEvent\u5206\u914d\u7a7a\u95f4 eventLoop -> fired = zmalloc ( sizeof ( aeFiredEvent ) * setsize ); if ( eventLoop -> events == NULL || eventLoop -> fired == NULL ) goto err ; eventLoop -> setsize = setsize ; eventLoop -> lastTime = time ( NULL ); eventLoop -> timeEventHead = NULL ; // aeTimeEvent\u548caeFileEvent\u4e0d\u540c\uff0c\u4f7f\u7528double linked list\u6765\u4fdd\u5b58\u5b83 eventLoop -> timeEventNextId = 0 ; eventLoop -> stop = 0 ; eventLoop -> maxfd = -1 ; eventLoop -> beforesleep = NULL ; eventLoop -> aftersleep = NULL ; if ( aeApiCreate ( eventLoop ) == -1 ) goto err ; /* Events with mask == AE_NONE are not set. So let's initialize the * vector with it. */ for ( i = 0 ; i < setsize ; i ++ ) eventLoop -> events [ i ]. mask = AE_NONE ; return eventLoop ; err : if ( eventLoop ) { zfree ( eventLoop -> events ); zfree ( eventLoop -> fired ); zfree ( eventLoop ); } return NULL ; } \u5165\u53c2 setsize \u662f\u539f\u6765\u521d\u59cb\u5316 struct aeEventLoop \u7684 setsize \u5b57\u6bb5\u7684\uff0c\u5176\u542b\u4e49\u9700\u8981\u67e5\u770b ae.h \u4e2d\u5b9a\u4e49\u7684 struct aeEventLoop \uff0c\u5176\u4e2d\u5bf9\u5176\u8fdb\u884c\u4e86\u89e3\u91ca\uff1b\u5728redis\u4e2d\uff0c aeCreateEventLoop \u662f\u5728 server.c \u7684 initServer \u51fd\u6570\u4e2d\u8c03\u7528\uff0c\u4f20\u5165\u7684\u503c\u4e3a server.maxclients+CONFIG_FDSET_INCR aeCreateTimeEvent long long aeCreateTimeEvent ( aeEventLoop * eventLoop , long long milliseconds , aeTimeProc * proc , void * clientData , aeEventFinalizerProc * finalizerProc ) { long long id = eventLoop -> timeEventNextId ++ ; aeTimeEvent * te ; te = zmalloc ( sizeof ( * te )); if ( te == NULL ) return AE_ERR ; te -> id = id ; aeAddMillisecondsToNow ( milliseconds , & te -> when_sec , & te -> when_ms ); te -> timeProc = proc ; te -> finalizerProc = finalizerProc ; te -> clientData = clientData ; te -> prev = NULL ; te -> next = eventLoop -> timeEventHead ; if ( te -> next ) te -> next -> prev = te ; eventLoop -> timeEventHead = te ; // push_front return id ; } aeCreateFileEvent int aeCreateFileEvent ( aeEventLoop * eventLoop , int fd , int mask , aeFileProc * proc , void * clientData ) { if ( fd >= eventLoop -> setsize ) { errno = ERANGE ; return AE_ERR ; } aeFileEvent * fe = & eventLoop -> events [ fd ]; if ( aeApiAddEvent ( eventLoop , fd , mask ) == -1 ) return AE_ERR ; fe -> mask |= mask ; if ( mask & AE_READABLE ) fe -> rfileProc = proc ; if ( mask & AE_WRITABLE ) fe -> wfileProc = proc ; fe -> clientData = clientData ; if ( fd > eventLoop -> maxfd ) eventLoop -> maxfd = fd ; return AE_OK ; } 1\u3001ae\u662f\u5426\u652f\u6301\u5728event loop\u5df2\u7ecf\u542f\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u7ee7\u7eed\u6dfb\u52a0event\uff1f \u80af\u5b9a\u662f\u53ef\u4ee5\u7684\uff0c\u56e0\u4e3a: a\u3001accept\u540e\uff0c\u9700\u8981\u521b\u5efa\u65b0\u7684\u7684network connection\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u8fd9\u4e9bnetwork connection\u6dfb\u52a0\u5230ae\u4e2d\uff0cae\u540e\u7eed\u9700\u8981\u5bf9\u8fd9\u4e9bfile descriptor\u8fdb\u884c\u76d1\u63a7 aeSearchNearestTimer /* Search the first timer to fire. * This operation is useful to know how many time the select can be * put in sleep without to delay any event. * If there are no timers NULL is returned. * * Note that's O(N) since time events are unsorted. * Possible optimizations (not needed by Redis so far, but...): * 1) Insert the event in order, so that the nearest is just the head. * Much better but still insertion or deletion of timers is O(N). * 2) Use a skiplist to have this operation as O(1) and insertion as O(log(N)). */ static aeTimeEvent * aeSearchNearestTimer ( aeEventLoop * eventLoop ) { aeTimeEvent * te = eventLoop -> timeEventHead ; aeTimeEvent * nearest = NULL ; while ( te ) { // \u901a\u8fc7\u6253\u64c2\u53f0\u7684\u65b9\u5f0f\uff0c\u9009\u62e9\u51fa\u7684nearest\u662flinked list\u4e2d\uff0c\u65f6\u95f4\u503c\u6700\u5c0f\u7684\uff0c\u5373\u6700\u8fd1\u7684 if ( ! nearest || te -> when_sec < nearest -> when_sec || ( te -> when_sec == nearest -> when_sec && te -> when_ms < nearest -> when_ms )) nearest = te ; te = te -> next ; } return nearest ; }","title":"Introduction"},{"location":"Basic/Event-library/High-level/#high#level#interface#and#implementation","text":"","title":"High level interface and implementation"},{"location":"Basic/Event-library/High-level/#source#code","text":"ae.h ae.c","title":"Source code"},{"location":"Basic/Event-library/High-level/#multiplex#on#time#and#file#event","text":"\u53c2\u8003 aeProcessEvents \u7684source code\u53ef\u77e5: Note that we want call select() even if there are no file events to process as long as we want to process time events, in order to sleep until the next time event is ready to fire. \u67e5\u770b\u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff1a aeApiPoll \u662f\u5c06time even\u548cfile event\u6742\u7cc5\u8d77\u6765\u4e86\uff0c\u5b83\u80fd\u591f\u5728\u8fd9\u4e24\u79cd\u4e8b\u4ef6\u4e0a\u8fdb\u884cmultiplex\uff1b\u67e5\u770bAPUE\u768414.4.1 select and pselect Functions\u4e2d\u5173\u4e8eselect\u7cfb\u7edf\u8c03\u7528\u53ef\u77e5\uff0c\u8be5\u7cfb\u7edf\u8c03\u7528\u662f\u652f\u6301\u7528\u6237\u8bbe\u7f6e\u4e00\u4e2atimeout\u7684\uff1b\u7531\u4e8e\u6587\u4ef6\u4e8b\u4ef6\u662f\u7531OS\u8fdb\u884c\u7ba1\u7406\uff0c\u800c\u65f6\u95f4\u4e8b\u4ef6\u662f\u6709ae\u5e93\u81ea\u5df1\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u6240\u4ee5\u4e0b\u9762\u7684\u4ee3\u7801\u4f1a\u5148\u81ea\u5df1\u6765\u67e5\u627e\u51fa\u9700\u8981\u5904\u7406\u7684\u65f6\u95f4\u4e8b\u4ef6\u7684\u6700\u77ed\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u7136\u540e\u5c06\u8be5\u65f6\u95f4\u4f5c\u4e3aselect\u7cfb\u7edf\u8c03\u7528\u7684\u8d85\u65f6\u65f6\u95f4\uff1b","title":"Multiplex on time and file event"},{"location":"Basic/Event-library/High-level/#ae#data#structure","text":"data structure struct aeFileEvent aeFileProc #define AE_FILE_EVENTS (1<<0) struct aeTimeEvent aeTimeProc #define AE_TIME_EVENTS (1<<1)","title":"ae data structure"},{"location":"Basic/Event-library/High-level/#callback#function#type","text":"typedef void aeFileProc ( struct aeEventLoop * eventLoop , int fd , void * clientData , int mask ); typedef int aeTimeProc ( struct aeEventLoop * eventLoop , long long id , void * clientData ); typedef void aeEventFinalizerProc ( struct aeEventLoop * eventLoop , void * clientData ); typedef void aeBeforeSleepProc ( struct aeEventLoop * eventLoop );","title":"Callback function type"},{"location":"Basic/Event-library/High-level/#struct#aefileevent","text":"/* File event structure */ typedef struct aeFileEvent { int mask ; /* one of AE_(READABLE|WRITABLE|BARRIER) */ aeFileProc * rfileProc ; // \u51fd\u6570\u6307\u9488 aeFileProc * wfileProc ; // \u51fd\u6570\u6307\u9488 void * clientData ; } aeFileEvent ; 1\u3001 struct aeFileEvent \u63cf\u8ff0\u7684\u662f\u9700\u8981\u76d1\u63a7\u7684file event: 2\u3001\u6bcf\u4e2a struct aeFileEvent object\uff0c\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684file descriptor\u3001file\uff1b\u65e2\u7136\u662ffile\uff0c\u90a3\u4e48\u663e\u7136\u5bf9\u4e8efile event\uff0c\u5b83\u7684\u89e6\u53d1\u6761\u4ef6\u5c31\u662f: readable\u3001writeable\uff1b\u53ef\u4ee5\u901a\u8fc7\u6210\u5458 mask \u6765\u63a7\u5236\u5230\u5e95\u662f\u7531readable\u3001writeable\u6765\u89e6\u53d1 3\u3001\u663e\u7136\uff0c\u7531 struct aeFileEvent \u8bb0\u5f55\u5b83\u7684callback: rfileProc \u3001 wfileProc 4\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728 aeFileEvent \u4e2d\u5e76\u6ca1\u6709file descriptor\u6210\u5458\u53d8\u91cf\uff0c\u90a3\u5b83\u662f\u5982\u4f55\u548cfile descriptor\u8fdb\u884c\u7ba1\u7406\u7684\u5462\uff1f a\u3001ae\u662f\u5c06file descriptor\u4f5c\u4e3a\u7d22\u5f15\uff0c\u5c06 aeFileEvent \u5bf9\u8c61\u6309\u7167\u8be5\u7d22\u5f15\u5b58\u653e\u5982 struct aeEventLoop.events \u4e2d\uff1b b\u3001OS poll system call\u4f1a\u4fdd\u5b58file descriptor\uff0c\u5e76\u4e14\u5728\u89e6\u53d1\u7684\u65f6\u5019\uff0c\u4f1a\u4f20\u56de\u7528\u6237\u4f20\u5165\u7684file descriptor 5\u3001 mask \u7684\u529f\u80fd\u662f\u4ec0\u4e48\uff1f\u63cf\u8ff0\u7684\u662f\u76d1\u63a7\u7684\u662freadable\u3001writeable","title":"struct aeFileEvent"},{"location":"Basic/Event-library/High-level/#struct#aefiredevent","text":"/* A fired event */ typedef struct aeFiredEvent { int fd ; int mask ; } aeFiredEvent ; 1\u3001\u63cf\u8ff0\u7684\u662f\u89e6\u53d1\u7684event 2\u3001\u53ef\u4ee5\u770b\u5230\uff0c\u5176\u4e2d\u5305\u542b\u4e86file descriptor","title":"struct aeFiredEvent"},{"location":"Basic/Event-library/High-level/#struct#aeeventloop","text":"/* State of an event based program */ typedef struct aeEventLoop { int maxfd ; /* highest file descriptor currently registered */ int setsize ; /* max number of file descriptors tracked */ long long timeEventNextId ; time_t lastTime ; /* Used to detect system clock skew */ aeFileEvent * events ; /* Registered events */ // \u5728select \u4e2d\u4f1a\u4f7f\u7528\u5b83 aeFiredEvent * fired ; /* Fired events */ aeTimeEvent * timeEventHead ; int stop ; // \u662f\u5426\u505c\u6b62 void * apidata ; /* This is used for polling API specific data */ aeBeforeSleepProc * beforesleep ; aeBeforeSleepProc * aftersleep ; } aeEventLoop ; 1\u3001 ae \u66b4\u9732\u51fa\u7684 aeCreateFileEvent \u65b9\u6cd5\u5141\u8bb8\u7528\u6237\u6dfb\u52a0 FileEvent 2\u3001\u53ef\u4ee5\u770b\u5230\u5728 aeCreateFileEvent \u4e2d\u4f1a\u66f4\u65b0 maxfd \u6210\u5458\u53d8\u91cf\uff0c\u4ece\u800c\u4f7f maxfd \u59cb\u7ec8\u8bb0\u5f55\u6700\u5927\u7684file descriptor 3\u3001 apidata \u662f\u5178\u578b\u7684type erasure\uff0c\u7528\u4e8eC\u4e2d\u5b9e\u73b0generic programming","title":"struct aeEventLoop"},{"location":"Basic/Event-library/High-level/#high#level#interface","text":"","title":"High level interface"},{"location":"Basic/Event-library/High-level/#aemain","text":"void aeMain ( aeEventLoop * eventLoop ) { eventLoop -> stop = 0 ; while ( ! eventLoop -> stop ) { if ( eventLoop -> beforesleep != NULL ) eventLoop -> beforesleep ( eventLoop ); aeProcessEvents ( eventLoop , AE_ALL_EVENTS | AE_CALL_AFTER_SLEEP ); } } \u8fd9\u5c31\u662fevent loop\u3001main loop","title":"aeMain"},{"location":"Basic/Event-library/High-level/#aeprocessevents","text":"Process every pending time event , then every pending file event (that may be registered by time event callbacks just processed). Without special flags the function sleeps until some file event fires, or when the next time event occurs (if any). If flags is 0, the function does nothing and returns. If flags has AE_ALL_EVENTS set, all the kind of events are processed. If flags has AE_FILE_EVENTS set, file events are processed. If flags has AE_TIME_EVENTS set, time events are processed. If flags has AE_DONT_WAIT set, the function returns ASAP until all the events that's possible to process without to wait are processed( AE_DONT_WAIT \u7684\u542b\u4e49\u662fdo not wait\uff0c\u5b83\u7684\u542b\u4e49\u662f\u4f7fevent loop\u4e0d\u53bb\u7b49\u5f85). If flags has AE_CALL_AFTER_SLEEP set, the aftersleep callback is called. The function returns the number of events processed. int aeProcessEvents ( aeEventLoop * eventLoop , int flags ) { int processed = 0 , numevents ; /* Nothing to do? return ASAP */ if ( ! ( flags & AE_TIME_EVENTS ) && ! ( flags & AE_FILE_EVENTS )) return 0 ; /* Note that we want call select() even if there are no * file events to process as long as we want to process time * events, in order to sleep until the next time event is ready * to fire. */ // \u67e5\u770b\u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff1aaeApiPoll\u662f\u5c06time even\u548cfile event\u6742\u7cc5\u8d77\u6765\u4e86\uff0c\u5b83\u80fd\u591f\u5728\u8fd9\u4e24\u79cd\u4e8b\u4ef6\u4e0a\u8fdb\u884cmultiplex\uff1b\u67e5\u770bAPUE\u768414.4.1 select and pselect Functions\u4e2d\u5173\u4e8eselect\u7cfb\u7edf\u8c03\u7528\u53ef\u77e5\uff0c\u8be5\u7cfb\u7edf\u8c03\u7528\u662f\u652f\u6301\u7528\u6237\u8bbe\u7f6e\u4e00\u4e2atimeout\u7684\uff1b\u7531\u4e8e\u6587\u4ef6\u4e8b\u4ef6\u662f\u7531OS\u8fdb\u884c\u7ba1\u7406\uff0c\u800c\u65f6\u95f4\u4e8b\u4ef6\u662f\u6709ae\u5e93\u81ea\u5df1\u6765\u8fdb\u884c\u7ef4\u62a4\uff0c\u6240\u4ee5\u4e0b\u9762\u7684\u4ee3\u7801\u4f1a\u5148\u81ea\u5df1\u6765\u67e5\u627e\u51fa\u9700\u8981\u5904\u7406\u7684\u65f6\u95f4\u4e8b\u4ef6\u7684\u6700\u77ed\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u7136\u540e\u5c06\u8be5\u65f6\u95f4\u4f5c\u4e3aselect\u7cfb\u7edf\u8c03\u7528\u7684\u8d85\u65f6\u65f6\u95f4\uff1b if ( eventLoop -> maxfd != -1 || (( flags & AE_TIME_EVENTS ) && ! ( flags & AE_DONT_WAIT ))) { // eventLoop->maxfd != -1 \u8868\u793a\u6709\u6587\u4ef6\u4e8b\u4ef6 // ((flags & AE_TIME_EVENTS) && !(flags & AE_DONT_WAIT)) \u8868\u793a\u8981\u5904\u7406\u65f6\u95f4\u4e8b\u4ef6 int j ; aeTimeEvent * shortest = NULL ; struct timeval tv , * tvp ; if ( flags & AE_TIME_EVENTS && ! ( flags & AE_DONT_WAIT )) shortest = aeSearchNearestTimer ( eventLoop ); if ( shortest ) { // \u6700\u8fd1\u9700\u8981\u5904\u7406\u7684\u4e8b\u4ef6 long now_sec , now_ms ; aeGetTime ( & now_sec , & now_ms ); tvp = & tv ; /* How many milliseconds we need to wait for the next * time event to fire? */ long long ms = ( shortest -> when_sec - now_sec ) * 1000 + shortest -> when_ms - now_ms ; if ( ms > 0 ) { tvp -> tv_sec = ms / 1000 ; tvp -> tv_usec = ( ms % 1000 ) * 1000 ; } else { tvp -> tv_sec = 0 ; tvp -> tv_usec = 0 ; } } else { // \u8868\u793a\u6ca1\u6709\u65f6\u95f4\u4e8b\u4ef6 /* If we have to check for events but need to return * ASAP because of AE_DONT_WAIT we need to set the timeout * to zero */ if ( flags & AE_DONT_WAIT ) { tv . tv_sec = tv . tv_usec = 0 ; tvp = & tv ; } else { /* Otherwise we can block */ tvp = NULL ; /* wait forever */ } } /* Call the multiplexing API, will return only on timeout or when * some event fires. */ numevents = aeApiPoll ( eventLoop , tvp ); /* After sleep callback. */ if ( eventLoop -> aftersleep != NULL && flags & AE_CALL_AFTER_SLEEP ) eventLoop -> aftersleep ( eventLoop ); for ( j = 0 ; j < numevents ; j ++ ) { aeFileEvent * fe = & eventLoop -> events [ eventLoop -> fired [ j ]. fd ]; int mask = eventLoop -> fired [ j ]. mask ; int fd = eventLoop -> fired [ j ]. fd ; int fired = 0 ; /* Number of events fired for current fd. */ /* Normally we execute the readable event first, and the writable * event laster. This is useful as sometimes we may be able * to serve the reply of a query immediately after processing the * query. * * However if AE_BARRIER is set in the mask, our application is * asking us to do the reverse: never fire the writable event * after the readable. In such a case, we invert the calls. * This is useful when, for instance, we want to do things * in the beforeSleep() hook, like fsynching a file to disk, * before replying to a client. */ int invert = fe -> mask & AE_BARRIER ; /* Note the \"fe->mask & mask & ...\" code: maybe an already * processed event removed an element that fired and we still * didn't processed, so we check if the event is still valid. * * Fire the readable event if the call sequence is not * inverted. */ if ( ! invert && fe -> mask & mask & AE_READABLE ) { fe -> rfileProc ( eventLoop , fd , fe -> clientData , mask ); fired ++ ; } /* Fire the writable event. */ if ( fe -> mask & mask & AE_WRITABLE ) { if ( ! fired || fe -> wfileProc != fe -> rfileProc ) { fe -> wfileProc ( eventLoop , fd , fe -> clientData , mask ); fired ++ ; } } /* If we have to invert the call, fire the readable event now * after the writable one. */ if ( invert && fe -> mask & mask & AE_READABLE ) { if ( ! fired || fe -> wfileProc != fe -> rfileProc ) { fe -> rfileProc ( eventLoop , fd , fe -> clientData , mask ); fired ++ ; } } processed ++ ; } } /* Check time events */ if ( flags & AE_TIME_EVENTS ) processed += processTimeEvents ( eventLoop ); return processed ; /* return the number of processed file/time events */ } \u770b\u4e86\u4e0a\u8ff0\u4ee3\u7801\uff0c\u53ef\u4ee5\u53d1\u73b0\uff0c aeApiPoll \u662f\u5c06time even\u548cfile event\u6742\u7cc5\u8d77\u6765\u4e86\uff0c\u5b83\u80fd\u591f\u5728\u8fd9\u4e24\u79cd\u4e8b\u4ef6\u4e0a\u8fdb\u884cmultiplex\u3002","title":"aeProcessEvents"},{"location":"Basic/Event-library/High-level/#aecreateeventloop","text":"aeEventLoop * aeCreateEventLoop ( int setsize ) { aeEventLoop * eventLoop ; int i ; if (( eventLoop = zmalloc ( sizeof ( * eventLoop ))) == NULL ) goto err ; eventLoop -> events = zmalloc ( sizeof ( aeFileEvent ) * setsize ); // \u9996\u5148\u5c31\u4e00\u6b21\u6027\u4e3a\u6240\u6709\u7684aeFileEvent\u5206\u914d\u7a7a\u95f4 eventLoop -> fired = zmalloc ( sizeof ( aeFiredEvent ) * setsize ); if ( eventLoop -> events == NULL || eventLoop -> fired == NULL ) goto err ; eventLoop -> setsize = setsize ; eventLoop -> lastTime = time ( NULL ); eventLoop -> timeEventHead = NULL ; // aeTimeEvent\u548caeFileEvent\u4e0d\u540c\uff0c\u4f7f\u7528double linked list\u6765\u4fdd\u5b58\u5b83 eventLoop -> timeEventNextId = 0 ; eventLoop -> stop = 0 ; eventLoop -> maxfd = -1 ; eventLoop -> beforesleep = NULL ; eventLoop -> aftersleep = NULL ; if ( aeApiCreate ( eventLoop ) == -1 ) goto err ; /* Events with mask == AE_NONE are not set. So let's initialize the * vector with it. */ for ( i = 0 ; i < setsize ; i ++ ) eventLoop -> events [ i ]. mask = AE_NONE ; return eventLoop ; err : if ( eventLoop ) { zfree ( eventLoop -> events ); zfree ( eventLoop -> fired ); zfree ( eventLoop ); } return NULL ; } \u5165\u53c2 setsize \u662f\u539f\u6765\u521d\u59cb\u5316 struct aeEventLoop \u7684 setsize \u5b57\u6bb5\u7684\uff0c\u5176\u542b\u4e49\u9700\u8981\u67e5\u770b ae.h \u4e2d\u5b9a\u4e49\u7684 struct aeEventLoop \uff0c\u5176\u4e2d\u5bf9\u5176\u8fdb\u884c\u4e86\u89e3\u91ca\uff1b\u5728redis\u4e2d\uff0c aeCreateEventLoop \u662f\u5728 server.c \u7684 initServer \u51fd\u6570\u4e2d\u8c03\u7528\uff0c\u4f20\u5165\u7684\u503c\u4e3a server.maxclients+CONFIG_FDSET_INCR","title":"aeCreateEventLoop"},{"location":"Basic/Event-library/High-level/#aecreatetimeevent","text":"long long aeCreateTimeEvent ( aeEventLoop * eventLoop , long long milliseconds , aeTimeProc * proc , void * clientData , aeEventFinalizerProc * finalizerProc ) { long long id = eventLoop -> timeEventNextId ++ ; aeTimeEvent * te ; te = zmalloc ( sizeof ( * te )); if ( te == NULL ) return AE_ERR ; te -> id = id ; aeAddMillisecondsToNow ( milliseconds , & te -> when_sec , & te -> when_ms ); te -> timeProc = proc ; te -> finalizerProc = finalizerProc ; te -> clientData = clientData ; te -> prev = NULL ; te -> next = eventLoop -> timeEventHead ; if ( te -> next ) te -> next -> prev = te ; eventLoop -> timeEventHead = te ; // push_front return id ; }","title":"aeCreateTimeEvent"},{"location":"Basic/Event-library/High-level/#aecreatefileevent","text":"int aeCreateFileEvent ( aeEventLoop * eventLoop , int fd , int mask , aeFileProc * proc , void * clientData ) { if ( fd >= eventLoop -> setsize ) { errno = ERANGE ; return AE_ERR ; } aeFileEvent * fe = & eventLoop -> events [ fd ]; if ( aeApiAddEvent ( eventLoop , fd , mask ) == -1 ) return AE_ERR ; fe -> mask |= mask ; if ( mask & AE_READABLE ) fe -> rfileProc = proc ; if ( mask & AE_WRITABLE ) fe -> wfileProc = proc ; fe -> clientData = clientData ; if ( fd > eventLoop -> maxfd ) eventLoop -> maxfd = fd ; return AE_OK ; } 1\u3001ae\u662f\u5426\u652f\u6301\u5728event loop\u5df2\u7ecf\u542f\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u7ee7\u7eed\u6dfb\u52a0event\uff1f \u80af\u5b9a\u662f\u53ef\u4ee5\u7684\uff0c\u56e0\u4e3a: a\u3001accept\u540e\uff0c\u9700\u8981\u521b\u5efa\u65b0\u7684\u7684network connection\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u8fd9\u4e9bnetwork connection\u6dfb\u52a0\u5230ae\u4e2d\uff0cae\u540e\u7eed\u9700\u8981\u5bf9\u8fd9\u4e9bfile descriptor\u8fdb\u884c\u76d1\u63a7","title":"aeCreateFileEvent"},{"location":"Basic/Event-library/High-level/#aesearchnearesttimer","text":"/* Search the first timer to fire. * This operation is useful to know how many time the select can be * put in sleep without to delay any event. * If there are no timers NULL is returned. * * Note that's O(N) since time events are unsorted. * Possible optimizations (not needed by Redis so far, but...): * 1) Insert the event in order, so that the nearest is just the head. * Much better but still insertion or deletion of timers is O(N). * 2) Use a skiplist to have this operation as O(1) and insertion as O(log(N)). */ static aeTimeEvent * aeSearchNearestTimer ( aeEventLoop * eventLoop ) { aeTimeEvent * te = eventLoop -> timeEventHead ; aeTimeEvent * nearest = NULL ; while ( te ) { // \u901a\u8fc7\u6253\u64c2\u53f0\u7684\u65b9\u5f0f\uff0c\u9009\u62e9\u51fa\u7684nearest\u662flinked list\u4e2d\uff0c\u65f6\u95f4\u503c\u6700\u5c0f\u7684\uff0c\u5373\u6700\u8fd1\u7684 if ( ! nearest || te -> when_sec < nearest -> when_sec || ( te -> when_sec == nearest -> when_sec && te -> when_ms < nearest -> when_ms )) nearest = te ; te = te -> next ; } return nearest ; }","title":"aeSearchNearestTimer"},{"location":"Basic/Event-library/Low-level-IO-multiplex/","text":"Low level IO multiplex Cross-plateform and static polymorphism 1\u3001Redis ae\u662f\u80fd\u591f\u652f\u6301\u591a\u4e2aUnix-like OS\u7684\uff0c\u8fd9\u5c31\u662f\u672c\u8282\u6807\u9898\"cross-plateform\"\u7684\u542b\u4e49 2\u3001\u7531\u4e8e\u5404\u4e2aUnix-like OS\u7684IO multiplex/polling system call\u5e76\u4e0d\u7edf\u4e00\uff0c\u56e0\u6b64Redis ae\u5728\u5404\u79cdimplementation\u4e0a\u5efa\u7acb\u4e86\u4e00\u5c42\u62bd\u8c61\u5c42(interface) 3\u3001\u901a\u8fc7static polymorphism\u6765\u5b9e\u73b0interface\u5230implementation\u7684dispatch Static polymorphism Static polymorphism\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u5982\u4e0b: 1\u3001plateform detection macro 2\u3001void pointer type erasure(generic programming) /* State of an event based program */ typedef struct aeEventLoop { void * apidata ; /* This is used for polling API specific data */ }; 3\u3001\u5efa\u7acbabstraction(interface)\u3001\u4e0a\u5c42\u4f9d\u8d56\u4e8eabstraction\u800c\u4e0d\u662fimplementation Interface struct aeApiState \u4fdd\u5b58IO multiplex/polling object\u7684resource handle int aeApiCreate(aeEventLoop *eventLoop) \u91cd\u8981\u662f\u521b\u5efa struct aeApiState object\uff0c\u5e76\u5c06 struct aeApiState object\u5b58\u653e\u5230 eventLoop.apidata \u3002 \u5b83\u4f1a\u8c03\u7528OS IO multiplex/polling system call\u3002 int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) \u6dfb\u52a0\u4e00\u4e2aevent\u3002 int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp) \u987e\u540d\u601d\u4e49\uff0c\u8fdb\u884c\u8f6e\u8bad\u3002 Implementation redis\u7684ae\u7684IO multiplex\u5b9e\u73b0: 1\u3001 ae_evport.c 2\u3001 ae_epoll.c 3\u3001 ae_kqueue.c 4\u3001 ae_select.c \u5728 ae.c \u4e2d\u6709\u5982\u4e0b\u4ee3\u7801\uff1a /* Include the best multiplexing layer supported by this system. * The following should be ordered by performances, descending. */ #ifdef HAVE_EVPORT #include \"ae_evport.c\" #else #ifdef HAVE_EPOLL #include \"ae_epoll.c\" #else #ifdef HAVE_KQUEUE #include \"ae_kqueue.c\" #else #include \"ae_select.c\" #endif #endif #endif \u5728\u4ee5\u4e0a\u56db\u4e2a\u6587\u4ef6\u4e2d\uff0c\u90fd\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684API\u548cdata structure\uff1b\u8fd9\u662fc\u4e2d\u5b9e\u73b0\u7c7b\u4f3c\u4e8e\u9759\u6001\u591a\u6001\u6027\u7684\u4e00\u79cd\u65b9\u5f0f\uff1b epoll \u67e5\u770b\u4e86\u4e00\u4e0bsource code\uff0cRedis\u5728\u4f7f\u7528 epoll \u7684\u65f6\u5019\uff0c\u5e76\u6ca1\u6709\u6307\u5b9aedge trigger\u3001level trigger\uff0c\u6839\u636e\u4e0b\u9762\u6587\u7ae0\u7684\u5185\u5bb9\u53ef\u77e5\uff0cepoll\u9ed8\u8ba4\u662flevel trigger\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0cRedis\u4f7f\u7528\u7684\u662flevel trigger\u3002 1\u3001chinaunix epoll\u9ed8\u8ba4\u6a21\u5f0f(LT\uff09\uff0c\u51e0\u4e4e\u603b\u662f\u89e6\u53d1EPOLL-OUT\u4e8b\u4ef6\uff0c\u600e\u4e48\u89e3\u51b3\uff1f 2\u3001 epoll_ctl(2) Requests edge-triggered notification for the associated file descriptor. The default behavior for epoll is level-triggered. See epoll(7) for more detailed information about edge-triggered and level-triggered notification. struct aeApiState #include <sys/epoll.h> typedef struct aeApiState { int epfd ; // epoll instance\u7684file descriptor struct epoll_event * events ; // \u7528\u4e8eepoll_wait\u4e2d\uff0c\u7528\u4e8e\u63a5\u6536\u5df2\u7ecf\u51fa\u53d1\u7684\u4e8b\u4ef6 \u6bcf\u4e2afile descriptor\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684epoll_event\uff0c\u5b83\u662f\u4e00\u4e2adynamic array } aeApiState ; aeApiCreate static int aeApiCreate ( aeEventLoop * eventLoop ) { aeApiState * state = zmalloc ( sizeof ( aeApiState )); if ( ! state ) return -1 ; state -> events = zmalloc ( sizeof ( struct epoll_event ) * eventLoop -> setsize ); if ( ! state -> events ) { zfree ( state ); return -1 ; } state -> epfd = epoll_create ( 1024 ); /* 1024 is just a hint for the kernel */ if ( state -> epfd == -1 ) { zfree ( state -> events ); zfree ( state ); return -1 ; } eventLoop -> apidata = state ; return 0 ; } aeApiPoll static int aeApiPoll ( aeEventLoop * eventLoop , struct timeval * tvp ) { aeApiState * state = eventLoop -> apidata ; int retval , numevents = 0 ; retval = epoll_wait ( state -> epfd , state -> events , eventLoop -> setsize , tvp ? ( tvp -> tv_sec * 1000 + tvp -> tv_usec / 1000 ) : -1 ); if ( retval > 0 ) { int j ; numevents = retval ; for ( j = 0 ; j < numevents ; j ++ ) { int mask = 0 ; struct epoll_event * e = state -> events + j ; if ( e -> events & EPOLLIN ) mask |= AE_READABLE ; if ( e -> events & EPOLLOUT ) mask |= AE_WRITABLE ; if ( e -> events & EPOLLERR ) mask |= AE_WRITABLE | AE_READABLE ; if ( e -> events & EPOLLHUP ) mask |= AE_WRITABLE | AE_READABLE ; eventLoop -> fired [ j ]. fd = e -> data . fd ; eventLoop -> fired [ j ]. mask = mask ; } } return numevents ; } select \u4e0b\u9762\u4ee5 select \u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e(\u56e0\u4e3aAPUE\u4e2d\u6240\u4ecb\u7ecd\u7684\u5c31\u662f`select)\u3002 struct aeApiState #include <sys/select.h> #include <string.h> typedef struct aeApiState { fd_set rfds , wfds ; // rfds\u8868\u793a\u7684\u662fread file descriptor\uff0cwfds\u8868\u793a\u7684\u662fwrite file descriptor /* We need to have a copy of the fd sets as it's not safe to reuse * FD sets after select(). */ fd_set _rfds , _wfds ; } aeApiState ; aeApiCreate static int aeApiCreate ( aeEventLoop * eventLoop ) { aeApiState * state = zmalloc ( sizeof ( aeApiState )); if ( ! state ) return -1 ; FD_ZERO ( & state -> rfds ); FD_ZERO ( & state -> wfds ); eventLoop -> apidata = state ; //\u8bbe\u7f6eapidata\uff0c\u4ece\u4e0a\u9762\u53ef\u4ee5\uff0c\u76ee\u524d \u4ec5\u4ec5\u5173\u6ce8\u7684\u662fread\u548cwrite return 0 ; } aeApiPoll static int aeApiPoll ( aeEventLoop * eventLoop , struct timeval * tvp ) { aeApiState * state = eventLoop -> apidata ; int retval , j , numevents = 0 ; memcpy ( & state -> _rfds , & state -> rfds , sizeof ( fd_set )); memcpy ( & state -> _wfds , & state -> wfds , sizeof ( fd_set )); retval = select ( eventLoop -> maxfd + 1 , & state -> _rfds , & state -> _wfds , NULL , tvp ); if ( retval > 0 ) { for ( j = 0 ; j <= eventLoop -> maxfd ; j ++ ) { // process\u7684file descriptor\u662f\u4f9d\u6b21\u9012\u589e\u7684\uff0c\u6240\u4ee5\u6b64\u5904\u53ef\u4ee5\u4f7f\u7528for\u5faa\u73af\uff1b int mask = 0 ; aeFileEvent * fe = & eventLoop -> events [ j ]; if ( fe -> mask == AE_NONE ) continue ; if ( fe -> mask & AE_READABLE && FD_ISSET ( j , & state -> _rfds )) mask |= AE_READABLE ; if ( fe -> mask & AE_WRITABLE && FD_ISSET ( j , & state -> _wfds )) mask |= AE_WRITABLE ; eventLoop -> fired [ numevents ]. fd = j ; eventLoop -> fired [ numevents ]. mask = mask ; numevents ++ ; } } return numevents ; }","title":"Introduction"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#low#level#io#multiplex","text":"","title":"Low level IO multiplex"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#cross-plateform#and#static#polymorphism","text":"1\u3001Redis ae\u662f\u80fd\u591f\u652f\u6301\u591a\u4e2aUnix-like OS\u7684\uff0c\u8fd9\u5c31\u662f\u672c\u8282\u6807\u9898\"cross-plateform\"\u7684\u542b\u4e49 2\u3001\u7531\u4e8e\u5404\u4e2aUnix-like OS\u7684IO multiplex/polling system call\u5e76\u4e0d\u7edf\u4e00\uff0c\u56e0\u6b64Redis ae\u5728\u5404\u79cdimplementation\u4e0a\u5efa\u7acb\u4e86\u4e00\u5c42\u62bd\u8c61\u5c42(interface) 3\u3001\u901a\u8fc7static polymorphism\u6765\u5b9e\u73b0interface\u5230implementation\u7684dispatch","title":"Cross-plateform and static polymorphism"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#static#polymorphism","text":"Static polymorphism\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u5982\u4e0b: 1\u3001plateform detection macro 2\u3001void pointer type erasure(generic programming) /* State of an event based program */ typedef struct aeEventLoop { void * apidata ; /* This is used for polling API specific data */ }; 3\u3001\u5efa\u7acbabstraction(interface)\u3001\u4e0a\u5c42\u4f9d\u8d56\u4e8eabstraction\u800c\u4e0d\u662fimplementation","title":"Static polymorphism"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#interface","text":"","title":"Interface"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#struct#aeapistate","text":"\u4fdd\u5b58IO multiplex/polling object\u7684resource handle","title":"struct aeApiState"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#int#aeapicreateaeeventloop#eventloop","text":"\u91cd\u8981\u662f\u521b\u5efa struct aeApiState object\uff0c\u5e76\u5c06 struct aeApiState object\u5b58\u653e\u5230 eventLoop.apidata \u3002 \u5b83\u4f1a\u8c03\u7528OS IO multiplex/polling system call\u3002","title":"int aeApiCreate(aeEventLoop *eventLoop)"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#int#aeapiaddeventaeeventloop#eventloop#int#fd#int#mask","text":"\u6dfb\u52a0\u4e00\u4e2aevent\u3002","title":"int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask)"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#int#aeapipollaeeventloop#eventloop#struct#timeval#tvp","text":"\u987e\u540d\u601d\u4e49\uff0c\u8fdb\u884c\u8f6e\u8bad\u3002","title":"int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp)"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#implementation","text":"redis\u7684ae\u7684IO multiplex\u5b9e\u73b0: 1\u3001 ae_evport.c 2\u3001 ae_epoll.c 3\u3001 ae_kqueue.c 4\u3001 ae_select.c \u5728 ae.c \u4e2d\u6709\u5982\u4e0b\u4ee3\u7801\uff1a /* Include the best multiplexing layer supported by this system. * The following should be ordered by performances, descending. */ #ifdef HAVE_EVPORT #include \"ae_evport.c\" #else #ifdef HAVE_EPOLL #include \"ae_epoll.c\" #else #ifdef HAVE_KQUEUE #include \"ae_kqueue.c\" #else #include \"ae_select.c\" #endif #endif #endif \u5728\u4ee5\u4e0a\u56db\u4e2a\u6587\u4ef6\u4e2d\uff0c\u90fd\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684API\u548cdata structure\uff1b\u8fd9\u662fc\u4e2d\u5b9e\u73b0\u7c7b\u4f3c\u4e8e\u9759\u6001\u591a\u6001\u6027\u7684\u4e00\u79cd\u65b9\u5f0f\uff1b","title":"Implementation"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#epoll","text":"\u67e5\u770b\u4e86\u4e00\u4e0bsource code\uff0cRedis\u5728\u4f7f\u7528 epoll \u7684\u65f6\u5019\uff0c\u5e76\u6ca1\u6709\u6307\u5b9aedge trigger\u3001level trigger\uff0c\u6839\u636e\u4e0b\u9762\u6587\u7ae0\u7684\u5185\u5bb9\u53ef\u77e5\uff0cepoll\u9ed8\u8ba4\u662flevel trigger\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0cRedis\u4f7f\u7528\u7684\u662flevel trigger\u3002 1\u3001chinaunix epoll\u9ed8\u8ba4\u6a21\u5f0f(LT\uff09\uff0c\u51e0\u4e4e\u603b\u662f\u89e6\u53d1EPOLL-OUT\u4e8b\u4ef6\uff0c\u600e\u4e48\u89e3\u51b3\uff1f 2\u3001 epoll_ctl(2) Requests edge-triggered notification for the associated file descriptor. The default behavior for epoll is level-triggered. See epoll(7) for more detailed information about edge-triggered and level-triggered notification.","title":"epoll"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#struct#aeapistate_1","text":"#include <sys/epoll.h> typedef struct aeApiState { int epfd ; // epoll instance\u7684file descriptor struct epoll_event * events ; // \u7528\u4e8eepoll_wait\u4e2d\uff0c\u7528\u4e8e\u63a5\u6536\u5df2\u7ecf\u51fa\u53d1\u7684\u4e8b\u4ef6 \u6bcf\u4e2afile descriptor\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684epoll_event\uff0c\u5b83\u662f\u4e00\u4e2adynamic array } aeApiState ;","title":"struct aeApiState"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#aeapicreate","text":"static int aeApiCreate ( aeEventLoop * eventLoop ) { aeApiState * state = zmalloc ( sizeof ( aeApiState )); if ( ! state ) return -1 ; state -> events = zmalloc ( sizeof ( struct epoll_event ) * eventLoop -> setsize ); if ( ! state -> events ) { zfree ( state ); return -1 ; } state -> epfd = epoll_create ( 1024 ); /* 1024 is just a hint for the kernel */ if ( state -> epfd == -1 ) { zfree ( state -> events ); zfree ( state ); return -1 ; } eventLoop -> apidata = state ; return 0 ; }","title":"aeApiCreate"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#aeapipoll","text":"static int aeApiPoll ( aeEventLoop * eventLoop , struct timeval * tvp ) { aeApiState * state = eventLoop -> apidata ; int retval , numevents = 0 ; retval = epoll_wait ( state -> epfd , state -> events , eventLoop -> setsize , tvp ? ( tvp -> tv_sec * 1000 + tvp -> tv_usec / 1000 ) : -1 ); if ( retval > 0 ) { int j ; numevents = retval ; for ( j = 0 ; j < numevents ; j ++ ) { int mask = 0 ; struct epoll_event * e = state -> events + j ; if ( e -> events & EPOLLIN ) mask |= AE_READABLE ; if ( e -> events & EPOLLOUT ) mask |= AE_WRITABLE ; if ( e -> events & EPOLLERR ) mask |= AE_WRITABLE | AE_READABLE ; if ( e -> events & EPOLLHUP ) mask |= AE_WRITABLE | AE_READABLE ; eventLoop -> fired [ j ]. fd = e -> data . fd ; eventLoop -> fired [ j ]. mask = mask ; } } return numevents ; }","title":"aeApiPoll"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#select","text":"\u4e0b\u9762\u4ee5 select \u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e(\u56e0\u4e3aAPUE\u4e2d\u6240\u4ecb\u7ecd\u7684\u5c31\u662f`select)\u3002","title":"select"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#struct#aeapistate_2","text":"#include <sys/select.h> #include <string.h> typedef struct aeApiState { fd_set rfds , wfds ; // rfds\u8868\u793a\u7684\u662fread file descriptor\uff0cwfds\u8868\u793a\u7684\u662fwrite file descriptor /* We need to have a copy of the fd sets as it's not safe to reuse * FD sets after select(). */ fd_set _rfds , _wfds ; } aeApiState ;","title":"struct aeApiState"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#aeapicreate_1","text":"static int aeApiCreate ( aeEventLoop * eventLoop ) { aeApiState * state = zmalloc ( sizeof ( aeApiState )); if ( ! state ) return -1 ; FD_ZERO ( & state -> rfds ); FD_ZERO ( & state -> wfds ); eventLoop -> apidata = state ; //\u8bbe\u7f6eapidata\uff0c\u4ece\u4e0a\u9762\u53ef\u4ee5\uff0c\u76ee\u524d \u4ec5\u4ec5\u5173\u6ce8\u7684\u662fread\u548cwrite return 0 ; }","title":"aeApiCreate"},{"location":"Basic/Event-library/Low-level-IO-multiplex/#aeapipoll_1","text":"static int aeApiPoll ( aeEventLoop * eventLoop , struct timeval * tvp ) { aeApiState * state = eventLoop -> apidata ; int retval , j , numevents = 0 ; memcpy ( & state -> _rfds , & state -> rfds , sizeof ( fd_set )); memcpy ( & state -> _wfds , & state -> wfds , sizeof ( fd_set )); retval = select ( eventLoop -> maxfd + 1 , & state -> _rfds , & state -> _wfds , NULL , tvp ); if ( retval > 0 ) { for ( j = 0 ; j <= eventLoop -> maxfd ; j ++ ) { // process\u7684file descriptor\u662f\u4f9d\u6b21\u9012\u589e\u7684\uff0c\u6240\u4ee5\u6b64\u5904\u53ef\u4ee5\u4f7f\u7528for\u5faa\u73af\uff1b int mask = 0 ; aeFileEvent * fe = & eventLoop -> events [ j ]; if ( fe -> mask == AE_NONE ) continue ; if ( fe -> mask & AE_READABLE && FD_ISSET ( j , & state -> _rfds )) mask |= AE_READABLE ; if ( fe -> mask & AE_WRITABLE && FD_ISSET ( j , & state -> _wfds )) mask |= AE_WRITABLE ; eventLoop -> fired [ numevents ]. fd = j ; eventLoop -> fired [ numevents ]. mask = mask ; numevents ++ ; } } return numevents ; }","title":"aeApiPoll"},{"location":"Basic/Event-library/official-doc/","text":"Redis event library \u672c\u6587\u9605\u8bfbRedis\u5b98\u65b9\u6587\u6863\u4e2d\u7ed9\u51fa\u7684\u5173\u4e8eRedis event library\u7684\u6587\u6863\u3002 Redis event library NOTE: 1\u3001\u8fd9\u4e00\u6bb5\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48Redis\u9700\u8981event library 2\u3001\u540c\u65f6\u8fd9\u6bb5\u8bdd\u89e3\u91ca\u6e05\u695a\u4e86\u5982\u4e0b\u5185\u5bb9: a\u3001 network server b\u3001 non-blocking Why is an Event Library needed at all? Let us figure it out through a series of Q&As. Q: What do you expect a network server to be doing all the time? A: Watch for inbound connections on the port its listening and accept them. Q: Calling accept yields a descriptor. What do I do with it? A: Save the descriptor and do a non-blocking read/write operation on it. Q: Why does the read/write have to be non-blocking ? A: If the file operation ( even a socket in Unix is a file ) is blocking how could the server for example accept other connection requests when its blocked in a file I/O operation. NOTE: \u89e3\u91ca\u6e05\u695a\u4e86\u4e3a\u4ec0\u4e48\u9700\u8981non-blocking Q: I guess I have to do many such non-blocking operations on the socket to see when it's ready. Am I right? A: Yes. That is what an event library does for you. Now you get it. Q: How do Event Libraries do what they do? A: They use the operating system's polling facility along with timers . NOTE : 1\u3001\u4e0a\u8ff0\u7684**timer**\u8ba9\u6211\u60f3\u5230\u4e86\u6211\u6700\u8fd1\u5728programming\u7684\u65f6\u5019\u4f7f\u7528 wait_until \u6765\u66ff\u4ee3 sleep 2\u3001 polling \u7684\u94fe\u63a5\u7684\u5185\u5bb9\u5df2\u7ecf\u6d88\u5931\u4e86\uff0c\u5176\u5b9e\u5c31\u662fIO multiplex Q: So are there any open source event libraries that do what you just described? A: Yes. libevent and libev are two such event libraries that I can recall off the top of my head. Q: Does Redis use such open source event libraries for handling socket I/O? A: No. For various reasons Redis uses its own event library. redis Redis Event Library NOTE: 1\u3001\u8fd9\u7bc7\u6587\u7ae0\u91cd\u8981\u8bb2\u89e3Redis event library\u7684implementation Redis implements its own event library. The event library is implemented in ae.c . The best way to understand how the Redis event library works is to understand how Redis uses it. Event Loop Initialization NOTE: 1\u3001redis\u7684event loop\u4e2d\u5171\u5b9a\u4e49\u4e86\u4e24\u7c7bevent\uff1a aeFileEvent \uff0c aeTimeEvent 2\u3001\u9700\u8981\u5904\u7406listening port\uff0c\u4ece\u800c\u5efa\u7acb\u65b0\u8fde\u63a5\uff1b\u9700\u8981\u5904\u7406\u5df2\u7ecf\u5efa\u7acb\u7684\u8fde\u63a5\u4e0a\u7684event 3\u3001\u4ece\u540e\u9762\u7684\u63cf\u7ed8\u6765\u770b\uff0cEvent Loop Initialization\u4e3b\u8981\u505a\u4e86\u5982\u4e0b\u5185\u5bb9: a\u3001 aeCreateEventLoop : \u521b\u5efaevent loop \u5bf9\u8c61 b\u3001 aeCreateTimeEvent : \u6ce8\u518c serverCron c\u3001 aeCreateFileEvent : \u6ce8\u518c listening port initServer function defined in redis.c initializes the numerous fields of the redisServer structure variable. One such field is the Redis event loop el : NOTE: 1\u3001\u610f\u601d\u662f: redisServer structure \u6709\u4e00\u4e2afiled: Redis event loop el 2\u3001 initServer \u51fd\u6570\u987e\u540d\u601d\u4e49: \u521d\u59cb\u5316 server\u7684 aeEventLoop * el aeEventLoop NOTE: 1\u3001\u6240\u6709\u9700\u8981\u6301\u7eed\u76d1\u63a7\u3001poll\u7684\u5bf9\u8c61\uff0c\u90fd\u9700\u8981\u5c06\u5b83\u7684descriptor\u6dfb\u52a0\u90fd server.el ( struct aeEventLoop ) initServer initializes server.el field by calling aeCreateEventLoop defined in ae.c . The definition of aeEventLoop is below: typedef struct aeEventLoop { int maxfd ; long long timeEventNextId ; aeFileEvent events [ AE_SETSIZE ]; /* Registered events */ aeFiredEvent fired [ AE_SETSIZE ]; /* Fired events */ aeTimeEvent * timeEventHead ; int stop ; void * apidata ; /* This is used for polling API specific data */ aeBeforeSleepProc * beforesleep ; } aeEventLoop ; aeCreateEventLoop aeCreateEventLoop first malloc s aeEventLoop structure then calls ae_epoll.c:aeApiCreate . aeApiCreate NOTE: 1\u3001\u4e0b\u9762\u7684\u8bb2\u8ff0\u91cd\u8981\u662f\u4ee5Linux epoll\u4e3a\u4f8b\u7684 2\u3001Linux system call\u4e00\u822c\u90fd\u662f\u8fd4\u56de\u7684resource handle\uff0c\u4e0b\u9762\u7684\" epfd that holds the epoll file descriptor returned by a call from epoll_create \"\u5c31\u662f\u5178\u578b\u7684\u8fd9\u79cd\u6a21\u5f0f\uff0c\u8fd9\u91cc\u7684\" epoll file descriptor \"\u5c31\u662fresource handle 3\u3001 aeApiCreate \u5c31\u662f\u8c03\u7528OS\u7684IO multiplex system call\uff0c\u4ee5Linux OS\u4e3a\u4f8b\uff0c\u5c31\u662f epoll_create 4\u3001\" aeApiCreate malloc s aeApiState \" \u8fd4\u56de\u7684 aeApiState object\u5b58\u653e\u5728\u54ea\u91cc\uff1f\u4ece\u540e\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u662f\u653e\u5728 server.el \u4e2d\uff0c\u90a3\u5bf9\u5e94\u7684\u662f\u54ea\u4e2afield\u5462\uff1f\u662f apidata \u5b57\u6bb5 aeApiCreate malloc s aeApiState that has two fields - epfd that holds the epoll file descriptor returned by a call from epoll_create and events that is of type struct epoll_event define by the Linux epoll library. The use of the events field will be described later. Next is ae.c:aeCreateTimeEvent . But before that initServer call anet.c:anetTcpServer that creates and returns a listening descriptor . The descriptor listens on port 6379 by default. The returned listening descriptor is stored in server.fd field. NOTE: listening descriptor\u5b58\u653e\u5728 server.fd \uff0c\u800c\u4e0d\u662f\u548c epoll file descriptor\u4e00\u8d77\u5b58\u653e\u5728 epfd \u4e2d\uff1b aeCreateTimeEvent aeCreateTimeEvent accepts the following as parameters: 1\u3001 eventLoop : This is server.el in redis.c 2\u3001milliseconds: The number of milliseconds from the current time after which the timer expires. 3\u3001 proc : Function pointer. Stores the address of the function that has to be called after the timer expires. 4\u3001 clientData : Mostly NULL . 5\u3001 finalizerProc : Pointer to the function that has to be called before the timed event is removed from the list of timed events. initServer calls aeCreateTimeEvent to add a timed event to timeEventHead field of server.el . timeEventHead is a pointer to a list of such timed events. The call to aeCreateTimeEvent from redis.c:initServer function is given below: aeCreateTimeEvent ( server . el /*eventLoop*/ , 1 /*milliseconds*/ , serverCron /*proc*/ , NULL /*clientData*/ , NULL /*finalizerProc*/ ); redis.c:serverCron performs many operations that helps keep Redis running properly. aeCreateFileEvent The essence of aeCreateFileEvent function is to execute epoll_ctl system call which adds a watch for EPOLLIN event on the listening descriptor create by anetTcpServer and associate it with the epoll descriptor created by a call to aeCreateEventLoop . Following is an explanation of what precisely aeCreateFileEvent does when called from redis.c:initServer . initServer passes the following arguments to aeCreateFileEvent : 1\u3001 server.el : The event loop created by aeCreateEventLoop . The epoll descriptor is got from server.el . 2\u3001 server.fd : The listening descriptor that also serves as an index to access the relevant file event structure from the eventLoop->events table and store extra information like the callback function. 3\u3001 AE_READABLE : Signifies that server.fd has to be watched for EPOLLIN event. 4\u3001 acceptHandler : The function that has to be executed when the event being watched for is ready. This function pointer is stored in eventLoop->events[server.fd]->rfileProc . This completes the initialization of Redis event loop. Event Loop Processing ae.c:aeMain called from redis.c:main does the job of processing the event loop that is initialized in the previous phase. ae.c:aeMain calls ae.c:aeProcessEvents in a while loop that processes pending time and file events . aeProcessEvents ae.c:aeProcessEvents looks for the time event that will be pending in the smallest amount of time by calling ae.c:aeSearchNearestTimer on the event loop . In our case there is only one timer event in the event loop that was created by ae.c:aeCreateTimeEvent . Remember, that timer event created by aeCreateTimeEvent has by now probably elapsed because it had a expiry(\u8fc7\u671f) time of one millisecond. Since, the timer has already expired the seconds and microseconds fields of the tvp timeval structure variable is initialized to zero. aeApiPoll The tvp structure variable along with the event loop variable is passed to ae_epoll.c:aeApiPoll . aeApiPoll functions does a epoll_wait on the epoll descriptor and populates the eventLoop->fired table with the details: 1\u3001 fd : The descriptor that is now ready to do a read/write operation depending on the mask value. 2\u3001 mask : The read/write event that can now be performed on the corresponding descriptor. aeApiPoll returns the number of such file events ready for operation. Now to put things in context, if any client has requested for a connection then aeApiPoll would have noticed it and populated the eventLoop->fired table with an entry of the descriptor being the listening descriptor and mask being AE_READABLE . Now, aeProcessEvents calls the redis.c:acceptHandler registered as the callback. acceptHandler executes accept on the listening descriptor returning a connected descriptor with the client. redis.c:createClient adds a file event on the connected descriptor through a call to ae.c:aeCreateFileEvent like below: if ( aeCreateFileEvent ( server . el , c -> fd , AE_READABLE , readQueryFromClient , c ) == AE_ERR ) { freeClient ( c ); return NULL ; } c is the redisClient structure variable and c->fd is the connected descriptor. Next the ae.c:aeProcessEvent calls ae.c:processTimeEvents processTimeEvents ae.processTimeEvents iterates over list of time events starting at eventLoop->timeEventHead . For every timed event that has elapsed processTimeEvents calls the registered callback. In this case it calls the only timed event callback registered, that is, redis.c:serverCron . The callback returns the time in milliseconds after which the callback must be called again. This change is recorded via a call to ae.c:aeAddMilliSeconds and will be handled on the next iteration of ae.c:aeMain while loop. That's all.","title":"Introduction"},{"location":"Basic/Event-library/official-doc/#redis#event#library","text":"\u672c\u6587\u9605\u8bfbRedis\u5b98\u65b9\u6587\u6863\u4e2d\u7ed9\u51fa\u7684\u5173\u4e8eRedis event library\u7684\u6587\u6863\u3002","title":"Redis event library"},{"location":"Basic/Event-library/official-doc/#redis#event#library_1","text":"NOTE: 1\u3001\u8fd9\u4e00\u6bb5\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48Redis\u9700\u8981event library 2\u3001\u540c\u65f6\u8fd9\u6bb5\u8bdd\u89e3\u91ca\u6e05\u695a\u4e86\u5982\u4e0b\u5185\u5bb9: a\u3001 network server b\u3001 non-blocking","title":"Redis event library"},{"location":"Basic/Event-library/official-doc/#why#is#an#event#library#needed#at#all","text":"Let us figure it out through a series of Q&As. Q: What do you expect a network server to be doing all the time? A: Watch for inbound connections on the port its listening and accept them. Q: Calling accept yields a descriptor. What do I do with it? A: Save the descriptor and do a non-blocking read/write operation on it. Q: Why does the read/write have to be non-blocking ? A: If the file operation ( even a socket in Unix is a file ) is blocking how could the server for example accept other connection requests when its blocked in a file I/O operation. NOTE: \u89e3\u91ca\u6e05\u695a\u4e86\u4e3a\u4ec0\u4e48\u9700\u8981non-blocking Q: I guess I have to do many such non-blocking operations on the socket to see when it's ready. Am I right? A: Yes. That is what an event library does for you. Now you get it. Q: How do Event Libraries do what they do? A: They use the operating system's polling facility along with timers . NOTE : 1\u3001\u4e0a\u8ff0\u7684**timer**\u8ba9\u6211\u60f3\u5230\u4e86\u6211\u6700\u8fd1\u5728programming\u7684\u65f6\u5019\u4f7f\u7528 wait_until \u6765\u66ff\u4ee3 sleep 2\u3001 polling \u7684\u94fe\u63a5\u7684\u5185\u5bb9\u5df2\u7ecf\u6d88\u5931\u4e86\uff0c\u5176\u5b9e\u5c31\u662fIO multiplex Q: So are there any open source event libraries that do what you just described? A: Yes. libevent and libev are two such event libraries that I can recall off the top of my head. Q: Does Redis use such open source event libraries for handling socket I/O? A: No. For various reasons Redis uses its own event library.","title":"Why is an Event Library needed at all?"},{"location":"Basic/Event-library/official-doc/#redis#redis#event#library","text":"NOTE: 1\u3001\u8fd9\u7bc7\u6587\u7ae0\u91cd\u8981\u8bb2\u89e3Redis event library\u7684implementation Redis implements its own event library. The event library is implemented in ae.c . The best way to understand how the Redis event library works is to understand how Redis uses it.","title":"redis Redis Event Library"},{"location":"Basic/Event-library/official-doc/#event#loop#initialization","text":"NOTE: 1\u3001redis\u7684event loop\u4e2d\u5171\u5b9a\u4e49\u4e86\u4e24\u7c7bevent\uff1a aeFileEvent \uff0c aeTimeEvent 2\u3001\u9700\u8981\u5904\u7406listening port\uff0c\u4ece\u800c\u5efa\u7acb\u65b0\u8fde\u63a5\uff1b\u9700\u8981\u5904\u7406\u5df2\u7ecf\u5efa\u7acb\u7684\u8fde\u63a5\u4e0a\u7684event 3\u3001\u4ece\u540e\u9762\u7684\u63cf\u7ed8\u6765\u770b\uff0cEvent Loop Initialization\u4e3b\u8981\u505a\u4e86\u5982\u4e0b\u5185\u5bb9: a\u3001 aeCreateEventLoop : \u521b\u5efaevent loop \u5bf9\u8c61 b\u3001 aeCreateTimeEvent : \u6ce8\u518c serverCron c\u3001 aeCreateFileEvent : \u6ce8\u518c listening port initServer function defined in redis.c initializes the numerous fields of the redisServer structure variable. One such field is the Redis event loop el : NOTE: 1\u3001\u610f\u601d\u662f: redisServer structure \u6709\u4e00\u4e2afiled: Redis event loop el 2\u3001 initServer \u51fd\u6570\u987e\u540d\u601d\u4e49: \u521d\u59cb\u5316 server\u7684 aeEventLoop * el","title":"Event Loop Initialization"},{"location":"Basic/Event-library/official-doc/#aeeventloop","text":"NOTE: 1\u3001\u6240\u6709\u9700\u8981\u6301\u7eed\u76d1\u63a7\u3001poll\u7684\u5bf9\u8c61\uff0c\u90fd\u9700\u8981\u5c06\u5b83\u7684descriptor\u6dfb\u52a0\u90fd server.el ( struct aeEventLoop ) initServer initializes server.el field by calling aeCreateEventLoop defined in ae.c . The definition of aeEventLoop is below: typedef struct aeEventLoop { int maxfd ; long long timeEventNextId ; aeFileEvent events [ AE_SETSIZE ]; /* Registered events */ aeFiredEvent fired [ AE_SETSIZE ]; /* Fired events */ aeTimeEvent * timeEventHead ; int stop ; void * apidata ; /* This is used for polling API specific data */ aeBeforeSleepProc * beforesleep ; } aeEventLoop ;","title":"aeEventLoop"},{"location":"Basic/Event-library/official-doc/#aecreateeventloop","text":"aeCreateEventLoop first malloc s aeEventLoop structure then calls ae_epoll.c:aeApiCreate .","title":"aeCreateEventLoop"},{"location":"Basic/Event-library/official-doc/#aeapicreate","text":"NOTE: 1\u3001\u4e0b\u9762\u7684\u8bb2\u8ff0\u91cd\u8981\u662f\u4ee5Linux epoll\u4e3a\u4f8b\u7684 2\u3001Linux system call\u4e00\u822c\u90fd\u662f\u8fd4\u56de\u7684resource handle\uff0c\u4e0b\u9762\u7684\" epfd that holds the epoll file descriptor returned by a call from epoll_create \"\u5c31\u662f\u5178\u578b\u7684\u8fd9\u79cd\u6a21\u5f0f\uff0c\u8fd9\u91cc\u7684\" epoll file descriptor \"\u5c31\u662fresource handle 3\u3001 aeApiCreate \u5c31\u662f\u8c03\u7528OS\u7684IO multiplex system call\uff0c\u4ee5Linux OS\u4e3a\u4f8b\uff0c\u5c31\u662f epoll_create 4\u3001\" aeApiCreate malloc s aeApiState \" \u8fd4\u56de\u7684 aeApiState object\u5b58\u653e\u5728\u54ea\u91cc\uff1f\u4ece\u540e\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u662f\u653e\u5728 server.el \u4e2d\uff0c\u90a3\u5bf9\u5e94\u7684\u662f\u54ea\u4e2afield\u5462\uff1f\u662f apidata \u5b57\u6bb5 aeApiCreate malloc s aeApiState that has two fields - epfd that holds the epoll file descriptor returned by a call from epoll_create and events that is of type struct epoll_event define by the Linux epoll library. The use of the events field will be described later. Next is ae.c:aeCreateTimeEvent . But before that initServer call anet.c:anetTcpServer that creates and returns a listening descriptor . The descriptor listens on port 6379 by default. The returned listening descriptor is stored in server.fd field. NOTE: listening descriptor\u5b58\u653e\u5728 server.fd \uff0c\u800c\u4e0d\u662f\u548c epoll file descriptor\u4e00\u8d77\u5b58\u653e\u5728 epfd \u4e2d\uff1b","title":"aeApiCreate"},{"location":"Basic/Event-library/official-doc/#aecreatetimeevent","text":"aeCreateTimeEvent accepts the following as parameters: 1\u3001 eventLoop : This is server.el in redis.c 2\u3001milliseconds: The number of milliseconds from the current time after which the timer expires. 3\u3001 proc : Function pointer. Stores the address of the function that has to be called after the timer expires. 4\u3001 clientData : Mostly NULL . 5\u3001 finalizerProc : Pointer to the function that has to be called before the timed event is removed from the list of timed events. initServer calls aeCreateTimeEvent to add a timed event to timeEventHead field of server.el . timeEventHead is a pointer to a list of such timed events. The call to aeCreateTimeEvent from redis.c:initServer function is given below: aeCreateTimeEvent ( server . el /*eventLoop*/ , 1 /*milliseconds*/ , serverCron /*proc*/ , NULL /*clientData*/ , NULL /*finalizerProc*/ ); redis.c:serverCron performs many operations that helps keep Redis running properly.","title":"aeCreateTimeEvent"},{"location":"Basic/Event-library/official-doc/#aecreatefileevent","text":"The essence of aeCreateFileEvent function is to execute epoll_ctl system call which adds a watch for EPOLLIN event on the listening descriptor create by anetTcpServer and associate it with the epoll descriptor created by a call to aeCreateEventLoop . Following is an explanation of what precisely aeCreateFileEvent does when called from redis.c:initServer . initServer passes the following arguments to aeCreateFileEvent : 1\u3001 server.el : The event loop created by aeCreateEventLoop . The epoll descriptor is got from server.el . 2\u3001 server.fd : The listening descriptor that also serves as an index to access the relevant file event structure from the eventLoop->events table and store extra information like the callback function. 3\u3001 AE_READABLE : Signifies that server.fd has to be watched for EPOLLIN event. 4\u3001 acceptHandler : The function that has to be executed when the event being watched for is ready. This function pointer is stored in eventLoop->events[server.fd]->rfileProc . This completes the initialization of Redis event loop.","title":"aeCreateFileEvent"},{"location":"Basic/Event-library/official-doc/#event#loop#processing","text":"ae.c:aeMain called from redis.c:main does the job of processing the event loop that is initialized in the previous phase. ae.c:aeMain calls ae.c:aeProcessEvents in a while loop that processes pending time and file events .","title":"Event Loop Processing"},{"location":"Basic/Event-library/official-doc/#aeprocessevents","text":"ae.c:aeProcessEvents looks for the time event that will be pending in the smallest amount of time by calling ae.c:aeSearchNearestTimer on the event loop . In our case there is only one timer event in the event loop that was created by ae.c:aeCreateTimeEvent . Remember, that timer event created by aeCreateTimeEvent has by now probably elapsed because it had a expiry(\u8fc7\u671f) time of one millisecond. Since, the timer has already expired the seconds and microseconds fields of the tvp timeval structure variable is initialized to zero.","title":"aeProcessEvents"},{"location":"Basic/Event-library/official-doc/#aeapipoll","text":"The tvp structure variable along with the event loop variable is passed to ae_epoll.c:aeApiPoll . aeApiPoll functions does a epoll_wait on the epoll descriptor and populates the eventLoop->fired table with the details: 1\u3001 fd : The descriptor that is now ready to do a read/write operation depending on the mask value. 2\u3001 mask : The read/write event that can now be performed on the corresponding descriptor. aeApiPoll returns the number of such file events ready for operation. Now to put things in context, if any client has requested for a connection then aeApiPoll would have noticed it and populated the eventLoop->fired table with an entry of the descriptor being the listening descriptor and mask being AE_READABLE . Now, aeProcessEvents calls the redis.c:acceptHandler registered as the callback. acceptHandler executes accept on the listening descriptor returning a connected descriptor with the client. redis.c:createClient adds a file event on the connected descriptor through a call to ae.c:aeCreateFileEvent like below: if ( aeCreateFileEvent ( server . el , c -> fd , AE_READABLE , readQueryFromClient , c ) == AE_ERR ) { freeClient ( c ); return NULL ; } c is the redisClient structure variable and c->fd is the connected descriptor. Next the ae.c:aeProcessEvent calls ae.c:processTimeEvents","title":"aeApiPoll"},{"location":"Basic/Event-library/official-doc/#processtimeevents","text":"ae.processTimeEvents iterates over list of time events starting at eventLoop->timeEventHead . For every timed event that has elapsed processTimeEvents calls the registered callback. In this case it calls the only timed event callback registered, that is, redis.c:serverCron . The callback returns the time in milliseconds after which the callback must be called again. This change is recorded via a call to ae.c:aeAddMilliSeconds and will be handled on the next iteration of ae.c:aeMain while loop. That's all.","title":"processTimeEvents"},{"location":"Basic/Memory-management/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbaRedis\u7684memory management\u3002","title":"Introduction"},{"location":"Basic/Memory-management/#_1","text":"\u672c\u7ae0\u8ba8\u8bbaRedis\u7684memory management\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Basic/Memory-management/TODO/Redis%E7%9A%84%E7%A2%8E%E7%89%87%E6%95%B4%E7%90%86%E5%8A%9F%E8%83%BD/","text":"csdn Redis\u7684\u788e\u7247\u6574\u7406\u529f\u80fd\u53ea\u6709\u5728\u4f7f\u7528jemalloc\u7684\u65f6\u5019\u624d\u652f\u6301","title":"csdn [Redis\u7684\u788e\u7247\u6574\u7406\u529f\u80fd\u53ea\u6709\u5728\u4f7f\u7528jemalloc\u7684\u65f6\u5019\u624d\u652f\u6301](https://blog.csdn.net/Rong_Toa/article/details/115055656)"},{"location":"Basic/Memory-management/TODO/Redis%E7%9A%84%E7%A2%8E%E7%89%87%E6%95%B4%E7%90%86%E5%8A%9F%E8%83%BD/#csdn#redisjemalloc","text":"","title":"csdn Redis\u7684\u788e\u7247\u6574\u7406\u529f\u80fd\u53ea\u6709\u5728\u4f7f\u7528jemalloc\u7684\u65f6\u5019\u624d\u652f\u6301"},{"location":"Basic/Memory-management/TODO/memory%20monitoring%20and%20memory%20consumption/","text":"This article is about redis memory monitoring and memory consumption","title":"[This article is about redis memory monitoring and memory consumption](https://developpaper.com/this-article-is-about-redis-memory-monitoring-and-memory-consumption/)"},{"location":"Basic/Memory-management/TODO/memory%20monitoring%20and%20memory%20consumption/#this#article#is#about#redis#memory#monitoring#and#memory#consumption","text":"","title":"This article is about redis memory monitoring and memory consumption"},{"location":"Basic/Memory-management/zmalloc/","text":"zmalloc zhuanlan Redis\u5b9e\u73b0\u7ec6\u8282\u4e4b\uff1a\u201dzmalloc\u201c\u51fd\u6570 \u4e4b\u524d\u5728\u4e3a Eufa \u6dfb\u52a0\u672c\u5730\u7f13\u5b58\u7ba1\u7406\u7684\u529f\u80fd\u65f6\uff0c\u57fa\u672c\u7684\u6570\u636e\u5b58\u50a8\u529f\u80fd\u4f7f\u7528\u4e86\u7b80\u5355\u7684\u53cc\u5411\u94fe\u8868\u6765\u5b9e\u73b0\uff0c\u4f46\u7531\u4e8e\u5728 Wasm32 \u67b6\u6784\u4e0a\u7684\u6700\u5927\u53ef\u7528\u5185\u5b58\u53ea\u6709 4GB \uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u5bf9\u672c\u5730\u5185\u5b58\u7684\u4f7f\u7528\u5927\u5c0f\u505a\u4e00\u4e2a\u9650\u5236\uff0c**\u4f46\u5982\u4f55\u624d\u80fd\u591f\u7cbe\u786e\u5730\u83b7\u5f97\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5185\u5b58\u5927\u5c0f\u5462\uff1f**\u6211\u4eec\u90fd\u77e5\u9053\uff0c\u8b6c\u5982 malloc \u7b49\u6807\u51c6\u5e93\u4e2d\u7684\u5185\u5b58\u5206\u914d\u51fd\u6570\u4f1a\u6839\u636e\u5f53\u524d\u7684\u7cfb\u7edf\u67b6\u6784\u7c7b\u578b\u81ea\u52a8\u5730\u8fdb\u884c4/8\u5b57\u8282\u7684\u5185\u5b58\u5bf9\u9f50\uff0c\u56e0\u6b64\u5bf9\u4e8e\u5e94\u7528\u5728\u5b58\u50a8\u6570\u636e\u65f6\u5e95\u5c42\u7cfb\u7edf\u5b9e\u9645\u5206\u914d\u7684\u5185\u5b58\u5927\u5c0f\u6211\u4eec\u5f88\u96be\u76f4\u63a5\u8fdb\u884c\u8ba1\u7b97\u3002 \u800c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u8fd9\u91cc\u6211\u76f4\u63a5\u501f\u9274\u4e86 Redis \u5728\u5176\u5185\u5b58\u7ba1\u7406\u4e0a\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002\u5982\u4e0b\u6240\u793a\uff0c\u6211\u4eec\u76f4\u63a5\u6765\u770b Redis \u6e90\u7801\uff08\u4e0d\u662f\u6700\u65b0\u7248\u672c\uff09\u4e2d\u81ea\u5b9a\u4e49\u7684 zmalloc \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u4e0e malloc \u7b49\u5e38\u89c4\u51fd\u6570\u7684\u4f7f\u7528\u65b9\u5f0f\u5b8c\u5168\u4e00\u81f4\uff0c\u4e0d\u540c\u7684\u5728\u4e8e\u5176\u5185\u90e8\u7684\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3002 void * zmalloc ( size_t size ) { // \u5206\u914d\u5185\u5b58\uff1b void * ptr = malloc ( size + PREFIX_SIZE ); // \u5206\u914d\u5931\u8d25\u629b\u51fa\u5f02\u5e38\uff1b if ( ! ptr ) zmalloc_oom_handler ( size ); // \u7cfb\u7edf\u662f\u5426\u53ef\u4ee5\u4f7f\u7528\u201dmalloc_size\u201c\u51fd\u6570\uff1f #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_alloc ( zmalloc_size ( ptr )); return ptr ; #else // \u5728\u6570\u636e\u57df\u4fdd\u5b58\u5206\u914d\u6570\u636e\u7684\u5b9e\u9645\u5927\u5c0f\uff1b * (( size_t * ) ptr ) = size ; // \u8ba1\u7b97\u5bf9\u9f50\u540e\u7684\u5185\u5b58\u4f7f\u7528\u5927\u5c0f\uff0c\u5e76\u66f4\u65b0\u201dused_memory\u201c\u53d8\u91cf\uff1b update_zmalloc_stat_alloc ( size + PREFIX_SIZE ); // \u8fd4\u56de\u6570\u636e\u4f53\u7684\u521d\u59cb\u4f4d\u7f6e\uff1b return ( char * ) ptr + PREFIX_SIZE ; #endif } \u5176\u5b9e\uff0c\u6807\u51c6\u5e93\u4e2d\u7684 malloc \u51fd\u6570\u5df2\u7ecf\u80fd\u591f\u81ea\u52a8\u4e3a\u5206\u914d\u7684\u5185\u5b58\u5b9e\u73b0\u5bf9\u9f50\uff0c\u56e0\u6b64 zmalloc \u65b9\u6cd5\u5728\u8fd9\u91cc\u5176\u4e3b\u8981\u76ee\u7684\u662f\u4e3a\u4e86\u80fd\u591f\u7cbe\u786e\u5730\u8ba1\u7b97\u6bcf\u4e00\u6b21\u6570\u636e\u5b58\u50a8\u65f6\u6240\u5206\u914d\u7684**\u5185\u5b58\u5927\u5c0f**\u3002\u5728\u6bcf\u4e00\u6b21\u5206\u914d\u5185\u5b58\u65f6\uff0c zmalloc \u90fd\u4f1a\u5728\u8be5\u6b21\u5206\u914d\u7684\u6570\u636e\u5185\u5b58\u5927\u5c0f\u7684\u57fa\u7840\u4e0a\u518d\u52a0\u4e0a\u4e00\u4e2a PREFIX_SIZE \u5927\u5c0f\u7684\u989d\u5916\u5185\u5b58\u7a7a\u95f4\uff0c\u8fd9\u4e2a PREFIX_SIZE \u5b8f\u4ee3\u8868\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u6700\u5927\u5185\u5b58\u5bfb\u5740\u7a7a\u95f4\u5927\u5c0f\uff08 size_t \uff09\uff0c\u5176\u4f9d\u8d56\u4e8e\u5177\u4f53\u7cfb\u7edf\u7684\u7c7b\u578b\u4e0d\u540c\u800c\u4e0d\u540c\u3002\u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u7b80\u79f0\u8fd9\u4e2a PREFIX_SIZE \u5927\u5c0f\u7684\u7a7a\u95f4\u4e3a\u4e00\u4e2a\u5b58\u50a8\u5355\u5143\u7684\u201c\u6570\u636e\u5934\u201d\u90e8\u5206\u3002 \u521d\u7248 Redis \u7684\u5b58\u50a8\u5355\u5143\u7ed3\u6784 \u5982\u4e0a\u56fe\u6240\u793a\uff0c\u901a\u8fc7 *((size_t*)ptr) = size; \u8bed\u53e5\uff0cRedis \u5728\u5f53\u524d\u5206\u914d\u5185\u5b58\u5757\u7684\u524d PREFIX_SIZE \u4e2a\u5b57\u8282\uff0c\u5373\u6570\u636e\u5934\u5185\u5b58\u50a8\u4e86\u672c\u6b21**\u5b9e\u9645\u5206\u914d\u7684\u6570\u636e\u5757\u5927\u5c0f**\uff0c\u800c\u5728\u540e\u9762 \u201dsize\u201c \u5927\u5c0f\u7684\u5185\u5b58\u7a7a\u95f4\u4e2d\u624d\u771f\u6b63\u5b58\u653e\u4e86\u4e8c\u8fdb\u5236\u7684\u6570\u636e\u5b9e\u4f53\u3002\u5728\u8fd9\u91cc\u540d\u4e3a update_zmalloc_stat_alloc \u7684\u51fd\u6570\u5728\u5176\u5185\u90e8\u4f1a\u7ef4\u62a4\u4e00\u4e2a\u540d\u4e3a used_memory \u7684\u5168\u5c40\u53d8\u91cf\uff0c\u8be5\u53d8\u91cf\u7d2f\u52a0\u4e86\u6bcf\u6b21\u65b0\u5206\u914d\u7684\u5185\u5b58\u5927\u5c0f\u3002\u51fd\u6570\u5728\u6700\u540e\u8fd4\u56de\u4e86\u4e00\u4e2a\u504f\u79fb\u7684\u6307\u9488\uff0c\u6307\u5411\u4e86\u5f53\u524d\u5206\u914d\u5185\u5b58\u7684\u6570\u636e\u4f53\u90e8\u5206\u3002 update_zmalloc_stat_alloc \u51fd\u6570\u7684\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u5982\u4e0b\u3002 #define update_zmalloc_stat_alloc(__n) do { size_t _n = ( __n ); // \u624b\u52a8\u5185\u5b58\u8865\u9f50\uff1b if ( _n & ( sizeof ( long ) -1 )) _n += sizeof ( long ) - ( _n & ( sizeof ( long ) -1 )); atomicIncr ( used_memory , __n ); } while ( 0 ) \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u91cd\u70b9\u662f _n += sizeof(long)-(_n&(sizeof(long)-1)); \u8fd9\u884c\u8bed\u53e5\u3002\u6574\u4e2a\u5b8f\u51fd\u6570\u9996\u5148\u5224\u65ad\u672c\u6b21\u5206\u914d\u7684\u5185\u5b58\u5927\u5c0f\u662f\u5426\u4e3a sizeof(long) \u5927\u5c0f\u7684\u6574\u6570\u500d\uff0864\u4f4d\u673a\u5bf9\u5e94\u77408\u5b57\u8282\u7684\u5185\u5b58\u5bf9\u9f50\uff1b32\u4f4d\u673a\u5219\u5bf9\u5e94\u77404\u5b57\u8282\u7684\u5185\u5b58\u5bf9\u9f50\uff09\uff0c\u5982\u679c\u4e0d\u662f\u5219\u901a\u8fc7\u6211\u4eec\u4e4b\u524d\u7ed9\u51fa\u7684\u8bed\u53e5\u5728\u8be5\u6570\u636e\u6bb5\u540e\u6dfb\u52a0\u76f8\u5e94\u7684\u5360\u4f4d\u7a7a\u95f4\u6765\u8865\u8db3\u4f4d\u6570\u4ee5\u6ee1\u8db3\u5185\u5b58\u5bf9\u9f50\uff084/8\u5b57\u8282\uff09\u7684\u8981\u6c42\u3002\u6700\u540e\u7684 atomicIncr \u51fd\u6570\u7528\u6765\u5728\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\u7684\u60c5\u51b5\u4e0b\u66f4\u65b0\u5168\u5c40\u7684 used_memory \u53d8\u91cf\u503c\u3002 \u800c\u8be5\u7248\u672c Redis \u4e2d\u5185\u5b58\u91ca\u653e\u4e0e\u5176\u5185\u5b58\u5206\u914d\u7684\u8fc7\u7a0b\u5219\u6b63\u597d\u76f8\u53cd\u3002\u5982\u4e0b\u6240\u793a\u4ee3\u7801\u4e3a\u5bf9\u5e94 \u201dzfree\u201c \u51fd\u6570\u7684\u5b9e\u73b0\u7ec6\u8282\u3002\u9996\u5148\u8be5\u51fd\u6570\u901a\u8fc7 (char*)ptr-PREFIX_SIZE \u8bed\u53e5\uff08\u5411\u5185\u5b58\u4f4e\u5730\u5740\u79fb\u52a8\uff09\u6307\u5411\u4e86\u5305\u542b\u6709\u8be5\u6570\u636e\u5757\u5b9e\u9645\u5360\u7528\u5927\u5c0f\u7684\u6570\u636e\u57df\u9996\u5730\u5740\uff0c\u7136\u540e\u901a\u8fc7 ***((size_t*)realptr)** \u8bed\u53e5\u83b7\u5f97\u5230\u4e86\u8be5\u6570\u636e\u5757\u5206\u914d\u7684\u771f\u5b9e\u5185\u5b58\u5927\u5c0f\uff08\u4e0d\u5305\u542b\u5185\u5b58\u5bf9\u9f50\u533a\u57df\uff09\u3002\u6700\u540e\u518d\u901a\u8fc7 update_zmalloc_stat_free \u51fd\u6570\u6765\u66f4\u65b0\u5168\u5c40\u53d8\u91cf used_memory \u7684\u503c\uff0c\u5e76\u91ca\u653e\u8be5\u6bb5\u5185\u5b58\u3002 void zfree(void *ptr) { #ifndef HAVE_MALLOC_SIZE void *realptr; size_t oldsize; #endif if (ptr == NULL) return; #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_free(zmalloc_size(ptr)); free(ptr); #else realptr = (char*)ptr-PREFIX_SIZE; oldsize = *((size_t*)realptr); update_zmalloc_stat_free(oldsize+PREFIX_SIZE); free(realptr); #endif } \u5982\u4e0b\u6240\u793a\uff0c\u8fd9\u91cc\u5982\u679c\u6211\u4eec\u518d\u6765\u770b update_zmalloc_stat_free \u51fd\u6570\u7684\u5b9e\u73b0\u7ec6\u8282\uff0c\u4f60\u4f1a\u53d1\u73b0\u5b83\u4e0e\u4e4b\u524d\u7684 update_zmalloc_stat_alloc \u51fd\u6570\u5176\u6267\u884c\u8fc7\u7a0b\u7c7b\u4f3c\u3002\u901a\u8fc7\u8ba1\u7b97\u9700\u8981\u8865\u8db3\u7684\u5185\u5b58\u5b57\u8282\u5927\u5c0f\uff0c\u5e76\u4ece used_memory \u53d8\u91cf\u4e2d\u51cf\u53bb\u76f8\u5e94\u5927\u5c0f\u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u5373\u53ef\u5b9e\u73b0\u5bf9\u5185\u5b58\u7a7a\u95f4\u4f7f\u7528\u7387\u7684\u7cbe\u786e\u8ba1\u7b97\u3002 #define update_zmalloc_stat_free(__n) do { \\ size_t _n = (__n); \\ if (_n&(sizeof(long)-1)) _n += sizeof(long)-(_n&(sizeof(long)-1)); \\ atomicDecr(used_memory,__n); \\ } while(0) \u6700\u540e\u518d\u6765\u4ed4\u7ec6\u56de\u987e\u4e00\u4e0b\u521a\u521a\u6211\u4eec\u5728 update_zmalloc_stat_alloc \u51fd\u6570\u4e2d\u6267\u884c\u5185\u5b58\u8865\u9f50\u64cd\u4f5c\u7684\u90e8\u5206\u3002\u5728 Wasm32 \u67b6\u6784\u4e2d\uff0c sizeof(long) \u7684\u503c\u662f4\uff0c\u56e0\u6b64\u5bf9\u5e94\u7684\u8865\u9f50\u64cd\u4f5c\u8bed\u53e5\u5c31\u53d8\u6210\u4e86 **if(_n&3) _n += 4 - (_n&3);** \uff0c\u5373\u9700\u8981\u6ee1\u8db34\u5b57\u8282\u5bf9\u9f50\u3002\u5982\u679c\u6b64\u65f6\u7528\u6237\u7aef\u6709\u4e00\u4e2a\u5927\u5c0f\u4e3a\u201d 13\u5b57\u8282 \u201c\u7684\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u90a3\u4e48\u7ecf\u8fc7\u5185\u5b58\u5bf9\u9f50\u5904\u7406\uff0c\u5e94\u7528\u5b9e\u9645\u5b58\u653e\u5230\u5185\u5b58\u4e2d\u7684\u6570\u636e\u5927\u5c0f\u4fbf\u4e3a\uff1a13+4-1=16\u5b57\u8282\u5927\u5c0f\uff0c\u5185\u5b58\u5bf9\u9f50\u5b57\u8282\u7684\u8ba1\u7b97\u8fc7\u7a0b\u53ef\u4ee5\u53c2\u8003\u4e0b\u56fe\u3002 \u5185\u5b58\u5bf9\u9f50\u5b57\u8282\u8ba1\u7b97\u6d41\u7a0b \u4ee5\u4e0a\u4fbf\u662f Redis \u5728\u5176\u6574\u4f53\u590d\u6742\u8bbe\u8ba1\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u7684\u4e00\u4e2a\u5341\u5206\u7ec6\u5c0f\u4f46\u5374\u7cbe\u5999\u7684\u8bbe\u8ba1\u70b9\u3002PS\uff1a\u51fd\u6570 atomicDecr \u5728\u5176\u5185\u90e8\u4f7f\u7528\u4e86 **pthread_mutex_lock** \u4e0e **pthread_mutex_unlock** \u8fd9\u4e24\u4e2a\u65b9\u6cd5\u6765\u4e3a\u9700\u8981\u7ebf\u7a0b\u5b89\u5168\u7684\u6570\u636e\u7ed3\u6784\u6dfb\u52a0**\u4e92\u65a5\u9501**\uff0c\u4ee5\u4fdd\u8bc1\u591a\u7ebf\u7a0b\u5bf9\u6570\u636e\u7684\u5b89\u5168\u8bfb\u5199\u64cd\u4f5c\u3002\u5173\u4e8e\u8fd9\u90e8\u5206\u5185\u5bb9\u4ee5\u53ca\u5176\u4ed6\u4e0e Redis \u5b9e\u73b0\u7ec6\u8282\u76f8\u5173\u7684\u8bbe\u8ba1\u70b9\u5256\u6790\uff0c\u6709\u673a\u4f1a\u6211\u4eec\u4f1a\u5728\u540e\u7eed\u7684\u6587\u7ae0\u4e2d\u8fdb\u884c\u4ecb\u7ecd\u3002 \u4ece\u5b9e\u73b0\u6765\u770b\uff0c zmalloc \u548c zfree \u6839\u636e HAVE_MALLOC_SIZE \u6709\u4e0d\u540c\u7684\u5b9e\u73b0\uff1b\u4e24\u79cd\u5b9e\u73b0\u4e2d\uff0c\u90fd\u4f1a malloc(size+PREFIX_SIZE) \u5373\u5728\u7528\u6237\u6307\u5b9a\u7684 size \u57fa\u7840\u4e0a\u591a\u5206\u914d PREFIX_SIZE \u3002","title":"Introduction"},{"location":"Basic/Memory-management/zmalloc/#zmalloc","text":"","title":"zmalloc"},{"location":"Basic/Memory-management/zmalloc/#zhuanlan#rediszmalloc","text":"\u4e4b\u524d\u5728\u4e3a Eufa \u6dfb\u52a0\u672c\u5730\u7f13\u5b58\u7ba1\u7406\u7684\u529f\u80fd\u65f6\uff0c\u57fa\u672c\u7684\u6570\u636e\u5b58\u50a8\u529f\u80fd\u4f7f\u7528\u4e86\u7b80\u5355\u7684\u53cc\u5411\u94fe\u8868\u6765\u5b9e\u73b0\uff0c\u4f46\u7531\u4e8e\u5728 Wasm32 \u67b6\u6784\u4e0a\u7684\u6700\u5927\u53ef\u7528\u5185\u5b58\u53ea\u6709 4GB \uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u5bf9\u672c\u5730\u5185\u5b58\u7684\u4f7f\u7528\u5927\u5c0f\u505a\u4e00\u4e2a\u9650\u5236\uff0c**\u4f46\u5982\u4f55\u624d\u80fd\u591f\u7cbe\u786e\u5730\u83b7\u5f97\u5e94\u7528\u6240\u4f7f\u7528\u7684\u5185\u5b58\u5927\u5c0f\u5462\uff1f**\u6211\u4eec\u90fd\u77e5\u9053\uff0c\u8b6c\u5982 malloc \u7b49\u6807\u51c6\u5e93\u4e2d\u7684\u5185\u5b58\u5206\u914d\u51fd\u6570\u4f1a\u6839\u636e\u5f53\u524d\u7684\u7cfb\u7edf\u67b6\u6784\u7c7b\u578b\u81ea\u52a8\u5730\u8fdb\u884c4/8\u5b57\u8282\u7684\u5185\u5b58\u5bf9\u9f50\uff0c\u56e0\u6b64\u5bf9\u4e8e\u5e94\u7528\u5728\u5b58\u50a8\u6570\u636e\u65f6\u5e95\u5c42\u7cfb\u7edf\u5b9e\u9645\u5206\u914d\u7684\u5185\u5b58\u5927\u5c0f\u6211\u4eec\u5f88\u96be\u76f4\u63a5\u8fdb\u884c\u8ba1\u7b97\u3002 \u800c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u8fd9\u91cc\u6211\u76f4\u63a5\u501f\u9274\u4e86 Redis \u5728\u5176\u5185\u5b58\u7ba1\u7406\u4e0a\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002\u5982\u4e0b\u6240\u793a\uff0c\u6211\u4eec\u76f4\u63a5\u6765\u770b Redis \u6e90\u7801\uff08\u4e0d\u662f\u6700\u65b0\u7248\u672c\uff09\u4e2d\u81ea\u5b9a\u4e49\u7684 zmalloc \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u4e0e malloc \u7b49\u5e38\u89c4\u51fd\u6570\u7684\u4f7f\u7528\u65b9\u5f0f\u5b8c\u5168\u4e00\u81f4\uff0c\u4e0d\u540c\u7684\u5728\u4e8e\u5176\u5185\u90e8\u7684\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3002 void * zmalloc ( size_t size ) { // \u5206\u914d\u5185\u5b58\uff1b void * ptr = malloc ( size + PREFIX_SIZE ); // \u5206\u914d\u5931\u8d25\u629b\u51fa\u5f02\u5e38\uff1b if ( ! ptr ) zmalloc_oom_handler ( size ); // \u7cfb\u7edf\u662f\u5426\u53ef\u4ee5\u4f7f\u7528\u201dmalloc_size\u201c\u51fd\u6570\uff1f #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_alloc ( zmalloc_size ( ptr )); return ptr ; #else // \u5728\u6570\u636e\u57df\u4fdd\u5b58\u5206\u914d\u6570\u636e\u7684\u5b9e\u9645\u5927\u5c0f\uff1b * (( size_t * ) ptr ) = size ; // \u8ba1\u7b97\u5bf9\u9f50\u540e\u7684\u5185\u5b58\u4f7f\u7528\u5927\u5c0f\uff0c\u5e76\u66f4\u65b0\u201dused_memory\u201c\u53d8\u91cf\uff1b update_zmalloc_stat_alloc ( size + PREFIX_SIZE ); // \u8fd4\u56de\u6570\u636e\u4f53\u7684\u521d\u59cb\u4f4d\u7f6e\uff1b return ( char * ) ptr + PREFIX_SIZE ; #endif } \u5176\u5b9e\uff0c\u6807\u51c6\u5e93\u4e2d\u7684 malloc \u51fd\u6570\u5df2\u7ecf\u80fd\u591f\u81ea\u52a8\u4e3a\u5206\u914d\u7684\u5185\u5b58\u5b9e\u73b0\u5bf9\u9f50\uff0c\u56e0\u6b64 zmalloc \u65b9\u6cd5\u5728\u8fd9\u91cc\u5176\u4e3b\u8981\u76ee\u7684\u662f\u4e3a\u4e86\u80fd\u591f\u7cbe\u786e\u5730\u8ba1\u7b97\u6bcf\u4e00\u6b21\u6570\u636e\u5b58\u50a8\u65f6\u6240\u5206\u914d\u7684**\u5185\u5b58\u5927\u5c0f**\u3002\u5728\u6bcf\u4e00\u6b21\u5206\u914d\u5185\u5b58\u65f6\uff0c zmalloc \u90fd\u4f1a\u5728\u8be5\u6b21\u5206\u914d\u7684\u6570\u636e\u5185\u5b58\u5927\u5c0f\u7684\u57fa\u7840\u4e0a\u518d\u52a0\u4e0a\u4e00\u4e2a PREFIX_SIZE \u5927\u5c0f\u7684\u989d\u5916\u5185\u5b58\u7a7a\u95f4\uff0c\u8fd9\u4e2a PREFIX_SIZE \u5b8f\u4ee3\u8868\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u6700\u5927\u5185\u5b58\u5bfb\u5740\u7a7a\u95f4\u5927\u5c0f\uff08 size_t \uff09\uff0c\u5176\u4f9d\u8d56\u4e8e\u5177\u4f53\u7cfb\u7edf\u7684\u7c7b\u578b\u4e0d\u540c\u800c\u4e0d\u540c\u3002\u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u7b80\u79f0\u8fd9\u4e2a PREFIX_SIZE \u5927\u5c0f\u7684\u7a7a\u95f4\u4e3a\u4e00\u4e2a\u5b58\u50a8\u5355\u5143\u7684\u201c\u6570\u636e\u5934\u201d\u90e8\u5206\u3002 \u521d\u7248 Redis \u7684\u5b58\u50a8\u5355\u5143\u7ed3\u6784 \u5982\u4e0a\u56fe\u6240\u793a\uff0c\u901a\u8fc7 *((size_t*)ptr) = size; \u8bed\u53e5\uff0cRedis \u5728\u5f53\u524d\u5206\u914d\u5185\u5b58\u5757\u7684\u524d PREFIX_SIZE \u4e2a\u5b57\u8282\uff0c\u5373\u6570\u636e\u5934\u5185\u5b58\u50a8\u4e86\u672c\u6b21**\u5b9e\u9645\u5206\u914d\u7684\u6570\u636e\u5757\u5927\u5c0f**\uff0c\u800c\u5728\u540e\u9762 \u201dsize\u201c \u5927\u5c0f\u7684\u5185\u5b58\u7a7a\u95f4\u4e2d\u624d\u771f\u6b63\u5b58\u653e\u4e86\u4e8c\u8fdb\u5236\u7684\u6570\u636e\u5b9e\u4f53\u3002\u5728\u8fd9\u91cc\u540d\u4e3a update_zmalloc_stat_alloc \u7684\u51fd\u6570\u5728\u5176\u5185\u90e8\u4f1a\u7ef4\u62a4\u4e00\u4e2a\u540d\u4e3a used_memory \u7684\u5168\u5c40\u53d8\u91cf\uff0c\u8be5\u53d8\u91cf\u7d2f\u52a0\u4e86\u6bcf\u6b21\u65b0\u5206\u914d\u7684\u5185\u5b58\u5927\u5c0f\u3002\u51fd\u6570\u5728\u6700\u540e\u8fd4\u56de\u4e86\u4e00\u4e2a\u504f\u79fb\u7684\u6307\u9488\uff0c\u6307\u5411\u4e86\u5f53\u524d\u5206\u914d\u5185\u5b58\u7684\u6570\u636e\u4f53\u90e8\u5206\u3002 update_zmalloc_stat_alloc \u51fd\u6570\u7684\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u5982\u4e0b\u3002 #define update_zmalloc_stat_alloc(__n) do { size_t _n = ( __n ); // \u624b\u52a8\u5185\u5b58\u8865\u9f50\uff1b if ( _n & ( sizeof ( long ) -1 )) _n += sizeof ( long ) - ( _n & ( sizeof ( long ) -1 )); atomicIncr ( used_memory , __n ); } while ( 0 ) \u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u91cd\u70b9\u662f _n += sizeof(long)-(_n&(sizeof(long)-1)); \u8fd9\u884c\u8bed\u53e5\u3002\u6574\u4e2a\u5b8f\u51fd\u6570\u9996\u5148\u5224\u65ad\u672c\u6b21\u5206\u914d\u7684\u5185\u5b58\u5927\u5c0f\u662f\u5426\u4e3a sizeof(long) \u5927\u5c0f\u7684\u6574\u6570\u500d\uff0864\u4f4d\u673a\u5bf9\u5e94\u77408\u5b57\u8282\u7684\u5185\u5b58\u5bf9\u9f50\uff1b32\u4f4d\u673a\u5219\u5bf9\u5e94\u77404\u5b57\u8282\u7684\u5185\u5b58\u5bf9\u9f50\uff09\uff0c\u5982\u679c\u4e0d\u662f\u5219\u901a\u8fc7\u6211\u4eec\u4e4b\u524d\u7ed9\u51fa\u7684\u8bed\u53e5\u5728\u8be5\u6570\u636e\u6bb5\u540e\u6dfb\u52a0\u76f8\u5e94\u7684\u5360\u4f4d\u7a7a\u95f4\u6765\u8865\u8db3\u4f4d\u6570\u4ee5\u6ee1\u8db3\u5185\u5b58\u5bf9\u9f50\uff084/8\u5b57\u8282\uff09\u7684\u8981\u6c42\u3002\u6700\u540e\u7684 atomicIncr \u51fd\u6570\u7528\u6765\u5728\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\u7684\u60c5\u51b5\u4e0b\u66f4\u65b0\u5168\u5c40\u7684 used_memory \u53d8\u91cf\u503c\u3002 \u800c\u8be5\u7248\u672c Redis \u4e2d\u5185\u5b58\u91ca\u653e\u4e0e\u5176\u5185\u5b58\u5206\u914d\u7684\u8fc7\u7a0b\u5219\u6b63\u597d\u76f8\u53cd\u3002\u5982\u4e0b\u6240\u793a\u4ee3\u7801\u4e3a\u5bf9\u5e94 \u201dzfree\u201c \u51fd\u6570\u7684\u5b9e\u73b0\u7ec6\u8282\u3002\u9996\u5148\u8be5\u51fd\u6570\u901a\u8fc7 (char*)ptr-PREFIX_SIZE \u8bed\u53e5\uff08\u5411\u5185\u5b58\u4f4e\u5730\u5740\u79fb\u52a8\uff09\u6307\u5411\u4e86\u5305\u542b\u6709\u8be5\u6570\u636e\u5757\u5b9e\u9645\u5360\u7528\u5927\u5c0f\u7684\u6570\u636e\u57df\u9996\u5730\u5740\uff0c\u7136\u540e\u901a\u8fc7 ***((size_t*)realptr)** \u8bed\u53e5\u83b7\u5f97\u5230\u4e86\u8be5\u6570\u636e\u5757\u5206\u914d\u7684\u771f\u5b9e\u5185\u5b58\u5927\u5c0f\uff08\u4e0d\u5305\u542b\u5185\u5b58\u5bf9\u9f50\u533a\u57df\uff09\u3002\u6700\u540e\u518d\u901a\u8fc7 update_zmalloc_stat_free \u51fd\u6570\u6765\u66f4\u65b0\u5168\u5c40\u53d8\u91cf used_memory \u7684\u503c\uff0c\u5e76\u91ca\u653e\u8be5\u6bb5\u5185\u5b58\u3002 void zfree(void *ptr) { #ifndef HAVE_MALLOC_SIZE void *realptr; size_t oldsize; #endif if (ptr == NULL) return; #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_free(zmalloc_size(ptr)); free(ptr); #else realptr = (char*)ptr-PREFIX_SIZE; oldsize = *((size_t*)realptr); update_zmalloc_stat_free(oldsize+PREFIX_SIZE); free(realptr); #endif } \u5982\u4e0b\u6240\u793a\uff0c\u8fd9\u91cc\u5982\u679c\u6211\u4eec\u518d\u6765\u770b update_zmalloc_stat_free \u51fd\u6570\u7684\u5b9e\u73b0\u7ec6\u8282\uff0c\u4f60\u4f1a\u53d1\u73b0\u5b83\u4e0e\u4e4b\u524d\u7684 update_zmalloc_stat_alloc \u51fd\u6570\u5176\u6267\u884c\u8fc7\u7a0b\u7c7b\u4f3c\u3002\u901a\u8fc7\u8ba1\u7b97\u9700\u8981\u8865\u8db3\u7684\u5185\u5b58\u5b57\u8282\u5927\u5c0f\uff0c\u5e76\u4ece used_memory \u53d8\u91cf\u4e2d\u51cf\u53bb\u76f8\u5e94\u5927\u5c0f\u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u5373\u53ef\u5b9e\u73b0\u5bf9\u5185\u5b58\u7a7a\u95f4\u4f7f\u7528\u7387\u7684\u7cbe\u786e\u8ba1\u7b97\u3002 #define update_zmalloc_stat_free(__n) do { \\ size_t _n = (__n); \\ if (_n&(sizeof(long)-1)) _n += sizeof(long)-(_n&(sizeof(long)-1)); \\ atomicDecr(used_memory,__n); \\ } while(0) \u6700\u540e\u518d\u6765\u4ed4\u7ec6\u56de\u987e\u4e00\u4e0b\u521a\u521a\u6211\u4eec\u5728 update_zmalloc_stat_alloc \u51fd\u6570\u4e2d\u6267\u884c\u5185\u5b58\u8865\u9f50\u64cd\u4f5c\u7684\u90e8\u5206\u3002\u5728 Wasm32 \u67b6\u6784\u4e2d\uff0c sizeof(long) \u7684\u503c\u662f4\uff0c\u56e0\u6b64\u5bf9\u5e94\u7684\u8865\u9f50\u64cd\u4f5c\u8bed\u53e5\u5c31\u53d8\u6210\u4e86 **if(_n&3) _n += 4 - (_n&3);** \uff0c\u5373\u9700\u8981\u6ee1\u8db34\u5b57\u8282\u5bf9\u9f50\u3002\u5982\u679c\u6b64\u65f6\u7528\u6237\u7aef\u6709\u4e00\u4e2a\u5927\u5c0f\u4e3a\u201d 13\u5b57\u8282 \u201c\u7684\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u90a3\u4e48\u7ecf\u8fc7\u5185\u5b58\u5bf9\u9f50\u5904\u7406\uff0c\u5e94\u7528\u5b9e\u9645\u5b58\u653e\u5230\u5185\u5b58\u4e2d\u7684\u6570\u636e\u5927\u5c0f\u4fbf\u4e3a\uff1a13+4-1=16\u5b57\u8282\u5927\u5c0f\uff0c\u5185\u5b58\u5bf9\u9f50\u5b57\u8282\u7684\u8ba1\u7b97\u8fc7\u7a0b\u53ef\u4ee5\u53c2\u8003\u4e0b\u56fe\u3002 \u5185\u5b58\u5bf9\u9f50\u5b57\u8282\u8ba1\u7b97\u6d41\u7a0b \u4ee5\u4e0a\u4fbf\u662f Redis \u5728\u5176\u6574\u4f53\u590d\u6742\u8bbe\u8ba1\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u7684\u4e00\u4e2a\u5341\u5206\u7ec6\u5c0f\u4f46\u5374\u7cbe\u5999\u7684\u8bbe\u8ba1\u70b9\u3002PS\uff1a\u51fd\u6570 atomicDecr \u5728\u5176\u5185\u90e8\u4f7f\u7528\u4e86 **pthread_mutex_lock** \u4e0e **pthread_mutex_unlock** \u8fd9\u4e24\u4e2a\u65b9\u6cd5\u6765\u4e3a\u9700\u8981\u7ebf\u7a0b\u5b89\u5168\u7684\u6570\u636e\u7ed3\u6784\u6dfb\u52a0**\u4e92\u65a5\u9501**\uff0c\u4ee5\u4fdd\u8bc1\u591a\u7ebf\u7a0b\u5bf9\u6570\u636e\u7684\u5b89\u5168\u8bfb\u5199\u64cd\u4f5c\u3002\u5173\u4e8e\u8fd9\u90e8\u5206\u5185\u5bb9\u4ee5\u53ca\u5176\u4ed6\u4e0e Redis \u5b9e\u73b0\u7ec6\u8282\u76f8\u5173\u7684\u8bbe\u8ba1\u70b9\u5256\u6790\uff0c\u6709\u673a\u4f1a\u6211\u4eec\u4f1a\u5728\u540e\u7eed\u7684\u6587\u7ae0\u4e2d\u8fdb\u884c\u4ecb\u7ecd\u3002 \u4ece\u5b9e\u73b0\u6765\u770b\uff0c zmalloc \u548c zfree \u6839\u636e HAVE_MALLOC_SIZE \u6709\u4e0d\u540c\u7684\u5b9e\u73b0\uff1b\u4e24\u79cd\u5b9e\u73b0\u4e2d\uff0c\u90fd\u4f1a malloc(size+PREFIX_SIZE) \u5373\u5728\u7528\u6237\u6307\u5b9a\u7684 size \u57fa\u7840\u4e0a\u591a\u5206\u914d PREFIX_SIZE \u3002","title":"zhuanlan Redis\u5b9e\u73b0\u7ec6\u8282\u4e4b\uff1a\u201dzmalloc\u201c\u51fd\u6570"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/","text":"csdn Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \u4f5c\u8005\uff1a\u679c\u51bb\u867e\u4ec1 \u6765\u6e90\uff1aCSDN \u539f\u6587\uff1a https://blog.csdn.net/guodongxiaren/article/details/44747719 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3a\u535a\u4e3b\u539f\u521b\u6587\u7ae0\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u535a\u6587\u94fe\u63a5\uff01 \u5f53\u6211\u7b2c\u4e00\u6b21\u9605\u8bfb\u4e86\u8fd9\u4e2a\u6587\u4ef6\u7684\u6e90\u7801\u7684\u65f6\u5019\uff0c\u6211\u7b11\u4e86\uff0c\u5ffd\u7136\u60f3\u8d77\u524d\u51e0\u5468\u963f\u91cc\u7535\u8bdd\u4e8c\u9762\u7684\u65f6\u5019\uff0c\u95ee\u5230\u4e86**\u81ea\u5b9a\u4e49\u5185\u5b58\u7ba1\u7406\u51fd\u6570**\u5e76\u5904\u7406**8\u5b57\u8282\u5bf9\u9f50\u95ee\u9898**\u3002\u5f53\u65f6\u65e0\u8a00\u4ee5\u5bf9\uff0c\u5728\u9762\u8bd5\u5b98\u65e0\u6570\u6b21\u7684\u63d0\u793a\u4e0b\u624d\u7b54\u4e86\u51fa\u6765\uff0c\u7ed3\u679c\u663e\u800c\u6613\u89c1\uff0c\u6302\u6389\u4e86\u4e8c\u9762\u3002\u800c\u8fd9\u4efd\u6e90\u7801\u4e2d\u51fd\u6570 zmalloc() \u548c zfree() \u7684\u8bbe\u8ba1\u601d\u8def\u548c\u5b9e\u73b0\u539f\u7406\uff0c\u6b63\u662f\u9762\u8bd5\u5b98\u60f3\u8981\u7684\u7b54\u6848\u3002 \u6e90\u7801\u7ed3\u6784 zmalloc.c \u6587\u4ef6\u7684\u5185\u5bb9\u5982\u4e0b\uff1a \u4e3b\u8981\u51fd\u6570 zmalloc() zfree() zcalloc() zrelloc() zstrdup() \u5b57\u957f\u4e0e\u5b57\u8282\u5bf9\u9f50 CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u79f0\u4e3a**\u5b57\u957f**\uff0c\u4e5f\u5c31\u662f\u6211\u4eec\u901a\u5e38\u6240\u8bf4\u768432\u4f4d\u7cfb\u7edf\uff08\u5b57\u957f4\u4e2a\u5b57\u8282\uff09\u300164\u4f4d\u7cfb\u7edf\uff08\u5b57\u957f8\u4e2a\u5b57\u8282\uff09\u7684\u7531\u6765\u3002\u6240\u8c13\u7684**8\u5b57\u8282\u5bf9\u9f50**\uff0c\u5c31\u662f\u6307**\u53d8\u91cf\u7684\u8d77\u59cb\u5730\u5740**\u662f8\u7684\u500d\u6570\u3002\u6bd4\u5982\u7a0b\u5e8f\u8fd0\u884c\u65f6\uff08CPU\uff09\u5728\u8bfb\u53d6 long \u578b\u6570\u636e\u7684\u65f6\u5019\uff0c\u53ea\u9700\u8981\u4e00\u4e2a\u603b\u7ebf\u5468\u671f\uff0c\u65f6\u95f4\u66f4\u77ed\uff0c\u5982\u679c\u4e0d\u662f8\u5b57\u8282\u5bf9\u9f50\u7684\u5219\u9700\u8981\u4e24\u4e2a\u603b\u7ebf\u5468\u671f\u624d\u80fd\u8bfb\u5b8c\u6570\u636e\u3002 \u672c\u6587\u4e2d\u6211\u63d0\u5230\u76848\u5b57\u8282\u5bf9\u9f50\u662f\u9488\u5bf964\u4f4d\u7cfb\u7edf\u800c\u8a00\u7684\uff0c\u5982\u679c\u662f32\u4f4d\u7cfb\u7edf\u90a3\u4e48\u5c31\u662f4\u5b57\u8282\u5bf9\u9f50\u3002\u5b9e\u9645\u4e0aRedis\u6e90\u7801\u4e2d\u7684**\u5b57\u8282\u5bf9\u9f50**\u662f\u8f6f\u7f16\u7801\uff0c\u800c\u975e\u786c\u7f16\u7801\u3002\u91cc\u9762\u591a\u7528 sizeof(long) \u6216 sizeof(size_t) \u6765\u8868\u793a\u3002 size_t \uff08gcc\u4e2d\u5176\u503c\u4e3a long unsigned int \uff09\u548c long \u7684\u957f\u5ea6\u662f\u4e00\u6837\u7684\uff0c long \u7684\u957f\u5ea6\u5c31\u662f\u8ba1\u7b97\u673a\u7684\u5b57\u957f\u3002\u8fd9\u6837\u5728\u672a\u6765\u7684\u7cfb\u7edf\u4e2d\u5982\u679c\u5b57\u957f\uff08long\u7684\u5927\u5c0f\uff09\u4e0d\u662f8\u4e2a\u5b57\u8282\u4e86\uff0c\u8be5\u6bb5\u4ee3\u7801\u4f9d\u7136\u80fd\u4fdd\u8bc1\u76f8\u5e94\u4ee3\u7801\u53ef\u7528\u3002 NOTE: Data structure alignment zmalloc \u8f85\u52a9\u7684\u51fd\u6570\uff1a 1\u3001 malloc() 2\u3001 zmalloc_oom_handler \u3010\u51fd\u6570\u6307\u9488\u3011 3\u3001 zmalloc_default_oom() \u3010\u88ab\u4e0a\u9762\u7684\u51fd\u6570\u6307\u9488\u6240\u6307\u5411\u3011 4\u3001 update_zmalloc_stat_alloc() \u3010\u5b8f\u51fd\u6570\u3011 5\u3001 update_zmalloc_stat_add() \u3010\u5b8f\u51fd\u6570\u3011 zmalloc() \u548c malloc() \u6709\u76f8\u540c\u7684\u51fd\u6570\u63a5\u53e3\uff08\u53c2\u6570\uff0c\u8fd4\u56de\u503c\uff09\u3002 zmalloc() \u6e90\u7801 ```c void *zmalloc(size_t size) { void *ptr = malloc(size+PREFIX_SIZE); if (!ptr) zmalloc_oom_handler(size); ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_alloc(zmalloc_size(ptr)); return ptr; else *((size_t*)ptr) = size; update_zmalloc_stat_alloc(size+PREFIX_SIZE); return (char*)ptr+PREFIX_SIZE; endif } ``` \u53c2\u6570 size \u662f\u6211\u4eec**\u9700\u8981\u5206\u914d**\u7684\u5185\u5b58\u5927\u5c0f\u3002\u5b9e\u9645\u4e0a\u6211\u4eec\u8c03\u7528 malloc **\u5b9e\u9645\u5206\u914d**\u7684\u5927\u5c0f\u662f size+PREFIX_SIZE \u3002 PREFIX_SIZE \u662f\u4e00\u4e2a\u6761\u4ef6\u7f16\u8bd1\u7684\u5b8f\uff0c\u4e0d\u540c\u7684\u5e73\u53f0\u6709\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u5728Linux\u4e2d\u5176\u503c\u662f sizeof(size_t) \uff0c\u6240\u4ee5\u6211\u4eec\u591a\u5206\u914d\u4e86\u4e00\u4e2a\u5b57\u957f(8\u4e2a\u5b57\u8282)\u7684\u7a7a\u95f4\uff08\u540e\u9762\u4ee3\u7801\u53ef\u4ee5\u770b\u5230\u591a\u5206\u914d8\u4e2a\u5b57\u8282\u7684\u76ee\u7684\u662f\u7528\u4e8e\u50a8\u5b58size\u7684\u503c\uff09\u3002 \u5982\u679c ptr \u6307\u9488\u4e3aNULL\uff08\u5185\u5b58\u5206\u914d\u5931\u8d25\uff09\uff0c\u8c03\u7528 zmalloc_oom_handler\uff08size\uff09 \u3002\u8be5\u51fd\u6570\u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u51fd\u6570\u6307\u9488\u6307\u5411\u51fd\u6570 zmalloc_default_oom \uff0c\u5176\u4e3b\u8981\u529f\u80fd\u5c31\u662f\u6253\u5370\u9519\u8bef\u4fe1\u606f\u5e76\u7ec8\u6b62\u7a0b\u5e8f\u3002 // oom\u662fout of memory\uff08\u5185\u5b58\u4e0d\u8db3\uff09\u7684\u610f\u601d static void zmalloc_default_oom ( size_t size ) { fprintf ( stderr , \"zmalloc: Out of memory trying to allocate %zu bytes \\n \" , size ); fflush ( stderr ); abort (); } \u63a5\u4e0b\u6765\u662f\u5b8f\u7684\u6761\u4ef6\u7f16\u8bd1\uff0c\u6211\u4eec\u805a\u7126\u5728#else\u7684\u90e8\u5206\u3002 * (( size_t * ) ptr ) = size ; update_zmalloc_stat_alloc ( size + PREFIX_SIZE ); return ( char * ) ptr + PREFIX_SIZE ; \u7b2c\u4e00\u884c\u5c31\u662f\u5728\u5df2\u5206\u914d\u7a7a\u95f4\u7684\u7b2c\u4e00\u4e2a\u5b57\u957f\uff08\u524d8\u4e2a\u5b57\u8282\uff09\u5904\u5b58\u50a8\u9700\u8981\u5206\u914d\u7684\u5b57\u8282\u5927\u5c0f\uff08size\uff09\u3002 \u7b2c\u4e8c\u884c\u8c03\u7528\u4e86update_zmalloc_stat_alloc()\u3010\u5b8f\u51fd\u6570\u3011\uff0c\u5b83\u7684\u529f\u80fd\u662f\u66f4\u65b0\u5168\u5c40\u53d8\u91cf used_memory \uff08\u5df2\u5206\u914d\u5185\u5b58\u7684\u5927\u5c0f\uff09\u7684\u503c\uff08\u6e90\u7801\u89e3\u8bfb\u89c1\u4e0b\u4e00\u8282\uff09\u3002 \u7b2c\u4e09\u884c\u8fd4\u56de\u7684 \uff08char *\uff09ptr+PREFIX_SIZE \u3002\u5c31\u662f\u5c06\u5df2\u5206\u914d\u5185\u5b58\u7684\u8d77\u59cb\u5730\u5740\u5411\u53f3\u504f\u79fb PREFIX_SIZE * sizeof(char) \u7684\u957f\u5ea6\uff08\u53738\u4e2a\u5b57\u8282\uff09\uff0c\u6b64\u65f6\u5f97\u5230\u7684\u65b0\u6307\u9488\u6307\u5411\u7684\u5185\u5b58\u7a7a\u95f4\u7684\u5927\u5c0f\u5c31\u7b49\u4e8esize\u4e86\u3002 \u63a5\u4e0b\u6765\uff0c\u5206\u6790\u4e00\u4e0b update_zmalloc_stat_alloc \u7684\u6e90\u7801 update_zmalloc_stat_alloc \u6e90\u7801 #define update_zmalloc_stat_alloc(__n) do { \\ size_t _n = (__n); \\ if (_n&(sizeof(long)-1)) _n += sizeof(long)-(_n&(sizeof(long)-1)); \\ if (zmalloc_thread_safe) { \\ update_zmalloc_stat_add(_n); \\ } else { \\ used_memory += _n; \\ } \\ } while(0) \u8fd9\u4e2a**\u5b8f\u51fd\u6570**\u6700\u5916\u5708\u6709\u4e00\u4e2a do{...}while(0) \u5faa\u73af\u770b\u4f3c\u6beb\u65e0\u610f\u4e49\uff0c\u5b9e\u9645\u4e0a\u5927\u6709\u6df1\u610f\u3002\u8fd9\u90e8\u5206\u5185\u5bb9\u4e0d\u662f\u672c\u6587\u8ba8\u8bba\u7684\u91cd\u70b9\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002\u5177\u4f53\u8bf7\u770b\u7f51\u4e0a\u7684\u8fd9\u7bc7\u6587\u7ae0 http://www.spongeliu.com/415.html \u3002 \u56e0\u4e3a sizeof(long) = 8 \u301064\u4f4d\u7cfb\u7edf\u4e2d\u3011\uff0c\u6240\u4ee5\u4e0a\u9762\u7684\u7b2c\u4e00\u4e2a if \u8bed\u53e5\uff0c\u53ef\u4ee5\u7b49\u4ef7\u4e8e\u4ee5\u4e0b\u4ee3\u7801\uff1a if ( _n & 7 ) _n += 8 - ( n & 7 ); \u8fd9\u6bb5\u4ee3\u7801\u5c31\u662f\u5224\u65ad\u5206\u914d\u7684**\u5185\u5b58\u7a7a\u95f4**\u7684\u5927\u5c0f\u662f\u4e0d\u662f8\u7684\u500d\u6570\u3002\u5982\u679c\u5185\u5b58\u5927\u5c0f\u4e0d\u662f8\u7684\u500d\u6570\uff0c\u5c31\u52a0\u4e0a\u76f8\u5e94\u7684\u504f\u79fb\u91cf\u4f7f\u4e4b\u53d8\u62108\u7684\u500d\u6570\u3002 _n&7 \u5728\u529f\u80fd\u4e0a\u7b49\u4ef7\u4e8e _n%8 \uff0c\u4e0d\u8fc7\u4f4d\u64cd\u4f5c\u7684\u6548\u7387\u663e\u7136\u66f4\u9ad8\u3002 malloc() \u672c\u8eab\u80fd\u591f\u4fdd\u8bc1\u6240\u5206\u914d\u7684\u5185\u5b58\u662f**8\u5b57\u8282**\u5bf9\u9f50\u7684\uff1a\u5982\u679c\u4f60\u8981\u5206\u914d\u7684\u5185\u5b58\u4e0d\u662f8\u7684\u500d\u6570\uff0c\u90a3\u4e48 malloc \u5c31\u4f1a\u591a\u5206\u914d\u4e00\u70b9\uff0c\u6765\u51d1\u62108\u7684\u500d\u6570\u3002\u6240\u4ee5 update_zmalloc_stat_alloc \u51fd\u6570\uff08\u6216\u8005\u8bf4 zmalloc() \u76f8\u5bf9 malloc() \u800c\u8a00\uff09\u771f\u6b63\u8981\u5b9e\u73b0\u7684\u529f\u80fd\u5e76\u4e0d\u662f\u8fdb\u884c8\u5b57\u8282\u5bf9\u9f50\uff08 malloc \u5df2\u7ecf\u4fdd\u8bc1\u4e86\uff09\uff0c\u5b83\u7684\u771f\u6b63\u76ee\u7684\u662f\u4f7f\u53d8\u91cf used_memory \u7cbe\u786e\u7684\u7ef4\u62a4\u5b9e\u9645\u5df2\u5206\u914d\u5185\u5b58\u7684\u5927\u5c0f\u3002 \u7b2c2\u4e2aif\u7684\u6761\u4ef6\u662f\u4e00\u4e2a\u6574\u578b\u53d8\u91cf zmalloc_thread_safe \u3002\u987e\u540d\u601d\u4e49\uff0c\u5b83\u7684\u503c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff0c\u5982\u679c\u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff08 else \uff09\uff0c\u5c31\u7ed9\u53d8\u91cf used_memory \u52a0\u4e0a n \u3002 used_memory \u662f zmalloc.c \u6587\u4ef6\u4e2d\u5b9a\u4e49\u7684**\u5168\u5c40\u9759\u6001\u53d8\u91cf**\uff0c\u8868\u793a\u5df2\u5206\u914d\u5185\u5b58\u7684\u5927\u5c0f\u3002\u5982\u679c\u662f\u5185\u5b58\u5b89\u5168\u7684\u5c31\u4f7f\u7528 update_zmalloc_stat_add \u6765\u7ed9 used_memory \u52a0\u4e0a n \u3002 update_zmalloc_stat_add \u4e5f\u662f\u4e00\u4e2a\u5b8f\u51fd\u6570\uff08Redis\u6548\u7387\u4e4b\u9ad8\uff0c\u901f\u5ea6\u4e4b\u5feb\uff0c\u8fd9\u4e9b**\u5b8f\u51fd\u6570**\u53ef\u8c13\u529f\u4e0d\u53ef\u6ca1\uff09\u3002\u5b83\u4e5f\u662f\u4e00\u4e2a\u6761\u4ef6\u7f16\u8bd1\u7684\u5b8f\uff0c\u4f9d\u636e\u4e0d\u540c\u7684\u5b8f\u6709\u4e0d\u540c\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u6211\u4eec\u6765\u770b\u4e00\u4e0b #else \u540e\u9762\u7684\u5b9a\u4e49\u7684\u6e90\u7801\u3010 zmalloc.c \u6709\u591a\u5904\u6761\u4ef6\u7f16\u8bd1\u7684\u5b8f\uff0c\u4e3a\u4e86\u628a\u7cbe\u529b\u90fd\u96c6\u4e2d\u5728\u5185\u5b58\u7ba1\u7406\u7684\u5b9e\u73b0\u7b97\u6cd5\u4e0a\uff0c\u8fd9\u91cc\u6211\u53ea\u5173\u6ce8Linux\u5e73\u53f0\u4e0b\u4f7f\u7528 glibc \u7684 malloc \u7684\u60c5\u51b5\u3011\u3002 #define update_zmalloc_stat_add(__n) do { \\ pthread_mutex_lock(&used_memory_mutex); \\ used_memory += (__n); \\ pthread_mutex_unlock(&used_memory_mutex); \\ } while(0) pthread_mutex_lock() \u548c pthread_mutex_unlock() \u4f7f\u7528\u4e92\u65a5\u9501\uff08mutex\uff09\u6765\u5b9e\u73b0\u7ebf\u7a0b\u540c\u6b65\uff0c\u524d\u8005\u8868\u793a\u52a0\u9501\uff0c\u540e\u8005\u8868\u793a\u89e3\u9501\uff0c\u5b83\u4eec\u662fPOSIX\u5b9a\u4e49\u7684\u7ebf\u7a0b\u540c\u6b65\u51fd\u6570\u3002\u5f53\u52a0\u9501\u4ee5\u540e\u5b83\u540e\u9762\u7684\u4ee3\u7801\u5728\u591a\u7ebf\u7a0b\u540c\u65f6\u6267\u884c\u8fd9\u6bb5\u4ee3\u7801\u7684\u65f6\u5019\u5c31\u53ea\u4f1a\u6267\u884c\u4e00\u6b21\uff0c\u4e5f\u5c31\u662f\u5b9e\u73b0\u4e86**\u7ebf\u7a0b\u5b89\u5168**\u3002 zfree zfree() \u548c free() \u6709\u76f8\u540c\u7684\u7f16\u7a0b\u63a5\u53e3\uff0c\u5b83\u8d1f\u8d23\u6e05\u9664 zmalloc() \u5206\u914d\u7684\u7a7a\u95f4\u3002 \u8f85\u52a9\u51fd\u6570: free() update_zmalloc_free() \u3010\u5b8f\u51fd\u6570\u3011 update_zmalloc_sub() \u3010\u5b8f\u51fd\u6570\u3011 zmalloc_size() zfree()\u6e90\u7801 void zfree ( void * ptr ) { #ifndef HAVE_MALLOC_SIZE void * realptr ; size_t oldsize ; #endif if ( ptr == NULL ) return ; #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_free ( zmalloc_size ( ptr )); free ( ptr ); #else realptr = ( char * ) ptr - PREFIX_SIZE ; oldsize = * (( size_t * ) realptr ); update_zmalloc_stat_free ( oldsize + PREFIX_SIZE ); free ( realptr ); #endif } \u91cd\u70b9\u5173\u6ce8 #else \u540e\u9762\u7684\u4ee3\u7801 realptr = ( char * ) ptr - PREFIX_SIZE ; \u8868\u793a\u7684\u662f ptr \u6307\u9488\u5411\u524d\u504f\u79fb8\u4e2a\u5b57\u8282\u7684\u957f\u5ea6\uff0c\u5373\u56de\u9000\u5230\u6700\u521d malloc \u8fd4\u56de\u7684\u5730\u5740\uff0c\u8fd9\u91cc\u79f0\u4e3a realptr \u3002\u7136\u540e oldsize = (( size_t ) realptr ); \u5148\u8fdb\u884c**\u7c7b\u578b\u8f6c\u6362**\u518d\u53d6\u6307\u9488\u6240\u6307\u5411\u7684\u503c\u3002\u901a\u8fc7 zmalloc() \u51fd\u6570\u7684\u5206\u6790\uff0c\u53ef\u77e5\u8fd9\u91cc\u5b58\u50a8\u7740\u6211\u4eec\u6700\u521d\u9700\u8981\u5206\u914d\u7684\u5185\u5b58\u5927\u5c0f\uff08 zmalloc \u4e2d\u7684 size \uff09\uff0c\u8fd9\u91cc\u8d4b\u503c\u4e2a oldsize update_zmalloc_stat_free ( oldsize + PREFIX_SIZE ); update_zmalloc_stat_free() \u4e5f\u662f\u4e00\u4e2a\u5b8f\u51fd\u6570\uff0c\u548c zmalloc \u4e2d update_zmalloc_stat_alloc() \u5927\u81f4\u76f8\u540c\uff0c\u552f\u4e00\u4e0d\u540c\u4e4b\u5904\u662f\u524d\u8005\u5728\u7ed9\u53d8\u91cf used_memory \u51cf\u53bb\u5206\u914d\u7684\u7a7a\u95f4\uff0c\u800c\u540e\u8005\u662f\u52a0\u4e0a\u8be5\u7a7a\u95f4\u5927\u5c0f\u3002 \u6700\u540e free(realptr) \uff0c\u6e05\u9664\u7a7a\u95f4 update_zmalloc_free\u6e90\u7801 #define update_zmalloc_stat_free(__n) do { \\ size_t _n = (__n); \\ if (_n&(sizeof(long)-1)) _n += sizeof(long)-(_n&(sizeof(long)-1)); \\ if (zmalloc_thread_safe) { \\ update_zmalloc_stat_sub(_n); \\ } else { \\ used_memory -= _n; \\ } \\ } while(0) \u5176\u4e2d\u7684\u51fd\u6570 update_zmalloc_sub \u4e0e zmalloc() \u4e2d\u7684 update_zmalloc_add \u76f8\u5bf9\u5e94\uff0c\u4f46\u529f\u80fd\u76f8\u53cd\uff0c\u63d0\u4f9b\u7ebf\u7a0b\u5b89\u5168\u5730 used_memory \u51cf\u6cd5\u64cd\u4f5c\u3002 #define update_zmalloc_stat_sub(__n) do { \\ pthread_mutex_lock(&used_memory_mutex); \\ used_memory -= (__n); \\ pthread_mutex_unlock(&used_memory_mutex); \\ } while(0) zcalloc zcalloc() \u7684\u5b9e\u73b0\u57fa\u4e8e calloc() \uff0c\u4f46\u662f\u4e24\u8005\u7f16\u7a0b\u63a5\u53e3\u4e0d\u540c\u3002\u770b\u4e00\u4e0b\u5bf9\u6bd4\uff1a void * calloc ( size_t nmemb , size_t size ); void * zcalloc ( size_t size ); calloc() \u7684\u529f\u80fd\u662f\u4e5f\u662f\u5206\u914d\u5185\u5b58\u7a7a\u95f4\uff0c\u4e0e malloc() \u7684\u4e0d\u540c\u4e4b\u5904\u6709\u4e24\u70b9\uff1a \u5b83\u5206\u914d\u7684\u7a7a\u95f4\u5927\u5c0f\u662f size * nmemb \u3002\u6bd4\u5982 calloc(10,sizoef(char)); // \u5206\u914d10\u4e2a\u5b57\u8282 calloc() \u4f1a\u5bf9\u5206\u914d\u7684\u7a7a\u95f4\u505a\u521d\u59cb\u5316\u5de5\u4f5c\uff08\u521d\u59cb\u5316\u4e3a0\uff09\uff0c\u800c malloc() \u4e0d\u4f1a \u8f85\u52a9\u51fd\u6570 calloc() update_zmalloc_stat_alloc()\u3010\u5b8f\u51fd\u6570\u3011 update_zmalloc_stat_add()\u3010\u5b8f\u51fd\u6570\u3011 zcalloc()\u6e90\u7801 void * zcalloc ( size_t size ) { void * ptr = calloc ( 1 , size + PREFIX_SIZE ); if ( ! ptr ) zmalloc_oom_handler ( size ); #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_alloc ( zmalloc_size ( ptr )); return ptr ; #else * (( size_t * ) ptr ) = size ; update_zmalloc_stat_alloc ( size + PREFIX_SIZE ); return ( char * ) ptr + PREFIX_SIZE ; #endif } zcalloc() \u4e2d\u6ca1\u6709 calloc() \u7684\u7b2c\u4e00\u4e2a\u51fd\u6570 nmemb \u3002\u56e0\u4e3a\u5b83\u6bcf\u6b21\u8c03\u7528 calloc() ,\u5176\u7b2c\u4e00\u4e2a\u53c2\u6570\u90fd\u662f1\u3002\u4e5f\u5c31\u662f\u8bf4 zcalloc() \u529f\u80fd\u662f\u6bcf\u6b21\u5206\u914d size+PREFIX_SIZE \u7684\u7a7a\u95f4\uff0c\u5e76\u521d\u59cb\u5316\u3002 \u5176\u4f59\u4ee3\u7801\u7684\u5206\u6790\u548c zmalloc() \u76f8\u540c\uff0c\u4e5f\u5c31\u662f\u8bf4\uff1a zcalloc() \u548c zmalloc() \u5177\u6709\u76f8\u540c\u7684\u7f16\u7a0b\u63a5\u53e3\uff0c\u5b9e\u73b0\u529f\u80fd\u57fa\u672c\u76f8\u540c\uff0c\u552f\u4e00\u4e0d\u540c\u4e4b\u5904\u662f zcalloc() \u4f1a\u505a\u521d\u59cb\u5316\u5de5\u4f5c\uff0c\u800c zmalloc() \u4e0d\u4f1a\u3002 zrealloc zrealloc() \u548c realloc() \u5177\u6709\u76f8\u540c\u7684\u7f16\u7a0b\u63a5\u53e3\uff1a void * realloc ( void * ptr , size_t size ); void * zrealloc ( void * ptr , size_t size ); realloc() \u8981\u5b8c\u6210\u7684\u529f\u80fd\u662f\u7ed9\u9996\u5730\u5740 ptr \u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u91cd\u65b0\u5206\u914d\u5927\u5c0f\u3002\u5982\u679c\u5931\u8d25\u4e86\uff0c\u5219\u5728\u5176\u5b83\u4f4d\u7f6e\u65b0\u5efa\u4e00\u5757\u5927\u5c0f\u4e3a size \u5b57\u8282\u7684\u7a7a\u95f4\uff0c\u5c06\u539f\u5148\u7684\u6570\u636e\u590d\u5236\u5230\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u5e76\u8fd4\u56de\u8fd9\u6bb5\u5185\u5b58\u9996\u5730\u5740\u3010\u539f\u5185\u5b58\u4f1a\u88ab\u7cfb\u7edf\u81ea\u7136\u91ca\u653e\u3011\u3002 zrealloc() \u8981\u5b8c\u6210\u7684\u529f\u80fd\u4e5f\u7c7b\u4f3c\u3002 \u8f85\u52a9\u51fd\u6570\uff1a zmalloc() zmalloc_size() realloc() zmalloc_oom_handler \u3010\u51fd\u6570\u6307\u9488\u3011 update_zmalloc_stat_free() \u3010\u5b8f\u51fd\u6570\u3011 update_zmalloc_stat_alloc() \u3010\u5b8f\u51fd\u6570\u3011 zrealloc()\u6e90\u7801 void * zrealloc ( void * ptr , size_t size ) { #ifndef HAVE_MALLOC_SIZE void * realptr ; #endif size_t oldsize ; void * newptr ; if ( ptr == NULL ) return zmalloc ( size ); #ifdef HAVE_MALLOC_SIZE oldsize = zmalloc_size ( ptr ); newptr = realloc ( ptr , size ); if ( ! newptr ) zmalloc_oom_handler ( size ); update_zmalloc_stat_free ( oldsize ); update_zmalloc_stat_alloc ( zmalloc_size ( newptr )); return newptr ; #else realptr = ( char * ) ptr - PREFIX_SIZE ; oldsize = * (( size_t * ) realptr ); newptr = realloc ( realptr , size + PREFIX_SIZE ); if ( ! newptr ) zmalloc_oom_handler ( size ); * (( size_t * ) newptr ) = size ; update_zmalloc_stat_free ( oldsize ); update_zmalloc_stat_alloc ( size ); return ( char * ) newptr + PREFIX_SIZE ; #endif } \u7ecf\u8fc7\u524d\u9762\u5173\u4e8e zmalloc() \u548c zfree() \u7684\u6e90\u7801\u89e3\u8bfb\uff0c\u76f8\u4fe1\u60a8\u4e00\u5b9a\u80fd\u591f\u5f88\u8f7b\u677e\u5730\u8bfb\u61c2 zrealloc() \u7684\u6e90\u7801\uff0c\u8fd9\u91cc\u6211\u5c31\u4e0d\u8d58\u8ff0\u4e86\u3002 zstrdup \u4ece\u8fd9\u4e2a\u51fd\u6570\u540d\u4e2d\uff0c\u5f88\u5bb9\u6613\u53d1\u73b0\u5b83\u662fstring duplicate\u7684\u7f29\u5199\uff0c\u5373\u5b57\u7b26\u4e32\u590d\u5236\u3002\u5b83\u7684\u4ee3\u7801\u6bd4\u8f83\u7b80\u5355\u3002\u5148\u770b\u4e00\u4e0b\u58f0\u660e\uff1a char * zstrdup ( const char * s ); \u529f\u80fd\u63cf\u8ff0\uff1a\u590d\u5236\u5b57\u7b26\u4e32 s \u7684\u5185\u5bb9\uff0c\u5230\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u6784\u9020\u65b0\u7684\u5b57\u7b26\u4e32\u3010\u5806\u533a\u3011\u3002\u5e76\u5c06\u8fd9\u6bb5\u65b0\u7684\u5b57\u7b26\u4e32\u5730\u5740\u8fd4\u56de\u3002 zstrdup\u6e90\u7801 char * zstrdup ( const char * s ) { size_t l = strlen ( s ) + 1 ; char * p = zmalloc ( l ); memcpy ( p , s , l ); return p ; } \u9996\u5148\uff0c\u5148\u83b7\u5f97\u5b57\u7b26\u4e32s\u7684\u957f\u5ea6\uff0c\u65b0\u95fbstrlen()\u51fd\u6570\u662f\u4e0d\u7edf\u8ba1'\\0'\u7684\uff0c\u6240\u4ee5\u6700\u540e\u8981\u52a01\u3002 \u7136\u540e\u8c03\u7528zmalloc()\u6765\u5206\u914d\u8db3\u591f\u7684\u7a7a\u95f4\uff0c\u9996\u5730\u5740\u4e3ap\u3002 \u8c03\u7528memcpy\u6765\u5b8c\u6210\u590d\u5236\u3002 \u7136\u540e\u8fd4\u56dep\u3002 \u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b memcpy memcpy \u8fd9\u662f\u6807\u51c6C\u3010ANSI C\u3011\u4e2d\u7528\u4e8e\u5185\u5b58\u590d\u5236\u7684\u51fd\u6570\uff0c\u5728\u5934\u6587\u4ef6 <string.h> \u4e2d\uff08gcc\uff09\u3002\u58f0\u660e\u5982\u4e0b\uff1a void * memcpy ( void * dest , const void * src , size_t n ); dest\u5373\u76ee\u7684\u5730\u5740\uff0csrc\u662f\u6e90\u5730\u5740\u3002n\u662f\u8981\u590d\u5236\u7684\u5b57\u8282\u6570\u3002","title":"Introduction"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#csdn#rediszmallcc","text":"\u4f5c\u8005\uff1a\u679c\u51bb\u867e\u4ec1 \u6765\u6e90\uff1aCSDN \u539f\u6587\uff1a https://blog.csdn.net/guodongxiaren/article/details/44747719 \u7248\u6743\u58f0\u660e\uff1a\u672c\u6587\u4e3a\u535a\u4e3b\u539f\u521b\u6587\u7ae0\uff0c\u8f6c\u8f7d\u8bf7\u9644\u4e0a\u535a\u6587\u94fe\u63a5\uff01 \u5f53\u6211\u7b2c\u4e00\u6b21\u9605\u8bfb\u4e86\u8fd9\u4e2a\u6587\u4ef6\u7684\u6e90\u7801\u7684\u65f6\u5019\uff0c\u6211\u7b11\u4e86\uff0c\u5ffd\u7136\u60f3\u8d77\u524d\u51e0\u5468\u963f\u91cc\u7535\u8bdd\u4e8c\u9762\u7684\u65f6\u5019\uff0c\u95ee\u5230\u4e86**\u81ea\u5b9a\u4e49\u5185\u5b58\u7ba1\u7406\u51fd\u6570**\u5e76\u5904\u7406**8\u5b57\u8282\u5bf9\u9f50\u95ee\u9898**\u3002\u5f53\u65f6\u65e0\u8a00\u4ee5\u5bf9\uff0c\u5728\u9762\u8bd5\u5b98\u65e0\u6570\u6b21\u7684\u63d0\u793a\u4e0b\u624d\u7b54\u4e86\u51fa\u6765\uff0c\u7ed3\u679c\u663e\u800c\u6613\u89c1\uff0c\u6302\u6389\u4e86\u4e8c\u9762\u3002\u800c\u8fd9\u4efd\u6e90\u7801\u4e2d\u51fd\u6570 zmalloc() \u548c zfree() \u7684\u8bbe\u8ba1\u601d\u8def\u548c\u5b9e\u73b0\u539f\u7406\uff0c\u6b63\u662f\u9762\u8bd5\u5b98\u60f3\u8981\u7684\u7b54\u6848\u3002","title":"csdn Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#_1","text":"zmalloc.c \u6587\u4ef6\u7684\u5185\u5bb9\u5982\u4e0b\uff1a \u4e3b\u8981\u51fd\u6570 zmalloc() zfree() zcalloc() zrelloc() zstrdup()","title":"\u6e90\u7801\u7ed3\u6784"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#_2","text":"CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u79f0\u4e3a**\u5b57\u957f**\uff0c\u4e5f\u5c31\u662f\u6211\u4eec\u901a\u5e38\u6240\u8bf4\u768432\u4f4d\u7cfb\u7edf\uff08\u5b57\u957f4\u4e2a\u5b57\u8282\uff09\u300164\u4f4d\u7cfb\u7edf\uff08\u5b57\u957f8\u4e2a\u5b57\u8282\uff09\u7684\u7531\u6765\u3002\u6240\u8c13\u7684**8\u5b57\u8282\u5bf9\u9f50**\uff0c\u5c31\u662f\u6307**\u53d8\u91cf\u7684\u8d77\u59cb\u5730\u5740**\u662f8\u7684\u500d\u6570\u3002\u6bd4\u5982\u7a0b\u5e8f\u8fd0\u884c\u65f6\uff08CPU\uff09\u5728\u8bfb\u53d6 long \u578b\u6570\u636e\u7684\u65f6\u5019\uff0c\u53ea\u9700\u8981\u4e00\u4e2a\u603b\u7ebf\u5468\u671f\uff0c\u65f6\u95f4\u66f4\u77ed\uff0c\u5982\u679c\u4e0d\u662f8\u5b57\u8282\u5bf9\u9f50\u7684\u5219\u9700\u8981\u4e24\u4e2a\u603b\u7ebf\u5468\u671f\u624d\u80fd\u8bfb\u5b8c\u6570\u636e\u3002 \u672c\u6587\u4e2d\u6211\u63d0\u5230\u76848\u5b57\u8282\u5bf9\u9f50\u662f\u9488\u5bf964\u4f4d\u7cfb\u7edf\u800c\u8a00\u7684\uff0c\u5982\u679c\u662f32\u4f4d\u7cfb\u7edf\u90a3\u4e48\u5c31\u662f4\u5b57\u8282\u5bf9\u9f50\u3002\u5b9e\u9645\u4e0aRedis\u6e90\u7801\u4e2d\u7684**\u5b57\u8282\u5bf9\u9f50**\u662f\u8f6f\u7f16\u7801\uff0c\u800c\u975e\u786c\u7f16\u7801\u3002\u91cc\u9762\u591a\u7528 sizeof(long) \u6216 sizeof(size_t) \u6765\u8868\u793a\u3002 size_t \uff08gcc\u4e2d\u5176\u503c\u4e3a long unsigned int \uff09\u548c long \u7684\u957f\u5ea6\u662f\u4e00\u6837\u7684\uff0c long \u7684\u957f\u5ea6\u5c31\u662f\u8ba1\u7b97\u673a\u7684\u5b57\u957f\u3002\u8fd9\u6837\u5728\u672a\u6765\u7684\u7cfb\u7edf\u4e2d\u5982\u679c\u5b57\u957f\uff08long\u7684\u5927\u5c0f\uff09\u4e0d\u662f8\u4e2a\u5b57\u8282\u4e86\uff0c\u8be5\u6bb5\u4ee3\u7801\u4f9d\u7136\u80fd\u4fdd\u8bc1\u76f8\u5e94\u4ee3\u7801\u53ef\u7528\u3002 NOTE: Data structure alignment","title":"\u5b57\u957f\u4e0e\u5b57\u8282\u5bf9\u9f50"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zmalloc","text":"\u8f85\u52a9\u7684\u51fd\u6570\uff1a 1\u3001 malloc() 2\u3001 zmalloc_oom_handler \u3010\u51fd\u6570\u6307\u9488\u3011 3\u3001 zmalloc_default_oom() \u3010\u88ab\u4e0a\u9762\u7684\u51fd\u6570\u6307\u9488\u6240\u6307\u5411\u3011 4\u3001 update_zmalloc_stat_alloc() \u3010\u5b8f\u51fd\u6570\u3011 5\u3001 update_zmalloc_stat_add() \u3010\u5b8f\u51fd\u6570\u3011 zmalloc() \u548c malloc() \u6709\u76f8\u540c\u7684\u51fd\u6570\u63a5\u53e3\uff08\u53c2\u6570\uff0c\u8fd4\u56de\u503c\uff09\u3002","title":"zmalloc"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zmalloc_1","text":"```c void *zmalloc(size_t size) { void *ptr = malloc(size+PREFIX_SIZE); if (!ptr) zmalloc_oom_handler(size);","title":"zmalloc()\u6e90\u7801"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#ifdef#have_malloc_size","text":"update_zmalloc_stat_alloc(zmalloc_size(ptr)); return ptr;","title":"ifdef HAVE_MALLOC_SIZE"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#else","text":"*((size_t*)ptr) = size; update_zmalloc_stat_alloc(size+PREFIX_SIZE); return (char*)ptr+PREFIX_SIZE;","title":"else"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#endif","text":"} ``` \u53c2\u6570 size \u662f\u6211\u4eec**\u9700\u8981\u5206\u914d**\u7684\u5185\u5b58\u5927\u5c0f\u3002\u5b9e\u9645\u4e0a\u6211\u4eec\u8c03\u7528 malloc **\u5b9e\u9645\u5206\u914d**\u7684\u5927\u5c0f\u662f size+PREFIX_SIZE \u3002 PREFIX_SIZE \u662f\u4e00\u4e2a\u6761\u4ef6\u7f16\u8bd1\u7684\u5b8f\uff0c\u4e0d\u540c\u7684\u5e73\u53f0\u6709\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u5728Linux\u4e2d\u5176\u503c\u662f sizeof(size_t) \uff0c\u6240\u4ee5\u6211\u4eec\u591a\u5206\u914d\u4e86\u4e00\u4e2a\u5b57\u957f(8\u4e2a\u5b57\u8282)\u7684\u7a7a\u95f4\uff08\u540e\u9762\u4ee3\u7801\u53ef\u4ee5\u770b\u5230\u591a\u5206\u914d8\u4e2a\u5b57\u8282\u7684\u76ee\u7684\u662f\u7528\u4e8e\u50a8\u5b58size\u7684\u503c\uff09\u3002 \u5982\u679c ptr \u6307\u9488\u4e3aNULL\uff08\u5185\u5b58\u5206\u914d\u5931\u8d25\uff09\uff0c\u8c03\u7528 zmalloc_oom_handler\uff08size\uff09 \u3002\u8be5\u51fd\u6570\u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u51fd\u6570\u6307\u9488\u6307\u5411\u51fd\u6570 zmalloc_default_oom \uff0c\u5176\u4e3b\u8981\u529f\u80fd\u5c31\u662f\u6253\u5370\u9519\u8bef\u4fe1\u606f\u5e76\u7ec8\u6b62\u7a0b\u5e8f\u3002 // oom\u662fout of memory\uff08\u5185\u5b58\u4e0d\u8db3\uff09\u7684\u610f\u601d static void zmalloc_default_oom ( size_t size ) { fprintf ( stderr , \"zmalloc: Out of memory trying to allocate %zu bytes \\n \" , size ); fflush ( stderr ); abort (); } \u63a5\u4e0b\u6765\u662f\u5b8f\u7684\u6761\u4ef6\u7f16\u8bd1\uff0c\u6211\u4eec\u805a\u7126\u5728#else\u7684\u90e8\u5206\u3002 * (( size_t * ) ptr ) = size ; update_zmalloc_stat_alloc ( size + PREFIX_SIZE ); return ( char * ) ptr + PREFIX_SIZE ; \u7b2c\u4e00\u884c\u5c31\u662f\u5728\u5df2\u5206\u914d\u7a7a\u95f4\u7684\u7b2c\u4e00\u4e2a\u5b57\u957f\uff08\u524d8\u4e2a\u5b57\u8282\uff09\u5904\u5b58\u50a8\u9700\u8981\u5206\u914d\u7684\u5b57\u8282\u5927\u5c0f\uff08size\uff09\u3002 \u7b2c\u4e8c\u884c\u8c03\u7528\u4e86update_zmalloc_stat_alloc()\u3010\u5b8f\u51fd\u6570\u3011\uff0c\u5b83\u7684\u529f\u80fd\u662f\u66f4\u65b0\u5168\u5c40\u53d8\u91cf used_memory \uff08\u5df2\u5206\u914d\u5185\u5b58\u7684\u5927\u5c0f\uff09\u7684\u503c\uff08\u6e90\u7801\u89e3\u8bfb\u89c1\u4e0b\u4e00\u8282\uff09\u3002 \u7b2c\u4e09\u884c\u8fd4\u56de\u7684 \uff08char *\uff09ptr+PREFIX_SIZE \u3002\u5c31\u662f\u5c06\u5df2\u5206\u914d\u5185\u5b58\u7684\u8d77\u59cb\u5730\u5740\u5411\u53f3\u504f\u79fb PREFIX_SIZE * sizeof(char) \u7684\u957f\u5ea6\uff08\u53738\u4e2a\u5b57\u8282\uff09\uff0c\u6b64\u65f6\u5f97\u5230\u7684\u65b0\u6307\u9488\u6307\u5411\u7684\u5185\u5b58\u7a7a\u95f4\u7684\u5927\u5c0f\u5c31\u7b49\u4e8esize\u4e86\u3002 \u63a5\u4e0b\u6765\uff0c\u5206\u6790\u4e00\u4e0b update_zmalloc_stat_alloc \u7684\u6e90\u7801","title":"endif"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#update_zmalloc_stat_alloc","text":"#define update_zmalloc_stat_alloc(__n) do { \\ size_t _n = (__n); \\ if (_n&(sizeof(long)-1)) _n += sizeof(long)-(_n&(sizeof(long)-1)); \\ if (zmalloc_thread_safe) { \\ update_zmalloc_stat_add(_n); \\ } else { \\ used_memory += _n; \\ } \\ } while(0) \u8fd9\u4e2a**\u5b8f\u51fd\u6570**\u6700\u5916\u5708\u6709\u4e00\u4e2a do{...}while(0) \u5faa\u73af\u770b\u4f3c\u6beb\u65e0\u610f\u4e49\uff0c\u5b9e\u9645\u4e0a\u5927\u6709\u6df1\u610f\u3002\u8fd9\u90e8\u5206\u5185\u5bb9\u4e0d\u662f\u672c\u6587\u8ba8\u8bba\u7684\u91cd\u70b9\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002\u5177\u4f53\u8bf7\u770b\u7f51\u4e0a\u7684\u8fd9\u7bc7\u6587\u7ae0 http://www.spongeliu.com/415.html \u3002 \u56e0\u4e3a sizeof(long) = 8 \u301064\u4f4d\u7cfb\u7edf\u4e2d\u3011\uff0c\u6240\u4ee5\u4e0a\u9762\u7684\u7b2c\u4e00\u4e2a if \u8bed\u53e5\uff0c\u53ef\u4ee5\u7b49\u4ef7\u4e8e\u4ee5\u4e0b\u4ee3\u7801\uff1a if ( _n & 7 ) _n += 8 - ( n & 7 ); \u8fd9\u6bb5\u4ee3\u7801\u5c31\u662f\u5224\u65ad\u5206\u914d\u7684**\u5185\u5b58\u7a7a\u95f4**\u7684\u5927\u5c0f\u662f\u4e0d\u662f8\u7684\u500d\u6570\u3002\u5982\u679c\u5185\u5b58\u5927\u5c0f\u4e0d\u662f8\u7684\u500d\u6570\uff0c\u5c31\u52a0\u4e0a\u76f8\u5e94\u7684\u504f\u79fb\u91cf\u4f7f\u4e4b\u53d8\u62108\u7684\u500d\u6570\u3002 _n&7 \u5728\u529f\u80fd\u4e0a\u7b49\u4ef7\u4e8e _n%8 \uff0c\u4e0d\u8fc7\u4f4d\u64cd\u4f5c\u7684\u6548\u7387\u663e\u7136\u66f4\u9ad8\u3002 malloc() \u672c\u8eab\u80fd\u591f\u4fdd\u8bc1\u6240\u5206\u914d\u7684\u5185\u5b58\u662f**8\u5b57\u8282**\u5bf9\u9f50\u7684\uff1a\u5982\u679c\u4f60\u8981\u5206\u914d\u7684\u5185\u5b58\u4e0d\u662f8\u7684\u500d\u6570\uff0c\u90a3\u4e48 malloc \u5c31\u4f1a\u591a\u5206\u914d\u4e00\u70b9\uff0c\u6765\u51d1\u62108\u7684\u500d\u6570\u3002\u6240\u4ee5 update_zmalloc_stat_alloc \u51fd\u6570\uff08\u6216\u8005\u8bf4 zmalloc() \u76f8\u5bf9 malloc() \u800c\u8a00\uff09\u771f\u6b63\u8981\u5b9e\u73b0\u7684\u529f\u80fd\u5e76\u4e0d\u662f\u8fdb\u884c8\u5b57\u8282\u5bf9\u9f50\uff08 malloc \u5df2\u7ecf\u4fdd\u8bc1\u4e86\uff09\uff0c\u5b83\u7684\u771f\u6b63\u76ee\u7684\u662f\u4f7f\u53d8\u91cf used_memory \u7cbe\u786e\u7684\u7ef4\u62a4\u5b9e\u9645\u5df2\u5206\u914d\u5185\u5b58\u7684\u5927\u5c0f\u3002 \u7b2c2\u4e2aif\u7684\u6761\u4ef6\u662f\u4e00\u4e2a\u6574\u578b\u53d8\u91cf zmalloc_thread_safe \u3002\u987e\u540d\u601d\u4e49\uff0c\u5b83\u7684\u503c\u8868\u793a\u64cd\u4f5c\u662f\u5426\u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff0c\u5982\u679c\u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684\uff08 else \uff09\uff0c\u5c31\u7ed9\u53d8\u91cf used_memory \u52a0\u4e0a n \u3002 used_memory \u662f zmalloc.c \u6587\u4ef6\u4e2d\u5b9a\u4e49\u7684**\u5168\u5c40\u9759\u6001\u53d8\u91cf**\uff0c\u8868\u793a\u5df2\u5206\u914d\u5185\u5b58\u7684\u5927\u5c0f\u3002\u5982\u679c\u662f\u5185\u5b58\u5b89\u5168\u7684\u5c31\u4f7f\u7528 update_zmalloc_stat_add \u6765\u7ed9 used_memory \u52a0\u4e0a n \u3002 update_zmalloc_stat_add \u4e5f\u662f\u4e00\u4e2a\u5b8f\u51fd\u6570\uff08Redis\u6548\u7387\u4e4b\u9ad8\uff0c\u901f\u5ea6\u4e4b\u5feb\uff0c\u8fd9\u4e9b**\u5b8f\u51fd\u6570**\u53ef\u8c13\u529f\u4e0d\u53ef\u6ca1\uff09\u3002\u5b83\u4e5f\u662f\u4e00\u4e2a\u6761\u4ef6\u7f16\u8bd1\u7684\u5b8f\uff0c\u4f9d\u636e\u4e0d\u540c\u7684\u5b8f\u6709\u4e0d\u540c\u7684\u5b9a\u4e49\uff0c\u8fd9\u91cc\u6211\u4eec\u6765\u770b\u4e00\u4e0b #else \u540e\u9762\u7684\u5b9a\u4e49\u7684\u6e90\u7801\u3010 zmalloc.c \u6709\u591a\u5904\u6761\u4ef6\u7f16\u8bd1\u7684\u5b8f\uff0c\u4e3a\u4e86\u628a\u7cbe\u529b\u90fd\u96c6\u4e2d\u5728\u5185\u5b58\u7ba1\u7406\u7684\u5b9e\u73b0\u7b97\u6cd5\u4e0a\uff0c\u8fd9\u91cc\u6211\u53ea\u5173\u6ce8Linux\u5e73\u53f0\u4e0b\u4f7f\u7528 glibc \u7684 malloc \u7684\u60c5\u51b5\u3011\u3002 #define update_zmalloc_stat_add(__n) do { \\ pthread_mutex_lock(&used_memory_mutex); \\ used_memory += (__n); \\ pthread_mutex_unlock(&used_memory_mutex); \\ } while(0) pthread_mutex_lock() \u548c pthread_mutex_unlock() \u4f7f\u7528\u4e92\u65a5\u9501\uff08mutex\uff09\u6765\u5b9e\u73b0\u7ebf\u7a0b\u540c\u6b65\uff0c\u524d\u8005\u8868\u793a\u52a0\u9501\uff0c\u540e\u8005\u8868\u793a\u89e3\u9501\uff0c\u5b83\u4eec\u662fPOSIX\u5b9a\u4e49\u7684\u7ebf\u7a0b\u540c\u6b65\u51fd\u6570\u3002\u5f53\u52a0\u9501\u4ee5\u540e\u5b83\u540e\u9762\u7684\u4ee3\u7801\u5728\u591a\u7ebf\u7a0b\u540c\u65f6\u6267\u884c\u8fd9\u6bb5\u4ee3\u7801\u7684\u65f6\u5019\u5c31\u53ea\u4f1a\u6267\u884c\u4e00\u6b21\uff0c\u4e5f\u5c31\u662f\u5b9e\u73b0\u4e86**\u7ebf\u7a0b\u5b89\u5168**\u3002","title":"update_zmalloc_stat_alloc\u6e90\u7801"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zfree","text":"zfree() \u548c free() \u6709\u76f8\u540c\u7684\u7f16\u7a0b\u63a5\u53e3\uff0c\u5b83\u8d1f\u8d23\u6e05\u9664 zmalloc() \u5206\u914d\u7684\u7a7a\u95f4\u3002 \u8f85\u52a9\u51fd\u6570: free() update_zmalloc_free() \u3010\u5b8f\u51fd\u6570\u3011 update_zmalloc_sub() \u3010\u5b8f\u51fd\u6570\u3011 zmalloc_size()","title":"zfree"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zfree_1","text":"void zfree ( void * ptr ) { #ifndef HAVE_MALLOC_SIZE void * realptr ; size_t oldsize ; #endif if ( ptr == NULL ) return ; #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_free ( zmalloc_size ( ptr )); free ( ptr ); #else realptr = ( char * ) ptr - PREFIX_SIZE ; oldsize = * (( size_t * ) realptr ); update_zmalloc_stat_free ( oldsize + PREFIX_SIZE ); free ( realptr ); #endif } \u91cd\u70b9\u5173\u6ce8 #else \u540e\u9762\u7684\u4ee3\u7801 realptr = ( char * ) ptr - PREFIX_SIZE ; \u8868\u793a\u7684\u662f ptr \u6307\u9488\u5411\u524d\u504f\u79fb8\u4e2a\u5b57\u8282\u7684\u957f\u5ea6\uff0c\u5373\u56de\u9000\u5230\u6700\u521d malloc \u8fd4\u56de\u7684\u5730\u5740\uff0c\u8fd9\u91cc\u79f0\u4e3a realptr \u3002\u7136\u540e oldsize = (( size_t ) realptr ); \u5148\u8fdb\u884c**\u7c7b\u578b\u8f6c\u6362**\u518d\u53d6\u6307\u9488\u6240\u6307\u5411\u7684\u503c\u3002\u901a\u8fc7 zmalloc() \u51fd\u6570\u7684\u5206\u6790\uff0c\u53ef\u77e5\u8fd9\u91cc\u5b58\u50a8\u7740\u6211\u4eec\u6700\u521d\u9700\u8981\u5206\u914d\u7684\u5185\u5b58\u5927\u5c0f\uff08 zmalloc \u4e2d\u7684 size \uff09\uff0c\u8fd9\u91cc\u8d4b\u503c\u4e2a oldsize update_zmalloc_stat_free ( oldsize + PREFIX_SIZE ); update_zmalloc_stat_free() \u4e5f\u662f\u4e00\u4e2a\u5b8f\u51fd\u6570\uff0c\u548c zmalloc \u4e2d update_zmalloc_stat_alloc() \u5927\u81f4\u76f8\u540c\uff0c\u552f\u4e00\u4e0d\u540c\u4e4b\u5904\u662f\u524d\u8005\u5728\u7ed9\u53d8\u91cf used_memory \u51cf\u53bb\u5206\u914d\u7684\u7a7a\u95f4\uff0c\u800c\u540e\u8005\u662f\u52a0\u4e0a\u8be5\u7a7a\u95f4\u5927\u5c0f\u3002 \u6700\u540e free(realptr) \uff0c\u6e05\u9664\u7a7a\u95f4","title":"zfree()\u6e90\u7801"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#update_zmalloc_free","text":"#define update_zmalloc_stat_free(__n) do { \\ size_t _n = (__n); \\ if (_n&(sizeof(long)-1)) _n += sizeof(long)-(_n&(sizeof(long)-1)); \\ if (zmalloc_thread_safe) { \\ update_zmalloc_stat_sub(_n); \\ } else { \\ used_memory -= _n; \\ } \\ } while(0) \u5176\u4e2d\u7684\u51fd\u6570 update_zmalloc_sub \u4e0e zmalloc() \u4e2d\u7684 update_zmalloc_add \u76f8\u5bf9\u5e94\uff0c\u4f46\u529f\u80fd\u76f8\u53cd\uff0c\u63d0\u4f9b\u7ebf\u7a0b\u5b89\u5168\u5730 used_memory \u51cf\u6cd5\u64cd\u4f5c\u3002 #define update_zmalloc_stat_sub(__n) do { \\ pthread_mutex_lock(&used_memory_mutex); \\ used_memory -= (__n); \\ pthread_mutex_unlock(&used_memory_mutex); \\ } while(0)","title":"update_zmalloc_free\u6e90\u7801"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zcalloc","text":"zcalloc() \u7684\u5b9e\u73b0\u57fa\u4e8e calloc() \uff0c\u4f46\u662f\u4e24\u8005\u7f16\u7a0b\u63a5\u53e3\u4e0d\u540c\u3002\u770b\u4e00\u4e0b\u5bf9\u6bd4\uff1a void * calloc ( size_t nmemb , size_t size ); void * zcalloc ( size_t size ); calloc() \u7684\u529f\u80fd\u662f\u4e5f\u662f\u5206\u914d\u5185\u5b58\u7a7a\u95f4\uff0c\u4e0e malloc() \u7684\u4e0d\u540c\u4e4b\u5904\u6709\u4e24\u70b9\uff1a \u5b83\u5206\u914d\u7684\u7a7a\u95f4\u5927\u5c0f\u662f size * nmemb \u3002\u6bd4\u5982 calloc(10,sizoef(char)); // \u5206\u914d10\u4e2a\u5b57\u8282 calloc() \u4f1a\u5bf9\u5206\u914d\u7684\u7a7a\u95f4\u505a\u521d\u59cb\u5316\u5de5\u4f5c\uff08\u521d\u59cb\u5316\u4e3a0\uff09\uff0c\u800c malloc() \u4e0d\u4f1a \u8f85\u52a9\u51fd\u6570 calloc() update_zmalloc_stat_alloc()\u3010\u5b8f\u51fd\u6570\u3011 update_zmalloc_stat_add()\u3010\u5b8f\u51fd\u6570\u3011","title":"zcalloc"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zcalloc_1","text":"void * zcalloc ( size_t size ) { void * ptr = calloc ( 1 , size + PREFIX_SIZE ); if ( ! ptr ) zmalloc_oom_handler ( size ); #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_alloc ( zmalloc_size ( ptr )); return ptr ; #else * (( size_t * ) ptr ) = size ; update_zmalloc_stat_alloc ( size + PREFIX_SIZE ); return ( char * ) ptr + PREFIX_SIZE ; #endif } zcalloc() \u4e2d\u6ca1\u6709 calloc() \u7684\u7b2c\u4e00\u4e2a\u51fd\u6570 nmemb \u3002\u56e0\u4e3a\u5b83\u6bcf\u6b21\u8c03\u7528 calloc() ,\u5176\u7b2c\u4e00\u4e2a\u53c2\u6570\u90fd\u662f1\u3002\u4e5f\u5c31\u662f\u8bf4 zcalloc() \u529f\u80fd\u662f\u6bcf\u6b21\u5206\u914d size+PREFIX_SIZE \u7684\u7a7a\u95f4\uff0c\u5e76\u521d\u59cb\u5316\u3002 \u5176\u4f59\u4ee3\u7801\u7684\u5206\u6790\u548c zmalloc() \u76f8\u540c\uff0c\u4e5f\u5c31\u662f\u8bf4\uff1a zcalloc() \u548c zmalloc() \u5177\u6709\u76f8\u540c\u7684\u7f16\u7a0b\u63a5\u53e3\uff0c\u5b9e\u73b0\u529f\u80fd\u57fa\u672c\u76f8\u540c\uff0c\u552f\u4e00\u4e0d\u540c\u4e4b\u5904\u662f zcalloc() \u4f1a\u505a\u521d\u59cb\u5316\u5de5\u4f5c\uff0c\u800c zmalloc() \u4e0d\u4f1a\u3002","title":"zcalloc()\u6e90\u7801"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zrealloc","text":"zrealloc() \u548c realloc() \u5177\u6709\u76f8\u540c\u7684\u7f16\u7a0b\u63a5\u53e3\uff1a void * realloc ( void * ptr , size_t size ); void * zrealloc ( void * ptr , size_t size ); realloc() \u8981\u5b8c\u6210\u7684\u529f\u80fd\u662f\u7ed9\u9996\u5730\u5740 ptr \u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u91cd\u65b0\u5206\u914d\u5927\u5c0f\u3002\u5982\u679c\u5931\u8d25\u4e86\uff0c\u5219\u5728\u5176\u5b83\u4f4d\u7f6e\u65b0\u5efa\u4e00\u5757\u5927\u5c0f\u4e3a size \u5b57\u8282\u7684\u7a7a\u95f4\uff0c\u5c06\u539f\u5148\u7684\u6570\u636e\u590d\u5236\u5230\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u5e76\u8fd4\u56de\u8fd9\u6bb5\u5185\u5b58\u9996\u5730\u5740\u3010\u539f\u5185\u5b58\u4f1a\u88ab\u7cfb\u7edf\u81ea\u7136\u91ca\u653e\u3011\u3002 zrealloc() \u8981\u5b8c\u6210\u7684\u529f\u80fd\u4e5f\u7c7b\u4f3c\u3002 \u8f85\u52a9\u51fd\u6570\uff1a zmalloc() zmalloc_size() realloc() zmalloc_oom_handler \u3010\u51fd\u6570\u6307\u9488\u3011 update_zmalloc_stat_free() \u3010\u5b8f\u51fd\u6570\u3011 update_zmalloc_stat_alloc() \u3010\u5b8f\u51fd\u6570\u3011","title":"zrealloc"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zrealloc_1","text":"void * zrealloc ( void * ptr , size_t size ) { #ifndef HAVE_MALLOC_SIZE void * realptr ; #endif size_t oldsize ; void * newptr ; if ( ptr == NULL ) return zmalloc ( size ); #ifdef HAVE_MALLOC_SIZE oldsize = zmalloc_size ( ptr ); newptr = realloc ( ptr , size ); if ( ! newptr ) zmalloc_oom_handler ( size ); update_zmalloc_stat_free ( oldsize ); update_zmalloc_stat_alloc ( zmalloc_size ( newptr )); return newptr ; #else realptr = ( char * ) ptr - PREFIX_SIZE ; oldsize = * (( size_t * ) realptr ); newptr = realloc ( realptr , size + PREFIX_SIZE ); if ( ! newptr ) zmalloc_oom_handler ( size ); * (( size_t * ) newptr ) = size ; update_zmalloc_stat_free ( oldsize ); update_zmalloc_stat_alloc ( size ); return ( char * ) newptr + PREFIX_SIZE ; #endif } \u7ecf\u8fc7\u524d\u9762\u5173\u4e8e zmalloc() \u548c zfree() \u7684\u6e90\u7801\u89e3\u8bfb\uff0c\u76f8\u4fe1\u60a8\u4e00\u5b9a\u80fd\u591f\u5f88\u8f7b\u677e\u5730\u8bfb\u61c2 zrealloc() \u7684\u6e90\u7801\uff0c\u8fd9\u91cc\u6211\u5c31\u4e0d\u8d58\u8ff0\u4e86\u3002","title":"zrealloc()\u6e90\u7801"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zstrdup","text":"\u4ece\u8fd9\u4e2a\u51fd\u6570\u540d\u4e2d\uff0c\u5f88\u5bb9\u6613\u53d1\u73b0\u5b83\u662fstring duplicate\u7684\u7f29\u5199\uff0c\u5373\u5b57\u7b26\u4e32\u590d\u5236\u3002\u5b83\u7684\u4ee3\u7801\u6bd4\u8f83\u7b80\u5355\u3002\u5148\u770b\u4e00\u4e0b\u58f0\u660e\uff1a char * zstrdup ( const char * s ); \u529f\u80fd\u63cf\u8ff0\uff1a\u590d\u5236\u5b57\u7b26\u4e32 s \u7684\u5185\u5bb9\uff0c\u5230\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u6784\u9020\u65b0\u7684\u5b57\u7b26\u4e32\u3010\u5806\u533a\u3011\u3002\u5e76\u5c06\u8fd9\u6bb5\u65b0\u7684\u5b57\u7b26\u4e32\u5730\u5740\u8fd4\u56de\u3002","title":"zstrdup"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#zstrdup_1","text":"char * zstrdup ( const char * s ) { size_t l = strlen ( s ) + 1 ; char * p = zmalloc ( l ); memcpy ( p , s , l ); return p ; } \u9996\u5148\uff0c\u5148\u83b7\u5f97\u5b57\u7b26\u4e32s\u7684\u957f\u5ea6\uff0c\u65b0\u95fbstrlen()\u51fd\u6570\u662f\u4e0d\u7edf\u8ba1'\\0'\u7684\uff0c\u6240\u4ee5\u6700\u540e\u8981\u52a01\u3002 \u7136\u540e\u8c03\u7528zmalloc()\u6765\u5206\u914d\u8db3\u591f\u7684\u7a7a\u95f4\uff0c\u9996\u5730\u5740\u4e3ap\u3002 \u8c03\u7528memcpy\u6765\u5b8c\u6210\u590d\u5236\u3002 \u7136\u540e\u8fd4\u56dep\u3002 \u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b memcpy","title":"zstrdup\u6e90\u7801"},{"location":"Basic/Memory-management/zmalloc/Redis%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E5%9F%BA%E7%9F%B3zmallc%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/#memcpy","text":"\u8fd9\u662f\u6807\u51c6C\u3010ANSI C\u3011\u4e2d\u7528\u4e8e\u5185\u5b58\u590d\u5236\u7684\u51fd\u6570\uff0c\u5728\u5934\u6587\u4ef6 <string.h> \u4e2d\uff08gcc\uff09\u3002\u58f0\u660e\u5982\u4e0b\uff1a void * memcpy ( void * dest , const void * src , size_t n ); dest\u5373\u76ee\u7684\u5730\u5740\uff0csrc\u662f\u6e90\u5730\u5740\u3002n\u662f\u8981\u590d\u5236\u7684\u5b57\u8282\u6570\u3002","title":"memcpy"},{"location":"Basic/Network/","text":"","title":"Introduction"},{"location":"Basic/Persistence/","text":"","title":"Introduction"},{"location":"Basic/Redis-db/","text":"Redis DB \u4e00\u3001DB\u63d0\u4f9b\u4e86\u4e00\u5b9a\u7684\u9694\u79bb \u5728cnblogs \u3010\u539f\u521b\u3011\u90a3\u4e9b\u5e74\u7528\u8fc7\u7684Redis\u96c6\u7fa4\u67b6\u6784\uff08\u542b\u9762\u8bd5\u89e3\u6790\uff09 \u4e2d\uff0c\u63d0\u53ca\u4e86Redis db \u95ee\u98982:Redis\u7684\u591a\u6570\u636e\u5e93\u673a\u5236\uff0c\u4e86\u89e3\u591a\u5c11\uff1f \u6b63\u5e38\u7248 \uff1aRedis\u652f\u6301\u591a\u4e2a\u6570\u636e\u5e93\uff0c\u5e76\u4e14\u6bcf\u4e2a\u6570\u636e\u5e93\u7684\u6570\u636e\u662f\u9694\u79bb\u7684\u4e0d\u80fd\u5171\u4eab\uff0c\u5355\u673a\u4e0b\u7684redis\u53ef\u4ee5\u652f\u630116\u4e2a\u6570\u636e\u5e93\uff08db0 ~ db15\uff09 \u9ad8\u8c03\u7248 : \u5728Redis Cluster\u96c6\u7fa4\u67b6\u6784\u4e0b\u53ea\u6709\u4e00\u4e2a\u6570\u636e\u5e93\u7a7a\u95f4\uff0c\u5373db0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u6ca1\u6709\u4f7f\u7528Redis\u7684\u591a\u6570\u636e\u5e93\u529f\u80fd\uff01","title":"Introduction"},{"location":"Basic/Redis-db/#redis#db","text":"\u4e00\u3001DB\u63d0\u4f9b\u4e86\u4e00\u5b9a\u7684\u9694\u79bb \u5728cnblogs \u3010\u539f\u521b\u3011\u90a3\u4e9b\u5e74\u7528\u8fc7\u7684Redis\u96c6\u7fa4\u67b6\u6784\uff08\u542b\u9762\u8bd5\u89e3\u6790\uff09 \u4e2d\uff0c\u63d0\u53ca\u4e86Redis db \u95ee\u98982:Redis\u7684\u591a\u6570\u636e\u5e93\u673a\u5236\uff0c\u4e86\u89e3\u591a\u5c11\uff1f \u6b63\u5e38\u7248 \uff1aRedis\u652f\u6301\u591a\u4e2a\u6570\u636e\u5e93\uff0c\u5e76\u4e14\u6bcf\u4e2a\u6570\u636e\u5e93\u7684\u6570\u636e\u662f\u9694\u79bb\u7684\u4e0d\u80fd\u5171\u4eab\uff0c\u5355\u673a\u4e0b\u7684redis\u53ef\u4ee5\u652f\u630116\u4e2a\u6570\u636e\u5e93\uff08db0 ~ db15\uff09 \u9ad8\u8c03\u7248 : \u5728Redis Cluster\u96c6\u7fa4\u67b6\u6784\u4e0b\u53ea\u6709\u4e00\u4e2a\u6570\u636e\u5e93\u7a7a\u95f4\uff0c\u5373db0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u6ca1\u6709\u4f7f\u7528Redis\u7684\u591a\u6570\u636e\u5e93\u529f\u80fd\uff01","title":"Redis DB"},{"location":"Basic/Redis-server/","text":"Redis server","title":"Introduction"},{"location":"Basic/Redis-server/#redis#server","text":"","title":"Redis server"},{"location":"Basic/Redis-server/Redis-Signals-Handling/","text":"Redis Signals Handling \u4e0b\u9762\u662f\u76f8\u5173code\u7684\u5206\u6790: initServer \u8be5\u51fd\u6570\u4e3b\u8981\u505a\u5982\u4e0b\u4e8b\u60c5 \u5b89\u88c5signal handler signal ( SIGHUP , SIG_IGN ); signal ( SIGPIPE , SIG_IGN ); setupSignalHandlers (); system call\u5bf9 EINTR \u7684\u5904\u7406 \u5728redis source code\u4e2d\u68c0\u7d22 EINTR \uff0c\u5f97\u5230\u7684\u7ed3\u679c\u5982\u4e0b\uff1a","title":"Introduction"},{"location":"Basic/Redis-server/Redis-Signals-Handling/#redis#signals#handling","text":"\u4e0b\u9762\u662f\u76f8\u5173code\u7684\u5206\u6790:","title":"Redis Signals Handling"},{"location":"Basic/Redis-server/Redis-Signals-Handling/#initserver","text":"\u8be5\u51fd\u6570\u4e3b\u8981\u505a\u5982\u4e0b\u4e8b\u60c5","title":"initServer"},{"location":"Basic/Redis-server/Redis-Signals-Handling/#signal#handler","text":"signal ( SIGHUP , SIG_IGN ); signal ( SIGPIPE , SIG_IGN ); setupSignalHandlers ();","title":"\u5b89\u88c5signal handler"},{"location":"Basic/Redis-server/Redis-Signals-Handling/#system#calleintr","text":"\u5728redis source code\u4e2d\u68c0\u7d22 EINTR \uff0c\u5f97\u5230\u7684\u7ed3\u679c\u5982\u4e0b\uff1a","title":"system call\u5bf9EINTR\u7684\u5904\u7406"},{"location":"Basic/Redis-server/Redis-Signals-Handling/redis-doc-Redis-Signals-Handling/","text":"redis.io Redis Signals Handling This document provides information about how Redis reacts to the reception of different POSIX signals such as SIGTERM , SIGSEGV and so forth. The information contained in this document is only applicable to Redis version 2.6 or greater . Handling of SIGTERM The SIGTERM signals tells Redis to shutdown gracefully. When this signal is received the server does not actually exits as a result, but it schedules a shutdown very similar to the one performed when the SHUTDOWN command is called. The scheduled shutdown starts ASAP, specifically as long as the current command in execution terminates (if any), with a possible additional delay of 0.1 seconds or less. NOTE: \u5904\u7406 SIGTERM \u7684\u8fc7\u7a0b\u548c\u5904\u7406 SHUTDOWN \u547d\u4ee4\u7684\u8fc7\u7a0b\u7c7b\u4f3c\uff0c\u670d\u52a1\u5668\u91c7\u7528\u7684\u63aa\u65bd\u662f\uff1a schedules a shutdown very similar to the one performed when the SHUTDOWN command is called In case the server is blocked by a Lua script that is taking too much time, if the script is killable with SCRIPT KILL the scheduled shutdown will be executed just after the script is killed, or if terminates spontaneously. NOTE: or if\u8868\u793a\u7684\u662f\u548c\u524d\u9762\u7684if\u6240\u4e3e\u7684\u76f8\u53cd\u7684\u60c5\u51b5 NOTE: \u5982\u679c\u5f53\u524dserver\u963b\u585e\u5728\u4e00\u4e2a\u9700\u8981\u5f88\u591a\u65f6\u95f4\u624d\u80fd\u591f\u5b8c\u6210\u7684Lua script\u4e2d\uff0c\u6b64\u65f6\u670d\u52a1\u5668\u91c7\u53d6\u7684\u63aa\u65bd\u662f\uff1a 1\u3001\u5982\u679c\u8fd9\u4e2aLua script\u662fkillable \uff0c\u5219\u670d\u52a1\u5668\u4f1a\u8c03\u7528 SCRIPT KILL \uff0c\u7136\u540e\u5728\u6267\u884cthe scheduled shutdown 2\u3001\u5982\u679cLua script\u4e0d\u662fkillable \uff0c\u5219\u81ea\u52a8\u7ec8\u6b62\uff08\u6b64\u5904\u7684\u81ea\u52a8\u7ec8\u6b62\u7684\u542b\u4e49\u662fserver\u4e0d\u4f1a\u8c03\u7528 SCRIPT KILL \uff0c\u800c\u662f\u7b49\u6b63\u5728\u6267\u884c\u7684Lua scripte\u6267\u884c\u5b8c\u6210\u540e\u624d\u6267\u884cthe scheduled shutdown The Shutdown performed in this condition includes the following actions: 1\u3001If there is a background child saving the RDB file or performing an AOF rewrite, the child is killed. 2\u3001If the AOF is active, Redis calls the fsync system call on the AOF file descriptor in order to flush the buffers on disk. 3\u3001If Redis is configured to persist on disk using RDB files , a synchronous (blocking) save is performed. Since the save is performed in a synchronous way no additional memory is used. 4\u3001If the server is daemonized, the pid file is removed. 5\u3001If the Unix domain socket is enabled, it gets removed. 6\u3001The server exits with an exit code of zero. In case the RDB file can't be saved, the shutdown fails, and the server continues to run in order to ensure no data loss. Since Redis 2.6.11 no further attempt to shut down will be made unless a new SIGTERM will be received or the SHUTDOWN command issued. Handling of SIGSEGV, SIGBUS, SIGFPE and SIGILL NOTE: \u5b83\u7684\u5904\u7406\u7b56\u7565\u662f\u503c\u5f97\u501f\u9274\u7684 The following follow signals are handled as a Redis crash: 1\u3001 SIGSEGV 2\u3001 SIGBUS 3\u3001 SIGFPE 4\u3001 SIGILL Once one of these signals is trapped\uff08\u88ab\u6355\u6349\uff09, Redis aborts any current operation and performs the following actions: 1\u3001A bug report is produced on the log file. This includes a stack trace , dump of registers , and information about the state of clients. NOTE: \u975e\u5e38\u503c\u5f97\u501f\u9274 2\u3001Since Redis 2.8 a fast memory test is performed as a first check of the reliability of the crashing system. 3\u3001If the server was daemonized, the pid file is removed. 4\u3001Finally the server unregisters its own signal handler for the received signal, and sends the same signal again to itself, in order to make sure that the default action is performed, for instance dumping the core on the file system. What happens when a child process gets killed When the child performing the Append Only File rewrite gets killed by a signal , Redis handles this as an error and discards the (probably partial or corrupted) AOF file. The rewrite will be re-triggered again later. When the child performing an RDB save is killed Redis will handle the condition as a more severe error, because while the effect of a lack of AOF file rewrite is a the AOF file enlargement, the effect of failed RDB file creation is lack of durability. NOTE: \u5f53\u6267\u884cRDB\u4fdd\u5b58\u7684\u5b50\u8282\u70b9\u88ab\u6740\u6b7b\u65f6\uff0cRedis\u5c06\u5904\u7406\u8be5\u6761\u4ef6\u4f5c\u4e3a\u66f4\u4e25\u91cd\u7684\u9519\u8bef\uff0c\u56e0\u4e3a\u867d\u7136\u7f3a\u5c11AOF\u6587\u4ef6\u91cd\u5199\u7684\u6548\u679c\u662fAOF\u6587\u4ef6\u653e\u5927\uff0c\u4f46\u662f\u5931\u8d25\u7684RDB\u6587\u4ef6\u521b\u5efa\u7684\u6548\u679c\u7f3a\u4e4f\u6301\u4e45\u6027\u3002 As a result of the child producing the RDB file being killed by a signal , or when the child exits with an error (non zero exit code), Redis enters a special error condition where no further write command is accepted. Redis will continue to reply to read commands. Redis will reply to all write commands with a MISCONFIG error. This error condition is cleared only once it will be possible to create an RDB file with success. Killing the RDB file without triggering an error condition However sometimes the user may want to kill the RDB saving child without generating an error. Since Redis version 2.6.10 this can be done using the special signal SIGUSR1 that is handled in a special way: it kills the child process as any other signal, but the parent process will not detect this as a critical error and will continue to serve write requests as usually.","title":"Introduction"},{"location":"Basic/Redis-server/Redis-Signals-Handling/redis-doc-Redis-Signals-Handling/#redisio#redis#signals#handling","text":"This document provides information about how Redis reacts to the reception of different POSIX signals such as SIGTERM , SIGSEGV and so forth. The information contained in this document is only applicable to Redis version 2.6 or greater .","title":"redis.io Redis Signals Handling"},{"location":"Basic/Redis-server/Redis-Signals-Handling/redis-doc-Redis-Signals-Handling/#handling#of#sigterm","text":"The SIGTERM signals tells Redis to shutdown gracefully. When this signal is received the server does not actually exits as a result, but it schedules a shutdown very similar to the one performed when the SHUTDOWN command is called. The scheduled shutdown starts ASAP, specifically as long as the current command in execution terminates (if any), with a possible additional delay of 0.1 seconds or less. NOTE: \u5904\u7406 SIGTERM \u7684\u8fc7\u7a0b\u548c\u5904\u7406 SHUTDOWN \u547d\u4ee4\u7684\u8fc7\u7a0b\u7c7b\u4f3c\uff0c\u670d\u52a1\u5668\u91c7\u7528\u7684\u63aa\u65bd\u662f\uff1a schedules a shutdown very similar to the one performed when the SHUTDOWN command is called In case the server is blocked by a Lua script that is taking too much time, if the script is killable with SCRIPT KILL the scheduled shutdown will be executed just after the script is killed, or if terminates spontaneously. NOTE: or if\u8868\u793a\u7684\u662f\u548c\u524d\u9762\u7684if\u6240\u4e3e\u7684\u76f8\u53cd\u7684\u60c5\u51b5 NOTE: \u5982\u679c\u5f53\u524dserver\u963b\u585e\u5728\u4e00\u4e2a\u9700\u8981\u5f88\u591a\u65f6\u95f4\u624d\u80fd\u591f\u5b8c\u6210\u7684Lua script\u4e2d\uff0c\u6b64\u65f6\u670d\u52a1\u5668\u91c7\u53d6\u7684\u63aa\u65bd\u662f\uff1a 1\u3001\u5982\u679c\u8fd9\u4e2aLua script\u662fkillable \uff0c\u5219\u670d\u52a1\u5668\u4f1a\u8c03\u7528 SCRIPT KILL \uff0c\u7136\u540e\u5728\u6267\u884cthe scheduled shutdown 2\u3001\u5982\u679cLua script\u4e0d\u662fkillable \uff0c\u5219\u81ea\u52a8\u7ec8\u6b62\uff08\u6b64\u5904\u7684\u81ea\u52a8\u7ec8\u6b62\u7684\u542b\u4e49\u662fserver\u4e0d\u4f1a\u8c03\u7528 SCRIPT KILL \uff0c\u800c\u662f\u7b49\u6b63\u5728\u6267\u884c\u7684Lua scripte\u6267\u884c\u5b8c\u6210\u540e\u624d\u6267\u884cthe scheduled shutdown The Shutdown performed in this condition includes the following actions: 1\u3001If there is a background child saving the RDB file or performing an AOF rewrite, the child is killed. 2\u3001If the AOF is active, Redis calls the fsync system call on the AOF file descriptor in order to flush the buffers on disk. 3\u3001If Redis is configured to persist on disk using RDB files , a synchronous (blocking) save is performed. Since the save is performed in a synchronous way no additional memory is used. 4\u3001If the server is daemonized, the pid file is removed. 5\u3001If the Unix domain socket is enabled, it gets removed. 6\u3001The server exits with an exit code of zero. In case the RDB file can't be saved, the shutdown fails, and the server continues to run in order to ensure no data loss. Since Redis 2.6.11 no further attempt to shut down will be made unless a new SIGTERM will be received or the SHUTDOWN command issued.","title":"Handling of SIGTERM"},{"location":"Basic/Redis-server/Redis-Signals-Handling/redis-doc-Redis-Signals-Handling/#handling#of#sigsegv#sigbus#sigfpe#and#sigill","text":"NOTE: \u5b83\u7684\u5904\u7406\u7b56\u7565\u662f\u503c\u5f97\u501f\u9274\u7684 The following follow signals are handled as a Redis crash: 1\u3001 SIGSEGV 2\u3001 SIGBUS 3\u3001 SIGFPE 4\u3001 SIGILL Once one of these signals is trapped\uff08\u88ab\u6355\u6349\uff09, Redis aborts any current operation and performs the following actions: 1\u3001A bug report is produced on the log file. This includes a stack trace , dump of registers , and information about the state of clients. NOTE: \u975e\u5e38\u503c\u5f97\u501f\u9274 2\u3001Since Redis 2.8 a fast memory test is performed as a first check of the reliability of the crashing system. 3\u3001If the server was daemonized, the pid file is removed. 4\u3001Finally the server unregisters its own signal handler for the received signal, and sends the same signal again to itself, in order to make sure that the default action is performed, for instance dumping the core on the file system.","title":"Handling of SIGSEGV, SIGBUS, SIGFPE and SIGILL"},{"location":"Basic/Redis-server/Redis-Signals-Handling/redis-doc-Redis-Signals-Handling/#what#happens#when#a#child#process#gets#killed","text":"When the child performing the Append Only File rewrite gets killed by a signal , Redis handles this as an error and discards the (probably partial or corrupted) AOF file. The rewrite will be re-triggered again later. When the child performing an RDB save is killed Redis will handle the condition as a more severe error, because while the effect of a lack of AOF file rewrite is a the AOF file enlargement, the effect of failed RDB file creation is lack of durability. NOTE: \u5f53\u6267\u884cRDB\u4fdd\u5b58\u7684\u5b50\u8282\u70b9\u88ab\u6740\u6b7b\u65f6\uff0cRedis\u5c06\u5904\u7406\u8be5\u6761\u4ef6\u4f5c\u4e3a\u66f4\u4e25\u91cd\u7684\u9519\u8bef\uff0c\u56e0\u4e3a\u867d\u7136\u7f3a\u5c11AOF\u6587\u4ef6\u91cd\u5199\u7684\u6548\u679c\u662fAOF\u6587\u4ef6\u653e\u5927\uff0c\u4f46\u662f\u5931\u8d25\u7684RDB\u6587\u4ef6\u521b\u5efa\u7684\u6548\u679c\u7f3a\u4e4f\u6301\u4e45\u6027\u3002 As a result of the child producing the RDB file being killed by a signal , or when the child exits with an error (non zero exit code), Redis enters a special error condition where no further write command is accepted. Redis will continue to reply to read commands. Redis will reply to all write commands with a MISCONFIG error. This error condition is cleared only once it will be possible to create an RDB file with success.","title":"What happens when a child process gets killed"},{"location":"Basic/Redis-server/Redis-Signals-Handling/redis-doc-Redis-Signals-Handling/#killing#the#rdb#file#without#triggering#an#error#condition","text":"However sometimes the user may want to kill the RDB saving child without generating an error. Since Redis version 2.6.10 this can be done using the special signal SIGUSR1 that is handled in a special way: it kills the child process as any other signal, but the parent process will not detect this as a critical error and will continue to serve write requests as usually.","title":"Killing the RDB file without triggering an error condition"},{"location":"Basic/Thread-model/","text":"Redis thread model \u53c2\u8003\u6587\u7ae0 antirez An update about Redis developments in 2019 \u4e00\u3001\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86Redis\u53ef\u80fd\u7684multiple thread: 1\u3001\u201c I/O threading \u201d 2\u3001\u201c Slow commands threading \u201d \u4e8c\u3001\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f5c\u8005\u4ee5Memcached\u6765\u5bf9\u6bd4\uff0c\u8ba8\u8bbaRedis\u5b9e\u73b0multiple thread\u65f6\uff0c\u9700\u8981\u8003\u8651\u7684\u5404\u79cd\u5185\u5bb9\u3001\u5404\u79cdtradeoff Redis 6 multiple thread implementation alibabacloud Improving Redis Performance through Multi-Thread Processing zhihu Redis 6.0 \u591a\u7ebf\u7a0bIO\u5904\u7406\u8fc7\u7a0b\u8be6\u89e3 \u901a\u8fc7\u4e0a\u8ff0\u4e24\u7bc7\u6587\u7ae0\u7684\u5185\u5bb9\uff0c\u57fa\u672c\u4e0a\u53ef\u4ee5\u638c\u63e1Redis 6 multiple thread implementation\u3002 TODO https://blog.cloudflare.com/io_submit-the-epoll-alternative-youve-never-heard-about/ toutiao Redis\u4e3a\u4ec0\u4e48\u53c8\u5f15\u5165\u4e86\u591a\u7ebf\u7a0b\uff1f\u96be\u9053\u4f5c\u8005\u4e5f\u9003\u4e0d\u8fc7\u201c\u771f\u9999\u5b9a\u7406\u201d\uff1f \u4e09\u3001\u4e3a\u4ec0\u4e48\u5f15\u5165\u591a\u7ebf\u7a0b\uff1f \u521a\u521a\u8bf4\u4e86\u4e00\u5806\u4f7f\u7528\u5355\u7ebf\u7a0b\u7684\u597d\u5904\uff0c\u73b0\u5728\u8bdd\u950b\u4e00\u8f6c\uff0c\u53c8\u8981\u8bf4\u4e3a\u4ec0\u4e48\u8981\u5f15\u5165\u591a\u7ebf\u7a0b\uff0c\u522b\u4e0d\u9002\u5e94\u3002\u5f15\u5165\u591a\u7ebf\u7a0b\u8bf4\u660eRedis\u5728\u6709\u4e9b\u65b9\u9762\uff0c\u5355\u7ebf\u7a0b\u5df2\u7ecf\u4e0d\u5177\u6709\u4f18\u52bf\u4e86\u3002 \u56e0\u4e3a\u8bfb\u5199\u7f51\u7edc\u7684read/write\u7cfb\u7edf\u8c03\u7528\u5728Redis\u6267\u884c\u671f\u95f4\u5360\u7528\u4e86\u5927\u90e8\u5206CPU\u65f6\u95f4\uff0c\u5982\u679c\u628a\u7f51\u7edc\u8bfb\u5199\u505a\u6210\u591a\u7ebf\u7a0b\u7684\u65b9\u5f0f\u5bf9\u6027\u80fd\u4f1a\u6709\u5f88\u5927\u63d0\u5347\u3002 Redis \u7684\u591a\u7ebf\u7a0b\u90e8\u5206\u53ea\u662f\u7528\u6765\u5904\u7406\u7f51\u7edc\u6570\u636e\u7684\u8bfb\u5199\u548c\u534f\u8bae\u89e3\u6790\uff0c\u6267\u884c\u547d\u4ee4\u4ecd\u7136\u662f\u5355\u7ebf\u7a0b\u3002\u4e4b\u6240\u4ee5\u8fd9\u4e48\u8bbe\u8ba1\u662f\u4e0d\u60f3 Redis \u56e0\u4e3a\u591a\u7ebf\u7a0b\u800c\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u53bb\u63a7\u5236 key\u3001lua\u3001\u4e8b\u52a1\uff0cLPUSH/LPOP \u7b49\u7b49\u7684\u5e76\u53d1\u95ee\u9898\u3002 zhihu Redis6 \u591a\u7ebf\u7a0b\u5256\u6790","title":"Introduction"},{"location":"Basic/Thread-model/#redis#thread#model","text":"","title":"Redis thread model"},{"location":"Basic/Thread-model/#_1","text":"antirez An update about Redis developments in 2019 \u4e00\u3001\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86Redis\u53ef\u80fd\u7684multiple thread: 1\u3001\u201c I/O threading \u201d 2\u3001\u201c Slow commands threading \u201d \u4e8c\u3001\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4f5c\u8005\u4ee5Memcached\u6765\u5bf9\u6bd4\uff0c\u8ba8\u8bbaRedis\u5b9e\u73b0multiple thread\u65f6\uff0c\u9700\u8981\u8003\u8651\u7684\u5404\u79cd\u5185\u5bb9\u3001\u5404\u79cdtradeoff","title":"\u53c2\u8003\u6587\u7ae0"},{"location":"Basic/Thread-model/#redis#6#multiple#thread#implementation","text":"alibabacloud Improving Redis Performance through Multi-Thread Processing zhihu Redis 6.0 \u591a\u7ebf\u7a0bIO\u5904\u7406\u8fc7\u7a0b\u8be6\u89e3 \u901a\u8fc7\u4e0a\u8ff0\u4e24\u7bc7\u6587\u7ae0\u7684\u5185\u5bb9\uff0c\u57fa\u672c\u4e0a\u53ef\u4ee5\u638c\u63e1Redis 6 multiple thread implementation\u3002","title":"Redis 6 multiple thread implementation"},{"location":"Basic/Thread-model/#todo","text":"https://blog.cloudflare.com/io_submit-the-epoll-alternative-youve-never-heard-about/","title":"TODO"},{"location":"Basic/Thread-model/#toutiao#redis","text":"","title":"toutiao Redis\u4e3a\u4ec0\u4e48\u53c8\u5f15\u5165\u4e86\u591a\u7ebf\u7a0b\uff1f\u96be\u9053\u4f5c\u8005\u4e5f\u9003\u4e0d\u8fc7\u201c\u771f\u9999\u5b9a\u7406\u201d\uff1f"},{"location":"Basic/Thread-model/#_2","text":"\u521a\u521a\u8bf4\u4e86\u4e00\u5806\u4f7f\u7528\u5355\u7ebf\u7a0b\u7684\u597d\u5904\uff0c\u73b0\u5728\u8bdd\u950b\u4e00\u8f6c\uff0c\u53c8\u8981\u8bf4\u4e3a\u4ec0\u4e48\u8981\u5f15\u5165\u591a\u7ebf\u7a0b\uff0c\u522b\u4e0d\u9002\u5e94\u3002\u5f15\u5165\u591a\u7ebf\u7a0b\u8bf4\u660eRedis\u5728\u6709\u4e9b\u65b9\u9762\uff0c\u5355\u7ebf\u7a0b\u5df2\u7ecf\u4e0d\u5177\u6709\u4f18\u52bf\u4e86\u3002 \u56e0\u4e3a\u8bfb\u5199\u7f51\u7edc\u7684read/write\u7cfb\u7edf\u8c03\u7528\u5728Redis\u6267\u884c\u671f\u95f4\u5360\u7528\u4e86\u5927\u90e8\u5206CPU\u65f6\u95f4\uff0c\u5982\u679c\u628a\u7f51\u7edc\u8bfb\u5199\u505a\u6210\u591a\u7ebf\u7a0b\u7684\u65b9\u5f0f\u5bf9\u6027\u80fd\u4f1a\u6709\u5f88\u5927\u63d0\u5347\u3002 Redis \u7684\u591a\u7ebf\u7a0b\u90e8\u5206\u53ea\u662f\u7528\u6765\u5904\u7406\u7f51\u7edc\u6570\u636e\u7684\u8bfb\u5199\u548c\u534f\u8bae\u89e3\u6790\uff0c\u6267\u884c\u547d\u4ee4\u4ecd\u7136\u662f\u5355\u7ebf\u7a0b\u3002\u4e4b\u6240\u4ee5\u8fd9\u4e48\u8bbe\u8ba1\u662f\u4e0d\u60f3 Redis \u56e0\u4e3a\u591a\u7ebf\u7a0b\u800c\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u53bb\u63a7\u5236 key\u3001lua\u3001\u4e8b\u52a1\uff0cLPUSH/LPOP \u7b49\u7b49\u7684\u5e76\u53d1\u95ee\u9898\u3002","title":"\u4e09\u3001\u4e3a\u4ec0\u4e48\u5f15\u5165\u591a\u7ebf\u7a0b\uff1f"},{"location":"Basic/Thread-model/#zhihu#redis6","text":"","title":"zhihu Redis6 \u591a\u7ebf\u7a0b\u5256\u6790"},{"location":"Basic/Thread-model/Background-IO-thread/","text":"Background I/O service for Redis. This file implements operations that we need to perform in the background. Currently there is only a single operation, that is a background close(2) system call. This is needed as when the process is the last owner of a reference to a file closing it means unlinking it, and the deletion of the file is slow, blocking the server. In the future we'll either continue implementing new things we need or we'll switch to libeio . However there are probably long term uses for this file as we may want to put here Redis specific background tasks (for instance it is not impossible that we'll need a non blocking FLUSHDB / FLUSHALL implementation). DESIGN The design is trivial, we have a structure representing a job to perform and a different thread and job queue for every job type. Every thread waits for new jobs in its queue, and process every job sequentially. Jobs of the same type are guaranteed to be processed from the least recently inserted to the most recently inserted (older jobs processed first). Currently there is no way for the creator of the job to be notified about the completion of the operation , this will only be added when/if needed.","title":"Introduction"},{"location":"Basic/Thread-model/Background-IO-thread/#background#io#service#for#redis","text":"This file implements operations that we need to perform in the background. Currently there is only a single operation, that is a background close(2) system call. This is needed as when the process is the last owner of a reference to a file closing it means unlinking it, and the deletion of the file is slow, blocking the server. In the future we'll either continue implementing new things we need or we'll switch to libeio . However there are probably long term uses for this file as we may want to put here Redis specific background tasks (for instance it is not impossible that we'll need a non blocking FLUSHDB / FLUSHALL implementation). DESIGN The design is trivial, we have a structure representing a job to perform and a different thread and job queue for every job type. Every thread waits for new jobs in its queue, and process every job sequentially. Jobs of the same type are guaranteed to be processed from the least recently inserted to the most recently inserted (older jobs processed first). Currently there is no way for the creator of the job to be notified about the completion of the operation , this will only be added when/if needed.","title":"Background I/O service for Redis."},{"location":"Basic/Thread-model/Read-code/","text":"\u6ce8\u610f\uff0cIO thread\u662fredis 6\u7684\u65b0\u7279\u6027 server.c:initServerConfig(){ server.io_threads_num = CONFIG_DEFAULT_IO_THREADS_NUM; server.io_threads_do_reads = CONFIG_DEFAULT_IO_THREADS_DO_READS; } networking.c: /* ========================================================================== * Threaded I/O * ========================================================================== */ \u4ece\u4ee3\u7801\u6765\u770b\uff0credis\u7684IO\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8fd8\u662fsingle thread\u7684\uff1b","title":"Introduction"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/","text":"alibabacloud Improving Redis Performance through Multi-Thread Processing Redis is generally known as a single-process, single-thread model. This is not true. Redis also runs multiple backend threads to perform backend cleaning works, such as cleansing the dirty data and closing file descriptors. NOTE: \"multiple backend threads\"\u6307\u7684\u662fbackground IO thread\uff0c\u5373 bio \uff0c\u53c2\u89c1 Background-IO-thread \u7ae0\u8282 In Redis, the main thread is responsible for the major tasks, including but not limited to: 1\u3001receiving the connections from clients, 2\u3001processing the connection read/write events, 3\u3001parsing requests, processing commands, 4\u3001processing timer events, and 5\u3001synchronizing data. Only one CPU core runs a single process and single thread. For small packets, a Redis server can process 80,000 to 100,000 QPS. A larger QPS is beyond the processing capacity of a Redis server. A common solution is to partition the data and adopt multiple servers in distributed architecture. However, this solution also has many drawbacks. For example, too many Redis servers to manage; some commands that are applicable to a single Redis server do not work on the data partitions; data partitions cannot solve the hot spot read/write problem; and data skew(\u6570\u636e\u503e\u659c), redistribution, and scale-up/down become more complex. Due to restrictions of the single process and single thread, we hope that the multi-thread can be reconstructed to fully utilize the advantages of the SMP multi-core architecture , thus increasing the throughput of a single Redis server. To make Redis multi-threaded, the simplest way to think of is that every thread performs both I/O and command processing. However, as the data structure processed by Redis is complex, the multi-thread needs to use the locks to ensure the thread security. Improper handling of the lock granularity may deteriorate(\u7834\u574f\u3001\u6076\u5316) the performance. We suggest that the number of I/O threads should be increased to enable an independent I/O thread to read/write data in the connections, parse commands, and reply data packets, and still let a single thread process the commands and execute the timer events. In this way, the throughput of a single Redis server can be increased. NOTE: \u8fd9\u662f\u76ee\u524dRedis\u6240\u91c7\u7528\u7684 Single Process and Single Thread Model Advantages 1\u3001Due to restrictions of the single-process and single-thread model, time-consuming operations (such as dict rehash and expired key deletion) are broken into multiple steps and executed one by one in Redis implementation. This prevents execution of an operation for a long time and therefore avoids long time blocking of the system by an operation. The single-process and single-thread code is easy to compile, which reduces the context switching and lock seizure(\u4e89\u593a) caused by multi-process and multi-thread. Disadvantages 1\u3001Only one CPU core can be used, and the multi-core advantages cannot be utilized. 2\u3001For heavy I/O applications, a large amount of CPU capacity is consumed by the network I/O operations. Applications that use Redis as cache are often heavy I/O applications. These applications basically have a high QPS, use relatively simple commands (such as get , set , and incr ), but are RT sensitive. They often have a high bandwidth usage, which may even reach hundreds of megabits. Thanks to popularization of the 10-GB and 25-GB network adapters, the network bandwidth is no longer a bottleneck. Therefore, what we need to think about is how to utilize the advantages of multi-core and performance of the network adapter. NOTE: \"RT sensitive\" \u4e2d \"RT\" \u5e94\u8be5\u662f realtime\u7684\u610f\u601d Multi-Thread Model and Implementation Thread Model There are three thread types, namely: Main thread I/O thread Worker thread 1\u3001Main thread: Receives connections, creates clients, and forwards connections to the I/O thread. 2\u3001I/O thread: Processes the connection read/write events, parses commands, forwards the complete parsed commands to the worker thread for processing, sends the response packets, and deletes connections. 3\u3001Worker thread: Processes commands, generates the client response packets, and executes the timer events. 4\u3001The main thread, I/O thread, and worker thread are driven by events separately. 5\u3001Threads exchange data through the lock-free queue and send notifications through tunnels. Benefits of Multi Thread Model Increased Read/Write Performance The stress test result indicates that the read/write performance can be improved by about three folds in the small packet scenario. Increased Master/Slave Synchronization Speed When the master sends the synchronization data to the slave, data is sent in the I/O thread. When reading data from the master, the slave reads the full data from the worker thread, and the incremental data from the I/O thread. This can efficiently increase the synchronization speed. Subsequent Tasks The first task is to increase the number of I/O threads and optimize the I/O read/write capability. Next, we can break down the worker thread so that each thread completes I/O reading, as well as the work of the worker thread. Setting of the Number of I/O threads 1\u3001Test results indicate that the number of I/O threads should not exceed 6. Otherwise, the worker thread will become a bottleneck for simple operations. 2\u3001Upon startup of a process, the number of I/O threads must be set. When the process is running, the number of I/O threads cannot be modified. Based on the current connection allocation policy, modification of the number of I/O threads involves re-allocation of connections, which is quite complex. Considerations 1\u3001With popularization of the 10-GB and 25-GB network adapters, how to fully utilize the hardware performance must be carefully considered. We can use technologies, such as multiple threads for networkI/O and the kernel bypass user-mode protocol stack. 2\u3001The I/O thread can be used to implement blocking-free data migration. The I/O thread encodes the data process or forwards commands, whereas the target node decodes data or executes commands. To learn more about Alibaba Cloud ApsaraDB for Redis, visit www.alibabacloud.com/product/apsaradb-for-redis","title":"Introduction"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#alibabacloud#improving#redis#performance#through#multi-thread#processing","text":"Redis is generally known as a single-process, single-thread model. This is not true. Redis also runs multiple backend threads to perform backend cleaning works, such as cleansing the dirty data and closing file descriptors. NOTE: \"multiple backend threads\"\u6307\u7684\u662fbackground IO thread\uff0c\u5373 bio \uff0c\u53c2\u89c1 Background-IO-thread \u7ae0\u8282 In Redis, the main thread is responsible for the major tasks, including but not limited to: 1\u3001receiving the connections from clients, 2\u3001processing the connection read/write events, 3\u3001parsing requests, processing commands, 4\u3001processing timer events, and 5\u3001synchronizing data. Only one CPU core runs a single process and single thread. For small packets, a Redis server can process 80,000 to 100,000 QPS. A larger QPS is beyond the processing capacity of a Redis server. A common solution is to partition the data and adopt multiple servers in distributed architecture. However, this solution also has many drawbacks. For example, too many Redis servers to manage; some commands that are applicable to a single Redis server do not work on the data partitions; data partitions cannot solve the hot spot read/write problem; and data skew(\u6570\u636e\u503e\u659c), redistribution, and scale-up/down become more complex. Due to restrictions of the single process and single thread, we hope that the multi-thread can be reconstructed to fully utilize the advantages of the SMP multi-core architecture , thus increasing the throughput of a single Redis server. To make Redis multi-threaded, the simplest way to think of is that every thread performs both I/O and command processing. However, as the data structure processed by Redis is complex, the multi-thread needs to use the locks to ensure the thread security. Improper handling of the lock granularity may deteriorate(\u7834\u574f\u3001\u6076\u5316) the performance. We suggest that the number of I/O threads should be increased to enable an independent I/O thread to read/write data in the connections, parse commands, and reply data packets, and still let a single thread process the commands and execute the timer events. In this way, the throughput of a single Redis server can be increased. NOTE: \u8fd9\u662f\u76ee\u524dRedis\u6240\u91c7\u7528\u7684","title":"alibabacloud Improving Redis Performance through Multi-Thread Processing"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#single#process#and#single#thread#model","text":"","title":"Single Process and Single Thread Model"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#advantages","text":"1\u3001Due to restrictions of the single-process and single-thread model, time-consuming operations (such as dict rehash and expired key deletion) are broken into multiple steps and executed one by one in Redis implementation. This prevents execution of an operation for a long time and therefore avoids long time blocking of the system by an operation. The single-process and single-thread code is easy to compile, which reduces the context switching and lock seizure(\u4e89\u593a) caused by multi-process and multi-thread.","title":"Advantages"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#disadvantages","text":"1\u3001Only one CPU core can be used, and the multi-core advantages cannot be utilized. 2\u3001For heavy I/O applications, a large amount of CPU capacity is consumed by the network I/O operations. Applications that use Redis as cache are often heavy I/O applications. These applications basically have a high QPS, use relatively simple commands (such as get , set , and incr ), but are RT sensitive. They often have a high bandwidth usage, which may even reach hundreds of megabits. Thanks to popularization of the 10-GB and 25-GB network adapters, the network bandwidth is no longer a bottleneck. Therefore, what we need to think about is how to utilize the advantages of multi-core and performance of the network adapter. NOTE: \"RT sensitive\" \u4e2d \"RT\" \u5e94\u8be5\u662f realtime\u7684\u610f\u601d","title":"Disadvantages"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#multi-thread#model#and#implementation","text":"","title":"Multi-Thread Model and Implementation"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#thread#model","text":"There are three thread types, namely: Main thread I/O thread Worker thread 1\u3001Main thread: Receives connections, creates clients, and forwards connections to the I/O thread. 2\u3001I/O thread: Processes the connection read/write events, parses commands, forwards the complete parsed commands to the worker thread for processing, sends the response packets, and deletes connections. 3\u3001Worker thread: Processes commands, generates the client response packets, and executes the timer events. 4\u3001The main thread, I/O thread, and worker thread are driven by events separately. 5\u3001Threads exchange data through the lock-free queue and send notifications through tunnels.","title":"Thread Model"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#benefits#of#multi#thread#model","text":"","title":"Benefits of Multi Thread Model"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#increased#readwrite#performance","text":"The stress test result indicates that the read/write performance can be improved by about three folds in the small packet scenario.","title":"Increased Read/Write Performance"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#increased#masterslave#synchronization#speed","text":"When the master sends the synchronization data to the slave, data is sent in the I/O thread. When reading data from the master, the slave reads the full data from the worker thread, and the incremental data from the I/O thread. This can efficiently increase the synchronization speed.","title":"Increased Master/Slave Synchronization Speed"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#subsequent#tasks","text":"The first task is to increase the number of I/O threads and optimize the I/O read/write capability. Next, we can break down the worker thread so that each thread completes I/O reading, as well as the work of the worker thread.","title":"Subsequent Tasks"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#setting#of#the#number#of#io#threads","text":"1\u3001Test results indicate that the number of I/O threads should not exceed 6. Otherwise, the worker thread will become a bottleneck for simple operations. 2\u3001Upon startup of a process, the number of I/O threads must be set. When the process is running, the number of I/O threads cannot be modified. Based on the current connection allocation policy, modification of the number of I/O threads involves re-allocation of connections, which is quite complex.","title":"Setting of the Number of I/O threads"},{"location":"Basic/Thread-model/alibabacloud-Improving-Redis-Performance-through-Multi-Thread/#considerations","text":"1\u3001With popularization of the 10-GB and 25-GB network adapters, how to fully utilize the hardware performance must be carefully considered. We can use technologies, such as multiple threads for networkI/O and the kernel bypass user-mode protocol stack. 2\u3001The I/O thread can be used to implement blocking-free data migration. The I/O thread encodes the data process or forwards commands, whereas the target node decodes data or executes commands. To learn more about Alibaba Cloud ApsaraDB for Redis, visit www.alibabacloud.com/product/apsaradb-for-redis","title":"Considerations"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/","text":"zhihu Redis 6.0 \u591a\u7ebf\u7a0bIO\u5904\u7406\u8fc7\u7a0b\u8be6\u89e3 \u5f02\u6b65\u5904\u7406IO Redis \u6838\u5fc3\u7684\u5de5\u4f5c\u8d1f\u8377\u662f\u4e00\u4e2a\u5355\u7ebf\u7a0b\u5728\u5904\u7406\uff0c \u4f46\u4e3a\u4ec0\u4e48\u8fd8\u90a3\u4e48\u5feb\uff1f 1\u3001\u5176\u4e00\u662f\u7eaf\u5185\u5b58\u64cd\u4f5c\u3002 2\u3001\u5176\u4e8c\u5c31\u662fIO\u6570\u636e\u7684\u5904\u7406\u662f\u5f02\u6b65\u7684\uff0c\u6bcf\u4e2a\u547d\u4ee4\u4ece\u63a5\u6536\u5230\u5904\u7406\uff0c\u518d\u5230\u8fd4\u56de\uff0c\u4f1a\u7ecf\u5386\u591a\u4e2a\u201c\u4e0d\u8fde\u7eed\u201d\u7684\u5de5\u5e8f\u3002 \uff08 \u4e3a\u907f\u514d\u6b67\u4e49\uff0c\u6b64\u5904\u7684**\u5f02\u6b65\u5904\u7406IO**\u4e0d\u662f\u201c\u540c\u6b65/\u5f02\u6b65IO\u201d\uff0c\u7279\u6307IO**\u5904\u7406\u8fc7\u7a0b**\u662f\u5f02\u6b65\u7684\uff0c\u63cf\u8ff0\u7684\u5bf9\u8c61\u662f\u5904\u7406\u8fc7\u7a0b\u3002 \uff09 NOTE: \u539f\u6587\u540e\u9762\u5bf9Redis\u7684**\u5f02\u6b65\u5904\u7406IO**\u8fdb\u884c\u8bf4\u660e Redis\u5904\u7406\u547d\u4ee4\u8fc7\u7a0b \u5047\u8bbe\u5ba2\u6237\u7aef\u53d1\u9001\u4e86\u4ee5\u4e0b\u547d\u4ee4\uff1a GET key-how-to-be-a-better-man\uff1f redis\u56de\u590d: \u52aa\u529b\u52a0\u628a\u52b2\u628a\u6587\u7ae0\u5199\u5b8c \u8981\u5904\u7406\u547d\u4ee4\uff0c\u5219redis\u5fc5\u987b\u5b8c\u6574\u5730\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\uff0c\u5e76\u5c06\u547d\u4ee4\u89e3\u6790\u51fa\u6765\uff0c\u518d\u5c06\u7ed3\u679c\u8bfb\u51fa\u6765\uff0c\u901a\u8fc7\u7f51\u7edc\u56de\u5199\u5230\u5ba2\u6237\u7aef\u3002\u6574\u4e2a\u5de5\u5e8f\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u90e8\u5206\uff1a 1\u3001\u63a5\u6536\u3002\u901a\u8fc7TCP\u63a5\u6536\u5230\u547d\u4ee4\uff0c\u53ef\u80fd\u4f1a\u5386\u7ecf\u591a\u6b21TCP\u5305\u3001ack\u3001IO\u64cd\u4f5c NOTE: read\u3001input 2\u3001\u89e3\u6790\u3002\u5c06\u547d\u4ee4\u53d6\u51fa\u6765 3\u3001\u6267\u884c\u3002\u5230\u5bf9\u5e94\u7684\u5730\u65b9\u5c06value\u8bfb\u51fa\u6765 4\u3001\u8fd4\u56de\u3002\u5c06value\u901a\u8fc7TCP\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\uff0c\u5982\u679cvalue\u8f83\u5927\uff0c\u5219IO\u8d1f\u8377\u4f1a\u66f4\u91cd NOTE: write\u3001output \u5176\u4e2d\u89e3\u6790\u548c\u6267\u884c\u662f\u7eafcpu/\u5185\u5b58\u64cd\u4f5c\uff0c\u800c\u63a5\u6536\u548c\u8fd4\u56de\u4e3b\u8981\u662fIO\u64cd\u4f5c\uff0c\u8fd9\u662f\u6211\u4eec\u8981\u5173\u6ce8\u7684\u91cd\u70b9\u3002\u4ee5\u63a5\u6536\u4e3a\u4f8b\uff0credis\u8981\u5b8c\u6574\u63a5\u6536\u5ba2\u6237\u7aef\u547d\u4ee4\uff0c\u6709\u4e24\u79cd\u7b56\u7565\uff1a 1\u3001 \u540c\u6b65 : \u63a5\u6536\u5ba2\u6237\u7aef\u547d\u4ee4\u65f6\u4e00\u76f4\u7b49\uff0c\u76f4\u5230\u63a5\u6536\u5230\u5b8c\u6574\u7684\u547d\u4ee4\uff0c\u7136\u540e\u6267\u884c\uff0c\u518d\u5c06\u7ed3\u679c\u8fd4\u56de\uff0c\u76f4\u5230\u5ba2\u6237\u7aef\u6536\u5230\u5b8c\u6574\u7ed3\u679c\uff0c \u7136\u540e\u624d\u5904\u7406\u4e0b\u4e00\u4e2a\u547d\u4ee4\u3002 \u8fd9\u53eb**\u540c\u6b65**\u3002 \u540c\u6b65\u7684\u8fc7\u7a0b\u4e2d\u6709\u5f88\u591a\u7b49\u5f85\u7684\u65f6\u95f4\uff0c\u4f8b\u5982\u6709\u4e2a\u5ba2\u6237\u7aef\u7f51\u7edc\u4e0d\u597d\uff0c\u90a3\u7b49\u5b83\u5b8c\u6574\u7684\u547d\u4ee4\u5c31\u4f1a\u66f4\u8017\u65f6\u3002 2\u3001 \u5f02\u6b65 : \u5ba2\u6237\u7aef\u7684TCP\u5305\u6765\u4e00\u4e2a\u624d\u5904\u7406\u4e00\u4e2a\uff0c\u5c06\u6570\u636e\u8ffd\u52a0\u5230**\u7f13\u51b2\u533a**\uff0c\u5904\u7406\u5b8c\u4e86\u5c31\u53bb\u7acb\u5373\u627e\u5176\u4ed6\u4e8b\u505a\uff0c\u4e0d\u7b49\u5f85\uff0c\u4e0b\u4e00\u4e2aTCP\u5305\u6765\u4e86\u518d\u7ee7\u7eed\u5904\u7406\u3002\u547d\u4ee4\u7684\u63a5\u6536\u8fc7\u7a0b\u662f\u7a7f\u63d2\u7684\uff0c\u4e0d\u8fde\u7eed\u3002\u4e00\u4f1a\u513f\u63a5\u6536\u8fd9\u4e2a\u547d\u4ee4\uff0c\u4e00\u4f1a\u513f\u53c8\u5728\u63a5\u6536\u53e6\u4e00\u4e2a\u3002 \u8fd9\u53eb\u505a*\u5f02\u6b65*\uff0c\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u989d\u5916\u7684\u7a7a\u95f2\u7b49\u5f85\u65f6\u95f4\u3002 NOTE: \u5176\u5b9e\u4f5c\u8005\u6240\u8ff0\u7684\"\u5f02\u6b65\u5904\u7406IO\"\uff0c\u672c\u8d28\u4e0a\u662f: \"buffered-queued IO\"\uff0c\u5373\u65e0\u8bba\u662finput\u8fd8\u662foutput\uff0c\u90fd\u4f1a\u8fdb\u5165\u5230buffer\u4e2d\uff0c\u5f85ready\u540e\uff0c\u518d\u6267\u884c\uff0c\u663e\u7136\uff0c\u8fd9\u6837\u5c31\u4e0d\u9700\u8981\u7b49\u5f85 \u7528\u804a\u5929\u7684\u4f8b\u5b50\u505a\u5bf9\u5e94\uff0c\u5047\u8bbe\u4f60\u5728\u56de\u7b54\u591a\u4e2a\u4eba\u7684\u95ee\u9898\uff0c\u4e5f\u6709\u540c\u6b65\u548c\u5f02\u6b65\u7684\u7b56\u7565\uff1a 1\u3001 \u540c\u6b65 : \u804a\u5929\u6846\u4e2d\u663e\u793a\u201c\u6b63\u5728\u8f93\u5165\u201d\u65f6\uff0c\u4f60\u4e00\u76f4\u7b49ta\u8f93\u5165\u5b8c\u6bd5\uff0c\u7136\u540e\u56de\u7b54ta\u7684\u95ee\u9898\uff0c\u518d\u53d1\u9001\u51fa\u53bb\uff0c\u53d1\u9001\u65f6\u4f1a\u6709\u7b49\u5f85\uff0c\u5e38\u89c4\u8868\u73b0\u5c31\u662f\u6709\u4e2a\u5706\u5708\u5728\u8f6c\u3002\u4f60\u7b49\u53d1\u9001\u5b8c\u6bd5\u540e\uff0c\u624d\u53bb\u56de\u7b54\u53e6\u4e00\u4e2a\u4eba\u7684\u95ee\u9898\u3002 2\u3001 \u5f02\u6b65 : \u663e\u793a\u201c\u6b63\u5728\u8f93\u5165\u201d\u65f6\uff0c\u4e0d\u7b49ta\uff0c\u800c\u662f\u53bb\u56de\u7b54\u5176\u4ed6\u8f93\u5165\u5b8c\u6bd5\u7684\u95ee\u9898\uff0c\u56de\u7b54\u5b8c\u540e\uff0c\u4e0d\u7b49\u53d1\u9001\u5b8c\u6bd5\uff0c\u53c8\u53bb\u56de\u7b54\u5176\u5b83\u95ee\u9898\u3002 \u5f88\u663e\u7136\u5f02\u6b65\u7684\u6548\u7387\u66f4\u9ad8\uff0c\u8981\u5b9e\u73b0\u9ad8\u5e76\u53d1\u5fc5\u987b\u8981\u5f02\u6b65\uff0c\u56e0\u4e3a\u540c\u6b65\u6709\u592a\u591a\u65f6\u95f4\u6d6a\u8d39\u5728\u7b49\u5f85\u4e0a\u4e86\uff0c\u9047\u5230\u7f51\u7edc\u4e0d\u597d\u7684\u5ba2\u6237\u7aef\u76f4\u63a5\u5c31\u88ab\u62d6\u57ae\u3002\u5f02\u6b65\u7684\u7b56\u7565\u7b80\u5355\u53ef\u603b\u7ed3\u5982\u4e0b\uff1a 1\u3001\u7f51\u7edc\u5305\u6709\u6570\u636e\u4e86\uff0c\u5c31\u53bb\u8bfb\u4e00\u4e0b\u653e\u5230\u7f13\u51b2\u533a\uff0c\u8bfb\u5b8c\u7acb\u9a6c\u5207\u5230\u5176\u4ed6\u4e8b\u60c5\u4e0a\uff0c\u4e0d\u7b49\u4e0b\u4e00\u4e2a\u5305 2\u3001\u89e3\u6790\u4e0b\u7f13\u51b2\u533a\u6570\u636e\u662f\u5426\u5b8c\u6574\u3002\u5982\u5b8c\u6574\u5219\u6267\u884c\u547d\u4ee4\uff0c\u4e0d\u5b8c\u6574\u5207\u5230\u5176\u4ed6\u4e8b\u60c5\u4e0a 3\u3001\u6570\u636e\u5b8c\u6574\u4e86\uff0c\u7acb\u5373\u6267\u884c\u547d\u4ee4\uff0c\u5c06\u6267\u884c\u7ed3\u679c\u653e\u5230\u7f13\u51b2\u533a 4\u3001\u5c06\u6570\u636e\u7ed9\u5ba2\u6237\u7aef\uff0c\u5982\u679c\u4e00\u6b21\u7ed9\u4e0d\u5b8c\uff0c\u5c31\u7b49 \u4e0b\u6b21\u80fd\u7ed9\u65f6 \u518d\u7ed9\uff0c\u4e0d\u7b49\uff0c\u76f4\u5230\u5168\u90e8\u7ed9\u5b8c \u4e8b\u4ef6\u9a71\u52a8 NOTE: 1\u3001Redis event library ae 2\u3001Linux epoll \u5355\u7ebf\u7a0bIO\u5904\u7406\u8fc7\u7a0b redis\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u4e00\u4e2a\u6b7b\u5faa\u73af aeMain \uff0c\u5728\u8fd9\u4e2a\u5faa\u73af\u91cc\u4e00\u76f4\u7b49\u5f85\u4e8b\u4ef6\u53d1\u751f\uff0c\u4e8b\u4ef6\u5206\u4e3aIO\u4e8b\u4ef6\u548ctimer\u4e8b\u4ef6\uff0ctimer\u4e8b\u4ef6\u662f\u4e00\u4e9b\u5b9a\u65f6\u6267\u884c\u7684\u4efb\u52a1\uff0c\u5982expire key\u7b49\uff0c\u672c\u6587\u53ea\u804aIO\u4e8b\u4ef6\u3002 NOTE: event loop epoll\u5904\u7406\u7684\u662fsocket\u7684\u53ef\u8bfb\u3001\u53ef\u5199\u4e8b\u4ef6\uff0c\u5f53\u4e8b\u4ef6\u53d1\u751f\u540e\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u7684\u901a\u77e5\u65b9\u5f0f\uff0c \u5f53\u60f3\u8981\u5f02\u6b65\u76d1\u542c\u67d0\u4e2asocket\u7684\u8bfb\u5199\u4e8b\u4ef6\u65f6\uff0c\u9700\u8981\u53bb\u4e8b\u4ef6\u9a71\u52a8\u6846\u67b6\u4e2d\u6ce8\u518c\u8981\u76d1\u542c\u4e8b\u4ef6\u7684socket\uff0c\u4ee5\u53ca\u5bf9\u5e94\u4e8b\u4ef6\u7684\u56de\u8c03function\u3002\u7136\u540e\u6b7b\u5faa\u73af\u4e2d\u53ef\u4ee5\u901a\u8fc7epoll_wait\u4e0d\u65ad\u5730\u53bb\u62ff\u53d1\u751f\u4e86\u53ef\u8bfb\u5199\u4e8b\u4ef6\u7684socket\uff0c\u4f9d\u6b21\u5904\u7406\u5373\u53ef\u3002 1\u3001 \u53ef\u8bfb \u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\uff0c\u5bf9\u5e94\u7684socket\u4e2d\u6709\u65b0\u7684tcp\u6570\u636e\u5305\u5230\u6765\u3002 2\u3001 \u53ef\u5199 \u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\uff0c\u5bf9\u5e94\u7684socket\u5199\u7f13\u51b2\u533a\u5df2\u7ecf\u7a7a\u4e86(\u6570\u636e\u901a\u8fc7\u7f51\u7edc\u5df2\u7ecf\u53d1\u7ed9\u4e86\u5ba2\u6237\u7aef) \u6d41\u7a0b\u56fe \u4e00\u56fe\u80dc\u524d\u8a00\uff0c\u5b8c\u6574\u3001\u8be6\u7ec6\u6d41\u7a0b\u56fe\u5982\u4e0b\uff1a NOTE: \u53f3\u4e0b\u89d2\u662f\u5b8c\u6574\u7684\u5199\u8fc7\u7a0b 1\u3001 aeMain() \u5185\u90e8\u662f\u4e00\u4e2a\u6b7b\u5faa\u73af\uff0c\u4f1a\u5728 epoll_wait \u5904\u77ed\u6682\u4f11\u7720 2\u3001 epoll_wait \u8fd4\u56de\u7684\u662f\u5f53\u524d\u53ef\u8bfb\u3001\u53ef\u5199\u7684socket\u5217\u8868 3\u3001 beforeSleep \u662f\u8fdb\u5165\u4f11\u7720\u524d\u6267\u884c\u7684\u903b\u8f91\uff0c\u6838\u5fc3\u662f\u56de\u5199\u6570\u636e\u5230socket 4\u3001\u6838\u5fc3\u903b\u8f91\u90fd\u662f\u7531IO\u4e8b\u4ef6\u89e6\u53d1\uff0c\u8981\u4e48\u53ef\u8bfb\uff0c\u8981\u4e48\u53ef\u5199\uff0c\u5426\u5219\u6267\u884ctimer\u5b9a\u65f6\u4efb\u52a1 5\u3001\u7b2c\u4e00\u6b21\u7684IO\u53ef\u8bfb\u4e8b\u4ef6\uff0c\u662f\u76d1\u542csocket(\u5982\u76d1\u542c6379\u7684socket)\uff0c\u5f53\u6709\u63e1\u624b\u8bf7\u6c42\u65f6\uff0c\u4f1a\u6267\u884c accept \u8c03\u7528\uff0c\u5f97\u5230\u4e00\u4e2a\u8fde\u63a5socket\uff0c\u6ce8\u518c\u53ef\u8bfb\u56de\u8c03 createClient \uff0c\u5f80\u540e\u5ba2\u6237\u7aef\u548credis\u7684\u6570\u636e\u90fd\u901a\u8fc7\u8fd9\u4e2asocket\u8fdb\u884c 6\u3001\u4e00\u4e2a\u5b8c\u6574\u7684\u547d\u4ee4\uff0c\u53ef\u80fd\u4f1a\u901a\u8fc7\u591a\u6b21 4 \u624d\u80fd\u4ecesocket\u8bfb\u5b8c\uff0c\u8fd9\u610f\u5473\u8fd9\u591a\u6b21\u53ef\u8bfbIO\u4e8b\u4ef6 7\u3001\u547d\u4ee4\u6267\u884c\u7684\u7ed3\u679c\u4f1a\u5199\uff0c\u4e5f\u662f\u8fd9\u6837\uff0c\u5927\u6982\u7387\u4f1a\u901a\u8fc7\u591a\u6b21\u53ef\u5199\u56de\u8c03\u624d\u80fd\u5199\u5b8c 8\u3001\u5f53\u547d\u4ee4\u88ab\u6267\u884c\u5b8c\u540e\uff0c\u5bf9\u5e94\u7684\u8fde\u63a5\u4f1a\u88ab\u8ffd\u52a0\u5230 clients_pending_write \uff0c beforeSleep \u4f1a\u5c1d\u8bd5\u56de\u5199\u5230 socket \uff0c\u5199\u4e0d\u5b8c\u4f1a\u6ce8\u518c\u53ef\u5199\u4e8b\u4ef6\uff0c\u4e0b\u6b21\u7ee7\u7eed\u5199 9\u3001\u6574\u4e2a\u8fc7\u7a0bIO\u5168\u90e8\u90fd\u662f\u540c\u6b65\u975e\u963b\u585e\uff0c\u6ca1\u6709\u6d6a\u8d39\u7b49\u5f85\u65f6\u95f4 10\u3001\u6ce8\u518c\u4e8b\u4ef6\u7684\u51fd\u6570\u53eb aeCreateFileEvent \u5355\u7ebf\u7a0bIO\u7684\u74f6\u9888 NOTE: Redis\u91c7\u7528\u7684\u662fReactor\u3001\u540c\u6b65\u975e\u963b\u585eIO\uff1bread\u3001write\u662f\u540c\u6b65\u7684\uff0c\u8fd9\u90e8\u5206\u5de5\u4f5c\u662f\u4f1a\u6d88\u8017\u5927\u91cf\u7684CPU\u65f6\u95f4\u7684\uff1b\u8fd9\u662fRedis\u5f15\u5165\u591a\u7ebf\u7a0bIO\u7684\u539f\u56e0 \u540c\u65f6\u8fd9\u4e2a\u6a21\u578b\u6709\u51e0\u4e2a\u7f3a\u9677\uff1a 1\u3001\u53ea\u80fd\u7528\u4e00\u4e2acpu\u6838(\u5ffd\u7565\u540e\u53f0\u7ebf\u7a0b) 2\u3001\u5982\u679cvalue\u6bd4\u8f83\u5927\uff0credis\u7684QPS\u4f1a\u4e0b\u964d\u5f97\u5f88\u5389\u5bb3\uff0c\u6709\u65f6\u4e00\u4e2a\u5927key\u5c31\u53ef\u4ee5\u62d6\u57ae 3\u3001QPS\u96be\u4ee5\u66f4\u4e0a\u4e00\u5c42\u697c redis\u4e3b\u7ebf\u7a0b\u7684\u65f6\u95f4\u6d88\u8017\u4e3b\u8981\u5728\u4e24\u4e2a\u65b9\u9762\uff1a 1\u3001\u903b\u8f91\u8ba1\u7b97\u7684\u6d88\u8017 2\u3001\u540c\u6b65IO\u8bfb\u5199\uff0c\u62f7\u8d1d\u6570\u636e\u5bfc\u81f4\u7684\u6d88\u8017 \u5f53value\u6bd4\u8f83\u5927\u65f6\uff0c\u74f6\u9888\u4f1a\u5148\u51fa\u73b0\u5728\u540c\u6b65IO\u4e0a(\u5047\u8bbe\u5e26\u5bbd\u548c\u5185\u5b58\u8db3\u591f)\uff0c\u8fd9\u90e8\u5206\u6d88\u8017\u5728\u4e8e\u4e24\u90e8\u5206\uff1a 1\u3001\u4ecesocket\u4e2d\u8bfb\u53d6\u8bf7\u6c42\u6570\u636e\uff0c\u4f1a\u4ece\u5185\u6838\u6001\u5c06\u6570\u636e\u62f7\u8d1d\u5230\u7528\u6237\u6001 \uff08read\u8c03\u7528\uff09 2\u3001\u5c06\u6570\u636e\u56de\u5199\u5230socket\uff0c\u4f1a\u5c06\u6570\u636e\u4ece\u7528\u6237\u6001\u62f7\u8d1d\u5230\u5185\u6838\u6001 \uff08write\u8c03\u7528\uff09 \u8fd9\u90e8\u5206\u6570\u636e\u8bfb\u5199\u4f1a\u5360\u7528\u5927\u91cf\u7684cpu\u65f6\u95f4\uff0c\u4e5f\u76f4\u63a5\u5bfc\u81f4\u4e86\u74f6\u9888\u3002 \u5982\u679c\u80fd\u6709\u591a\u4e2a\u7ebf\u7a0b\u6765\u5206\u62c5\u8fd9\u90e8\u5206\u6d88\u8017\uff0c\u90a3redis\u7684\u541e\u5410\u91cf\u8fd8\u80fd\u66f4\u4e0a\u4e00\u5c42\u697c\uff0c\u8fd9\u4e5f\u662fredis\u5f15\u5165\u591a\u7ebf\u7a0bIO\u7684\u76ee\u7684\u3002[3] NOTE: \u4e0a\u8ff0\u603b\u7ed3\u4e86Redis\u5f15\u5165\u591a\u7ebf\u7a0b\u7684\u539f\u56e0 \u591a\u7ebf\u7a0bIO \u4e0a\u9762\u5df2\u7ecf\u68b3\u7406\u4e86\u5355\u7ebf\u7a0bIO\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u53ca\u591a\u7ebf\u7a0bIO\u8981\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u63a5\u4e0b\u6765\u5c06\u76ee\u5149\u653e\u5230\uff1a \u5982\u4f55\u7528\u591a\u7ebf\u7a0b\u5206\u62c5IO\u7684\u8d1f\u8377\u3002\u5176\u505a\u6cd5\u7528\u7b80\u5355\u7684\u8bdd\u6765\u8bf4\u5c31\u662f\uff1a 1\u3001\u7528\u4e00\u7ec4\u5355\u72ec\u7684\u7ebf\u7a0b\u4e13\u95e8\u8fdb\u884c read/write socket\u8bfb\u5199\u8c03\u7528 \uff08\u540c\u6b65IO\uff09 NOTE: IO threads 2\u3001**\u8bfb\u56de\u8c03\u51fd\u6570**\u4e2d\u4e0d\u518d\u8bfb\u6570\u636e\uff0c\u800c\u662f\u5c06\u5bf9\u5e94\u7684\u8fde\u63a5\u8ffd\u52a0\u5230\u53ef\u8bfb clients_pending_read \u7684\u94fe\u8868 3\u3001\u4e3b\u7ebf\u7a0b\u5728 beforeSleep \u4e2d\u5c06**IO\u8bfb\u4efb\u52a1**\u5206\u7ed9IO\u7ebf\u7a0b\u7ec4 4\u3001\u4e3b\u7ebf\u7a0b\u81ea\u5df1\u4e5f\u5904\u7406\u4e00\u4e2a**IO\u8bfb\u4efb\u52a1**\uff0c\u5e76**\u81ea\u65cb\u5f0f**\u7b49IO\u7ebf\u7a0b\u7ec4\u5904\u7406\u5b8c\uff0c\u518d\u7ee7\u7eed\u5f80\u4e0b 5\u3001\u4e3b\u7ebf\u7a0b\u5728 beforeSleep \u4e2d\u5c06**IO\u5199\u4efb\u52a1**\u5206\u7ed9IO\u7ebf\u7a0b\u7ec4 6\u3001\u4e3b\u7ebf\u7a0b\u81ea\u5df1\u4e5f\u5904\u7406\u4e00\u4e2a**IO\u5199\u4efb\u52a1**\uff0c\u5e76**\u81ea\u65cb\u5f0f**\u7b49IO\u7ebf\u7a0b\u7ec4\u5904\u7406\u5b8c\uff0c\u518d\u7ee7\u7eed\u5f80\u4e0b 7\u3001IO\u7ebf\u7a0b\u7ec4\u8981\u4e48\u540c\u65f6\u5728\u8bfb\uff0c\u8981\u4e48\u540c\u65f6\u5728\u5199 8\u3001\u547d\u4ee4\u7684\u6267\u884c\u7531\u4e3b\u7ebf\u7a0b\u4e32\u884c\u6267\u884c(\u4fdd\u6301\u5355\u7ebf\u7a0b) 9\u3001IO\u7ebf\u7a0b\u6570\u91cf\u53ef\u914d\u7f6e \u5b8c\u6574\u6d41\u7a0b\u56fe NOTE: \u547d\u4ee4\u7684\u6267\u884c\u653e\u5230\u4e86 beforeSleep \u56de\u8c03\u4e2d\u4e86 beforesleep \u4e2d\uff0c\u5148\u8ba9IO\u7ebf\u7a0b\u8bfb\u6570\u636e\uff0c\u7136\u540e\u518d\u8ba9IO\u7ebf\u7a0b\u5199\u6570\u636e\u3002 \u8bfb\u5199\u65f6\uff0c\u591a\u7ebf\u7a0b\u80fd\u5e76\u53d1\u6267\u884c\uff0c\u5229\u7528\u591a\u6838\u3002 1\u3001\u5c06**\u8bfb\u4efb\u52a1**\u5747\u5300\u5206\u53d1\u5230\u5404\u4e2aIO\u7ebf\u7a0b\u7684\u4efb\u52a1\u94fe\u8868 io_threads_list[i] \uff0c\u5c06 io_threads_pending[i] \u8bbe\u7f6e\u4e3a\u5bf9\u5e94\u7684\u4efb\u52a1\u6570\uff0c\u6b64\u65f6IO\u7ebf\u7a0b\u5c06\u4ece\u6b7b\u5faa\u73af\u4e2d\u88ab\u6fc0\u6d3b\uff0c\u5f00\u59cb\u6267\u884c\u4efb\u52a1\uff0c\u6267\u884c\u5b8c\u6bd5\u540e\uff0c\u4f1a\u5c06 io_threads_pending[i] \u6e05\u96f6\u3002 \u51fd\u6570\u540d\u4e3a\uff1a handleClientsWithPendingReadsUsingThreads 2\u3001\u5c06**\u5199\u4efb\u52a1**\u5747\u5300\u5206\u53d1\u5230\u5404\u4e2aIO\u7ebf\u7a0b\u7684\u4efb\u52a1\u94fe\u8868 io_threads_list[i] \uff0c\u5c06 io_threads_pending[i] \u8bbe\u7f6e\u4e3a\u5bf9\u5e94\u7684\u4efb\u52a1\u6570\uff0c\u6b64\u65f6IO\u7ebf\u7a0b\u5c06\u4ece\u6b7b\u5faa\u73af\u4e2d\u88ab\u6fc0\u6d3b\uff0c\u5f00\u59cb\u6267\u884c\u4efb\u52a1\uff0c\u6267\u884c\u5b8c\u6bd5\u540e\uff0c\u4f1a\u5c06 io_threads_pending[i] \u6e05\u96f6\u3002 \u51fd\u6570\u540d\u4e3a\uff1a handleClientsWithPendingWritesUsingThreads 3\u3001 beforeSleep \u4e2d\u4e3b\u7ebf\u7a0b\u4e5f\u4f1a\u6267\u884c\u5176\u4e2d\u4e00\u4e2a\u4efb\u52a1(\u56fe\u4e2d\u5ffd\u7565\u4e86)\uff0c\u6267\u884c\u5b8c\u540e\u81ea\u65cb\u7b49\u5f85IO\u7ebf\u7a0b\u5904\u7406\u5b8c\u3002 4\u3001\u8bfb\u4efb\u52a1\u8981\u4e48\u5728beforeSleep\u4e2d\u88ab\u6267\u884c\uff0c\u8981\u4e48\u5728IO\u7ebf\u7a0b\u88ab\u6267\u884c\uff0c\u4e0d\u4f1a\u518d\u5728\u8bfb\u56de\u8c03\u4e2d\u6267\u884c 5\u3001\u5199\u4efb\u52a1\u4f1a\u5206\u6563\u5230 beforeSleep\u3001IO\u7ebf\u7a0b\u3001\u5199\u56de\u8c03\u4e2d\u6267\u884c 6\u3001\u4e3b\u7ebf\u7a0b\u548cIO\u7ebf\u7a0b\u4ea4\u4e92\u662f\u65e0\u9501\u7684\uff0c\u901a\u8fc7**\u6807\u5fd7\u4f4d**\u8bbe\u7f6e\u8fdb\u884c\uff0c\u4e0d\u4f1a\u540c\u65f6\u5199\u4efb\u52a1\u94fe\u8868 \u6027\u80fd\u636e\u6d4b\u8bd5\u63d0\u5347\u4e86\u4e00\u500d\u4ee5\u4e0a(4\u4e2aIO\u7ebf\u7a0b)\u3002 [4] \u6b22\u8fce\u60a8\u7684\u63d0\u95ee\u3001\u6307\u6b63\u3001\u5efa\u8bae\u7b49\u3002","title":"Introduction"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#zhihu#redis#60#io","text":"","title":"zhihu Redis 6.0 \u591a\u7ebf\u7a0bIO\u5904\u7406\u8fc7\u7a0b\u8be6\u89e3"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#io","text":"Redis \u6838\u5fc3\u7684\u5de5\u4f5c\u8d1f\u8377\u662f\u4e00\u4e2a\u5355\u7ebf\u7a0b\u5728\u5904\u7406\uff0c \u4f46\u4e3a\u4ec0\u4e48\u8fd8\u90a3\u4e48\u5feb\uff1f 1\u3001\u5176\u4e00\u662f\u7eaf\u5185\u5b58\u64cd\u4f5c\u3002 2\u3001\u5176\u4e8c\u5c31\u662fIO\u6570\u636e\u7684\u5904\u7406\u662f\u5f02\u6b65\u7684\uff0c\u6bcf\u4e2a\u547d\u4ee4\u4ece\u63a5\u6536\u5230\u5904\u7406\uff0c\u518d\u5230\u8fd4\u56de\uff0c\u4f1a\u7ecf\u5386\u591a\u4e2a\u201c\u4e0d\u8fde\u7eed\u201d\u7684\u5de5\u5e8f\u3002 \uff08 \u4e3a\u907f\u514d\u6b67\u4e49\uff0c\u6b64\u5904\u7684**\u5f02\u6b65\u5904\u7406IO**\u4e0d\u662f\u201c\u540c\u6b65/\u5f02\u6b65IO\u201d\uff0c\u7279\u6307IO**\u5904\u7406\u8fc7\u7a0b**\u662f\u5f02\u6b65\u7684\uff0c\u63cf\u8ff0\u7684\u5bf9\u8c61\u662f\u5904\u7406\u8fc7\u7a0b\u3002 \uff09 NOTE: \u539f\u6587\u540e\u9762\u5bf9Redis\u7684**\u5f02\u6b65\u5904\u7406IO**\u8fdb\u884c\u8bf4\u660e","title":"\u5f02\u6b65\u5904\u7406IO"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#redis","text":"\u5047\u8bbe\u5ba2\u6237\u7aef\u53d1\u9001\u4e86\u4ee5\u4e0b\u547d\u4ee4\uff1a GET key-how-to-be-a-better-man\uff1f redis\u56de\u590d: \u52aa\u529b\u52a0\u628a\u52b2\u628a\u6587\u7ae0\u5199\u5b8c \u8981\u5904\u7406\u547d\u4ee4\uff0c\u5219redis\u5fc5\u987b\u5b8c\u6574\u5730\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\uff0c\u5e76\u5c06\u547d\u4ee4\u89e3\u6790\u51fa\u6765\uff0c\u518d\u5c06\u7ed3\u679c\u8bfb\u51fa\u6765\uff0c\u901a\u8fc7\u7f51\u7edc\u56de\u5199\u5230\u5ba2\u6237\u7aef\u3002\u6574\u4e2a\u5de5\u5e8f\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u90e8\u5206\uff1a 1\u3001\u63a5\u6536\u3002\u901a\u8fc7TCP\u63a5\u6536\u5230\u547d\u4ee4\uff0c\u53ef\u80fd\u4f1a\u5386\u7ecf\u591a\u6b21TCP\u5305\u3001ack\u3001IO\u64cd\u4f5c NOTE: read\u3001input 2\u3001\u89e3\u6790\u3002\u5c06\u547d\u4ee4\u53d6\u51fa\u6765 3\u3001\u6267\u884c\u3002\u5230\u5bf9\u5e94\u7684\u5730\u65b9\u5c06value\u8bfb\u51fa\u6765 4\u3001\u8fd4\u56de\u3002\u5c06value\u901a\u8fc7TCP\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\uff0c\u5982\u679cvalue\u8f83\u5927\uff0c\u5219IO\u8d1f\u8377\u4f1a\u66f4\u91cd NOTE: write\u3001output \u5176\u4e2d\u89e3\u6790\u548c\u6267\u884c\u662f\u7eafcpu/\u5185\u5b58\u64cd\u4f5c\uff0c\u800c\u63a5\u6536\u548c\u8fd4\u56de\u4e3b\u8981\u662fIO\u64cd\u4f5c\uff0c\u8fd9\u662f\u6211\u4eec\u8981\u5173\u6ce8\u7684\u91cd\u70b9\u3002\u4ee5\u63a5\u6536\u4e3a\u4f8b\uff0credis\u8981\u5b8c\u6574\u63a5\u6536\u5ba2\u6237\u7aef\u547d\u4ee4\uff0c\u6709\u4e24\u79cd\u7b56\u7565\uff1a 1\u3001 \u540c\u6b65 : \u63a5\u6536\u5ba2\u6237\u7aef\u547d\u4ee4\u65f6\u4e00\u76f4\u7b49\uff0c\u76f4\u5230\u63a5\u6536\u5230\u5b8c\u6574\u7684\u547d\u4ee4\uff0c\u7136\u540e\u6267\u884c\uff0c\u518d\u5c06\u7ed3\u679c\u8fd4\u56de\uff0c\u76f4\u5230\u5ba2\u6237\u7aef\u6536\u5230\u5b8c\u6574\u7ed3\u679c\uff0c \u7136\u540e\u624d\u5904\u7406\u4e0b\u4e00\u4e2a\u547d\u4ee4\u3002 \u8fd9\u53eb**\u540c\u6b65**\u3002 \u540c\u6b65\u7684\u8fc7\u7a0b\u4e2d\u6709\u5f88\u591a\u7b49\u5f85\u7684\u65f6\u95f4\uff0c\u4f8b\u5982\u6709\u4e2a\u5ba2\u6237\u7aef\u7f51\u7edc\u4e0d\u597d\uff0c\u90a3\u7b49\u5b83\u5b8c\u6574\u7684\u547d\u4ee4\u5c31\u4f1a\u66f4\u8017\u65f6\u3002 2\u3001 \u5f02\u6b65 : \u5ba2\u6237\u7aef\u7684TCP\u5305\u6765\u4e00\u4e2a\u624d\u5904\u7406\u4e00\u4e2a\uff0c\u5c06\u6570\u636e\u8ffd\u52a0\u5230**\u7f13\u51b2\u533a**\uff0c\u5904\u7406\u5b8c\u4e86\u5c31\u53bb\u7acb\u5373\u627e\u5176\u4ed6\u4e8b\u505a\uff0c\u4e0d\u7b49\u5f85\uff0c\u4e0b\u4e00\u4e2aTCP\u5305\u6765\u4e86\u518d\u7ee7\u7eed\u5904\u7406\u3002\u547d\u4ee4\u7684\u63a5\u6536\u8fc7\u7a0b\u662f\u7a7f\u63d2\u7684\uff0c\u4e0d\u8fde\u7eed\u3002\u4e00\u4f1a\u513f\u63a5\u6536\u8fd9\u4e2a\u547d\u4ee4\uff0c\u4e00\u4f1a\u513f\u53c8\u5728\u63a5\u6536\u53e6\u4e00\u4e2a\u3002 \u8fd9\u53eb\u505a*\u5f02\u6b65*\uff0c\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u989d\u5916\u7684\u7a7a\u95f2\u7b49\u5f85\u65f6\u95f4\u3002 NOTE: \u5176\u5b9e\u4f5c\u8005\u6240\u8ff0\u7684\"\u5f02\u6b65\u5904\u7406IO\"\uff0c\u672c\u8d28\u4e0a\u662f: \"buffered-queued IO\"\uff0c\u5373\u65e0\u8bba\u662finput\u8fd8\u662foutput\uff0c\u90fd\u4f1a\u8fdb\u5165\u5230buffer\u4e2d\uff0c\u5f85ready\u540e\uff0c\u518d\u6267\u884c\uff0c\u663e\u7136\uff0c\u8fd9\u6837\u5c31\u4e0d\u9700\u8981\u7b49\u5f85 \u7528\u804a\u5929\u7684\u4f8b\u5b50\u505a\u5bf9\u5e94\uff0c\u5047\u8bbe\u4f60\u5728\u56de\u7b54\u591a\u4e2a\u4eba\u7684\u95ee\u9898\uff0c\u4e5f\u6709\u540c\u6b65\u548c\u5f02\u6b65\u7684\u7b56\u7565\uff1a 1\u3001 \u540c\u6b65 : \u804a\u5929\u6846\u4e2d\u663e\u793a\u201c\u6b63\u5728\u8f93\u5165\u201d\u65f6\uff0c\u4f60\u4e00\u76f4\u7b49ta\u8f93\u5165\u5b8c\u6bd5\uff0c\u7136\u540e\u56de\u7b54ta\u7684\u95ee\u9898\uff0c\u518d\u53d1\u9001\u51fa\u53bb\uff0c\u53d1\u9001\u65f6\u4f1a\u6709\u7b49\u5f85\uff0c\u5e38\u89c4\u8868\u73b0\u5c31\u662f\u6709\u4e2a\u5706\u5708\u5728\u8f6c\u3002\u4f60\u7b49\u53d1\u9001\u5b8c\u6bd5\u540e\uff0c\u624d\u53bb\u56de\u7b54\u53e6\u4e00\u4e2a\u4eba\u7684\u95ee\u9898\u3002 2\u3001 \u5f02\u6b65 : \u663e\u793a\u201c\u6b63\u5728\u8f93\u5165\u201d\u65f6\uff0c\u4e0d\u7b49ta\uff0c\u800c\u662f\u53bb\u56de\u7b54\u5176\u4ed6\u8f93\u5165\u5b8c\u6bd5\u7684\u95ee\u9898\uff0c\u56de\u7b54\u5b8c\u540e\uff0c\u4e0d\u7b49\u53d1\u9001\u5b8c\u6bd5\uff0c\u53c8\u53bb\u56de\u7b54\u5176\u5b83\u95ee\u9898\u3002 \u5f88\u663e\u7136\u5f02\u6b65\u7684\u6548\u7387\u66f4\u9ad8\uff0c\u8981\u5b9e\u73b0\u9ad8\u5e76\u53d1\u5fc5\u987b\u8981\u5f02\u6b65\uff0c\u56e0\u4e3a\u540c\u6b65\u6709\u592a\u591a\u65f6\u95f4\u6d6a\u8d39\u5728\u7b49\u5f85\u4e0a\u4e86\uff0c\u9047\u5230\u7f51\u7edc\u4e0d\u597d\u7684\u5ba2\u6237\u7aef\u76f4\u63a5\u5c31\u88ab\u62d6\u57ae\u3002\u5f02\u6b65\u7684\u7b56\u7565\u7b80\u5355\u53ef\u603b\u7ed3\u5982\u4e0b\uff1a 1\u3001\u7f51\u7edc\u5305\u6709\u6570\u636e\u4e86\uff0c\u5c31\u53bb\u8bfb\u4e00\u4e0b\u653e\u5230\u7f13\u51b2\u533a\uff0c\u8bfb\u5b8c\u7acb\u9a6c\u5207\u5230\u5176\u4ed6\u4e8b\u60c5\u4e0a\uff0c\u4e0d\u7b49\u4e0b\u4e00\u4e2a\u5305 2\u3001\u89e3\u6790\u4e0b\u7f13\u51b2\u533a\u6570\u636e\u662f\u5426\u5b8c\u6574\u3002\u5982\u5b8c\u6574\u5219\u6267\u884c\u547d\u4ee4\uff0c\u4e0d\u5b8c\u6574\u5207\u5230\u5176\u4ed6\u4e8b\u60c5\u4e0a 3\u3001\u6570\u636e\u5b8c\u6574\u4e86\uff0c\u7acb\u5373\u6267\u884c\u547d\u4ee4\uff0c\u5c06\u6267\u884c\u7ed3\u679c\u653e\u5230\u7f13\u51b2\u533a 4\u3001\u5c06\u6570\u636e\u7ed9\u5ba2\u6237\u7aef\uff0c\u5982\u679c\u4e00\u6b21\u7ed9\u4e0d\u5b8c\uff0c\u5c31\u7b49 \u4e0b\u6b21\u80fd\u7ed9\u65f6 \u518d\u7ed9\uff0c\u4e0d\u7b49\uff0c\u76f4\u5230\u5168\u90e8\u7ed9\u5b8c","title":"Redis\u5904\u7406\u547d\u4ee4\u8fc7\u7a0b"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#_1","text":"NOTE: 1\u3001Redis event library ae 2\u3001Linux epoll","title":"\u4e8b\u4ef6\u9a71\u52a8"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#io_1","text":"redis\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u4e00\u4e2a\u6b7b\u5faa\u73af aeMain \uff0c\u5728\u8fd9\u4e2a\u5faa\u73af\u91cc\u4e00\u76f4\u7b49\u5f85\u4e8b\u4ef6\u53d1\u751f\uff0c\u4e8b\u4ef6\u5206\u4e3aIO\u4e8b\u4ef6\u548ctimer\u4e8b\u4ef6\uff0ctimer\u4e8b\u4ef6\u662f\u4e00\u4e9b\u5b9a\u65f6\u6267\u884c\u7684\u4efb\u52a1\uff0c\u5982expire key\u7b49\uff0c\u672c\u6587\u53ea\u804aIO\u4e8b\u4ef6\u3002 NOTE: event loop epoll\u5904\u7406\u7684\u662fsocket\u7684\u53ef\u8bfb\u3001\u53ef\u5199\u4e8b\u4ef6\uff0c\u5f53\u4e8b\u4ef6\u53d1\u751f\u540e\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u7684\u901a\u77e5\u65b9\u5f0f\uff0c \u5f53\u60f3\u8981\u5f02\u6b65\u76d1\u542c\u67d0\u4e2asocket\u7684\u8bfb\u5199\u4e8b\u4ef6\u65f6\uff0c\u9700\u8981\u53bb\u4e8b\u4ef6\u9a71\u52a8\u6846\u67b6\u4e2d\u6ce8\u518c\u8981\u76d1\u542c\u4e8b\u4ef6\u7684socket\uff0c\u4ee5\u53ca\u5bf9\u5e94\u4e8b\u4ef6\u7684\u56de\u8c03function\u3002\u7136\u540e\u6b7b\u5faa\u73af\u4e2d\u53ef\u4ee5\u901a\u8fc7epoll_wait\u4e0d\u65ad\u5730\u53bb\u62ff\u53d1\u751f\u4e86\u53ef\u8bfb\u5199\u4e8b\u4ef6\u7684socket\uff0c\u4f9d\u6b21\u5904\u7406\u5373\u53ef\u3002 1\u3001 \u53ef\u8bfb \u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\uff0c\u5bf9\u5e94\u7684socket\u4e2d\u6709\u65b0\u7684tcp\u6570\u636e\u5305\u5230\u6765\u3002 2\u3001 \u53ef\u5199 \u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\uff0c\u5bf9\u5e94\u7684socket\u5199\u7f13\u51b2\u533a\u5df2\u7ecf\u7a7a\u4e86(\u6570\u636e\u901a\u8fc7\u7f51\u7edc\u5df2\u7ecf\u53d1\u7ed9\u4e86\u5ba2\u6237\u7aef)","title":"\u5355\u7ebf\u7a0bIO\u5904\u7406\u8fc7\u7a0b"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#_2","text":"\u4e00\u56fe\u80dc\u524d\u8a00\uff0c\u5b8c\u6574\u3001\u8be6\u7ec6\u6d41\u7a0b\u56fe\u5982\u4e0b\uff1a NOTE: \u53f3\u4e0b\u89d2\u662f\u5b8c\u6574\u7684\u5199\u8fc7\u7a0b 1\u3001 aeMain() \u5185\u90e8\u662f\u4e00\u4e2a\u6b7b\u5faa\u73af\uff0c\u4f1a\u5728 epoll_wait \u5904\u77ed\u6682\u4f11\u7720 2\u3001 epoll_wait \u8fd4\u56de\u7684\u662f\u5f53\u524d\u53ef\u8bfb\u3001\u53ef\u5199\u7684socket\u5217\u8868 3\u3001 beforeSleep \u662f\u8fdb\u5165\u4f11\u7720\u524d\u6267\u884c\u7684\u903b\u8f91\uff0c\u6838\u5fc3\u662f\u56de\u5199\u6570\u636e\u5230socket 4\u3001\u6838\u5fc3\u903b\u8f91\u90fd\u662f\u7531IO\u4e8b\u4ef6\u89e6\u53d1\uff0c\u8981\u4e48\u53ef\u8bfb\uff0c\u8981\u4e48\u53ef\u5199\uff0c\u5426\u5219\u6267\u884ctimer\u5b9a\u65f6\u4efb\u52a1 5\u3001\u7b2c\u4e00\u6b21\u7684IO\u53ef\u8bfb\u4e8b\u4ef6\uff0c\u662f\u76d1\u542csocket(\u5982\u76d1\u542c6379\u7684socket)\uff0c\u5f53\u6709\u63e1\u624b\u8bf7\u6c42\u65f6\uff0c\u4f1a\u6267\u884c accept \u8c03\u7528\uff0c\u5f97\u5230\u4e00\u4e2a\u8fde\u63a5socket\uff0c\u6ce8\u518c\u53ef\u8bfb\u56de\u8c03 createClient \uff0c\u5f80\u540e\u5ba2\u6237\u7aef\u548credis\u7684\u6570\u636e\u90fd\u901a\u8fc7\u8fd9\u4e2asocket\u8fdb\u884c 6\u3001\u4e00\u4e2a\u5b8c\u6574\u7684\u547d\u4ee4\uff0c\u53ef\u80fd\u4f1a\u901a\u8fc7\u591a\u6b21 4 \u624d\u80fd\u4ecesocket\u8bfb\u5b8c\uff0c\u8fd9\u610f\u5473\u8fd9\u591a\u6b21\u53ef\u8bfbIO\u4e8b\u4ef6 7\u3001\u547d\u4ee4\u6267\u884c\u7684\u7ed3\u679c\u4f1a\u5199\uff0c\u4e5f\u662f\u8fd9\u6837\uff0c\u5927\u6982\u7387\u4f1a\u901a\u8fc7\u591a\u6b21\u53ef\u5199\u56de\u8c03\u624d\u80fd\u5199\u5b8c 8\u3001\u5f53\u547d\u4ee4\u88ab\u6267\u884c\u5b8c\u540e\uff0c\u5bf9\u5e94\u7684\u8fde\u63a5\u4f1a\u88ab\u8ffd\u52a0\u5230 clients_pending_write \uff0c beforeSleep \u4f1a\u5c1d\u8bd5\u56de\u5199\u5230 socket \uff0c\u5199\u4e0d\u5b8c\u4f1a\u6ce8\u518c\u53ef\u5199\u4e8b\u4ef6\uff0c\u4e0b\u6b21\u7ee7\u7eed\u5199 9\u3001\u6574\u4e2a\u8fc7\u7a0bIO\u5168\u90e8\u90fd\u662f\u540c\u6b65\u975e\u963b\u585e\uff0c\u6ca1\u6709\u6d6a\u8d39\u7b49\u5f85\u65f6\u95f4 10\u3001\u6ce8\u518c\u4e8b\u4ef6\u7684\u51fd\u6570\u53eb aeCreateFileEvent","title":"\u6d41\u7a0b\u56fe"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#io_2","text":"NOTE: Redis\u91c7\u7528\u7684\u662fReactor\u3001\u540c\u6b65\u975e\u963b\u585eIO\uff1bread\u3001write\u662f\u540c\u6b65\u7684\uff0c\u8fd9\u90e8\u5206\u5de5\u4f5c\u662f\u4f1a\u6d88\u8017\u5927\u91cf\u7684CPU\u65f6\u95f4\u7684\uff1b\u8fd9\u662fRedis\u5f15\u5165\u591a\u7ebf\u7a0bIO\u7684\u539f\u56e0 \u540c\u65f6\u8fd9\u4e2a\u6a21\u578b\u6709\u51e0\u4e2a\u7f3a\u9677\uff1a 1\u3001\u53ea\u80fd\u7528\u4e00\u4e2acpu\u6838(\u5ffd\u7565\u540e\u53f0\u7ebf\u7a0b) 2\u3001\u5982\u679cvalue\u6bd4\u8f83\u5927\uff0credis\u7684QPS\u4f1a\u4e0b\u964d\u5f97\u5f88\u5389\u5bb3\uff0c\u6709\u65f6\u4e00\u4e2a\u5927key\u5c31\u53ef\u4ee5\u62d6\u57ae 3\u3001QPS\u96be\u4ee5\u66f4\u4e0a\u4e00\u5c42\u697c redis\u4e3b\u7ebf\u7a0b\u7684\u65f6\u95f4\u6d88\u8017\u4e3b\u8981\u5728\u4e24\u4e2a\u65b9\u9762\uff1a 1\u3001\u903b\u8f91\u8ba1\u7b97\u7684\u6d88\u8017 2\u3001\u540c\u6b65IO\u8bfb\u5199\uff0c\u62f7\u8d1d\u6570\u636e\u5bfc\u81f4\u7684\u6d88\u8017 \u5f53value\u6bd4\u8f83\u5927\u65f6\uff0c\u74f6\u9888\u4f1a\u5148\u51fa\u73b0\u5728\u540c\u6b65IO\u4e0a(\u5047\u8bbe\u5e26\u5bbd\u548c\u5185\u5b58\u8db3\u591f)\uff0c\u8fd9\u90e8\u5206\u6d88\u8017\u5728\u4e8e\u4e24\u90e8\u5206\uff1a 1\u3001\u4ecesocket\u4e2d\u8bfb\u53d6\u8bf7\u6c42\u6570\u636e\uff0c\u4f1a\u4ece\u5185\u6838\u6001\u5c06\u6570\u636e\u62f7\u8d1d\u5230\u7528\u6237\u6001 \uff08read\u8c03\u7528\uff09 2\u3001\u5c06\u6570\u636e\u56de\u5199\u5230socket\uff0c\u4f1a\u5c06\u6570\u636e\u4ece\u7528\u6237\u6001\u62f7\u8d1d\u5230\u5185\u6838\u6001 \uff08write\u8c03\u7528\uff09 \u8fd9\u90e8\u5206\u6570\u636e\u8bfb\u5199\u4f1a\u5360\u7528\u5927\u91cf\u7684cpu\u65f6\u95f4\uff0c\u4e5f\u76f4\u63a5\u5bfc\u81f4\u4e86\u74f6\u9888\u3002 \u5982\u679c\u80fd\u6709\u591a\u4e2a\u7ebf\u7a0b\u6765\u5206\u62c5\u8fd9\u90e8\u5206\u6d88\u8017\uff0c\u90a3redis\u7684\u541e\u5410\u91cf\u8fd8\u80fd\u66f4\u4e0a\u4e00\u5c42\u697c\uff0c\u8fd9\u4e5f\u662fredis\u5f15\u5165\u591a\u7ebf\u7a0bIO\u7684\u76ee\u7684\u3002[3] NOTE: \u4e0a\u8ff0\u603b\u7ed3\u4e86Redis\u5f15\u5165\u591a\u7ebf\u7a0b\u7684\u539f\u56e0","title":"\u5355\u7ebf\u7a0bIO\u7684\u74f6\u9888"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#io_3","text":"\u4e0a\u9762\u5df2\u7ecf\u68b3\u7406\u4e86\u5355\u7ebf\u7a0bIO\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u53ca\u591a\u7ebf\u7a0bIO\u8981\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u63a5\u4e0b\u6765\u5c06\u76ee\u5149\u653e\u5230\uff1a \u5982\u4f55\u7528\u591a\u7ebf\u7a0b\u5206\u62c5IO\u7684\u8d1f\u8377\u3002\u5176\u505a\u6cd5\u7528\u7b80\u5355\u7684\u8bdd\u6765\u8bf4\u5c31\u662f\uff1a 1\u3001\u7528\u4e00\u7ec4\u5355\u72ec\u7684\u7ebf\u7a0b\u4e13\u95e8\u8fdb\u884c read/write socket\u8bfb\u5199\u8c03\u7528 \uff08\u540c\u6b65IO\uff09 NOTE: IO threads 2\u3001**\u8bfb\u56de\u8c03\u51fd\u6570**\u4e2d\u4e0d\u518d\u8bfb\u6570\u636e\uff0c\u800c\u662f\u5c06\u5bf9\u5e94\u7684\u8fde\u63a5\u8ffd\u52a0\u5230\u53ef\u8bfb clients_pending_read \u7684\u94fe\u8868 3\u3001\u4e3b\u7ebf\u7a0b\u5728 beforeSleep \u4e2d\u5c06**IO\u8bfb\u4efb\u52a1**\u5206\u7ed9IO\u7ebf\u7a0b\u7ec4 4\u3001\u4e3b\u7ebf\u7a0b\u81ea\u5df1\u4e5f\u5904\u7406\u4e00\u4e2a**IO\u8bfb\u4efb\u52a1**\uff0c\u5e76**\u81ea\u65cb\u5f0f**\u7b49IO\u7ebf\u7a0b\u7ec4\u5904\u7406\u5b8c\uff0c\u518d\u7ee7\u7eed\u5f80\u4e0b 5\u3001\u4e3b\u7ebf\u7a0b\u5728 beforeSleep \u4e2d\u5c06**IO\u5199\u4efb\u52a1**\u5206\u7ed9IO\u7ebf\u7a0b\u7ec4 6\u3001\u4e3b\u7ebf\u7a0b\u81ea\u5df1\u4e5f\u5904\u7406\u4e00\u4e2a**IO\u5199\u4efb\u52a1**\uff0c\u5e76**\u81ea\u65cb\u5f0f**\u7b49IO\u7ebf\u7a0b\u7ec4\u5904\u7406\u5b8c\uff0c\u518d\u7ee7\u7eed\u5f80\u4e0b 7\u3001IO\u7ebf\u7a0b\u7ec4\u8981\u4e48\u540c\u65f6\u5728\u8bfb\uff0c\u8981\u4e48\u540c\u65f6\u5728\u5199 8\u3001\u547d\u4ee4\u7684\u6267\u884c\u7531\u4e3b\u7ebf\u7a0b\u4e32\u884c\u6267\u884c(\u4fdd\u6301\u5355\u7ebf\u7a0b) 9\u3001IO\u7ebf\u7a0b\u6570\u91cf\u53ef\u914d\u7f6e","title":"\u591a\u7ebf\u7a0bIO"},{"location":"Basic/Thread-model/zhihu-Redis-6.0-%E5%A4%9A%E7%BA%BF%E7%A8%8BIO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3/#_3","text":"NOTE: \u547d\u4ee4\u7684\u6267\u884c\u653e\u5230\u4e86 beforeSleep \u56de\u8c03\u4e2d\u4e86 beforesleep \u4e2d\uff0c\u5148\u8ba9IO\u7ebf\u7a0b\u8bfb\u6570\u636e\uff0c\u7136\u540e\u518d\u8ba9IO\u7ebf\u7a0b\u5199\u6570\u636e\u3002 \u8bfb\u5199\u65f6\uff0c\u591a\u7ebf\u7a0b\u80fd\u5e76\u53d1\u6267\u884c\uff0c\u5229\u7528\u591a\u6838\u3002 1\u3001\u5c06**\u8bfb\u4efb\u52a1**\u5747\u5300\u5206\u53d1\u5230\u5404\u4e2aIO\u7ebf\u7a0b\u7684\u4efb\u52a1\u94fe\u8868 io_threads_list[i] \uff0c\u5c06 io_threads_pending[i] \u8bbe\u7f6e\u4e3a\u5bf9\u5e94\u7684\u4efb\u52a1\u6570\uff0c\u6b64\u65f6IO\u7ebf\u7a0b\u5c06\u4ece\u6b7b\u5faa\u73af\u4e2d\u88ab\u6fc0\u6d3b\uff0c\u5f00\u59cb\u6267\u884c\u4efb\u52a1\uff0c\u6267\u884c\u5b8c\u6bd5\u540e\uff0c\u4f1a\u5c06 io_threads_pending[i] \u6e05\u96f6\u3002 \u51fd\u6570\u540d\u4e3a\uff1a handleClientsWithPendingReadsUsingThreads 2\u3001\u5c06**\u5199\u4efb\u52a1**\u5747\u5300\u5206\u53d1\u5230\u5404\u4e2aIO\u7ebf\u7a0b\u7684\u4efb\u52a1\u94fe\u8868 io_threads_list[i] \uff0c\u5c06 io_threads_pending[i] \u8bbe\u7f6e\u4e3a\u5bf9\u5e94\u7684\u4efb\u52a1\u6570\uff0c\u6b64\u65f6IO\u7ebf\u7a0b\u5c06\u4ece\u6b7b\u5faa\u73af\u4e2d\u88ab\u6fc0\u6d3b\uff0c\u5f00\u59cb\u6267\u884c\u4efb\u52a1\uff0c\u6267\u884c\u5b8c\u6bd5\u540e\uff0c\u4f1a\u5c06 io_threads_pending[i] \u6e05\u96f6\u3002 \u51fd\u6570\u540d\u4e3a\uff1a handleClientsWithPendingWritesUsingThreads 3\u3001 beforeSleep \u4e2d\u4e3b\u7ebf\u7a0b\u4e5f\u4f1a\u6267\u884c\u5176\u4e2d\u4e00\u4e2a\u4efb\u52a1(\u56fe\u4e2d\u5ffd\u7565\u4e86)\uff0c\u6267\u884c\u5b8c\u540e\u81ea\u65cb\u7b49\u5f85IO\u7ebf\u7a0b\u5904\u7406\u5b8c\u3002 4\u3001\u8bfb\u4efb\u52a1\u8981\u4e48\u5728beforeSleep\u4e2d\u88ab\u6267\u884c\uff0c\u8981\u4e48\u5728IO\u7ebf\u7a0b\u88ab\u6267\u884c\uff0c\u4e0d\u4f1a\u518d\u5728\u8bfb\u56de\u8c03\u4e2d\u6267\u884c 5\u3001\u5199\u4efb\u52a1\u4f1a\u5206\u6563\u5230 beforeSleep\u3001IO\u7ebf\u7a0b\u3001\u5199\u56de\u8c03\u4e2d\u6267\u884c 6\u3001\u4e3b\u7ebf\u7a0b\u548cIO\u7ebf\u7a0b\u4ea4\u4e92\u662f\u65e0\u9501\u7684\uff0c\u901a\u8fc7**\u6807\u5fd7\u4f4d**\u8bbe\u7f6e\u8fdb\u884c\uff0c\u4e0d\u4f1a\u540c\u65f6\u5199\u4efb\u52a1\u94fe\u8868 \u6027\u80fd\u636e\u6d4b\u8bd5\u63d0\u5347\u4e86\u4e00\u500d\u4ee5\u4e0a(4\u4e2aIO\u7ebf\u7a0b)\u3002 [4] \u6b22\u8fce\u60a8\u7684\u63d0\u95ee\u3001\u6307\u6b63\u3001\u5efa\u8bae\u7b49\u3002","title":"\u5b8c\u6574\u6d41\u7a0b\u56fe"},{"location":"Basic/main/","text":"mian 1\u3001\u9700\u8981\u52fe\u753b\u51faRedis\u7684\u4e3b\u6d41\u7a0b int main () { parse argv ; load config file ; daemonize ; initServer ; // \u975e\u5e38\u91cd\u8981\u7684\u4e00\u6b65\uff0c\u5efa\u7acbevent \u548c event handler\u7684\u6620\u5c04 aeMain ( server . el ); // event loop\u3001main loop aeDeleteEventLoop ( server . el ); // \u9000\u51faevent loop\u3001main loop }","title":"Introduction"},{"location":"Basic/main/#mian","text":"1\u3001\u9700\u8981\u52fe\u753b\u51faRedis\u7684\u4e3b\u6d41\u7a0b int main () { parse argv ; load config file ; daemonize ; initServer ; // \u975e\u5e38\u91cd\u8981\u7684\u4e00\u6b65\uff0c\u5efa\u7acbevent \u548c event handler\u7684\u6620\u5c04 aeMain ( server . el ); // event loop\u3001main loop aeDeleteEventLoop ( server . el ); // \u9000\u51faevent loop\u3001main loop }","title":"mian"},{"location":"Basic/main/Event%26%26handler/","text":"Event and handler \u4e00\u3001\u5b8c\u5168event-driven\uff0c\u6bcf\u79cdevent\u90fd\u6709\u5bf9\u5e94\u7684handler\u3001callback \u4e8c\u3001state machine\u3001idempotent\u5e42\u7b49 1\u3001\u8fd9\u662f\u5728\u9605\u8bfb Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b \u65f6\uff0c\u5176\u4e2d sentinelFailoverStateMachine 2\u3001\u5e42\u7b49\uff0c\u7531\u4e8e\u662f periodical event\uff0c\u56e0\u6b64\u4f1a\u88ab\u4e0d\u65ad\u5730\u8c03\u7528\uff0c\u56e0\u6b64\u5c31\u9700\u8981\u8003\u8651idempotent\u5e42\u7b49\uff0c\u53ef\u4ee5\u770b\u5230\uff0cRedis\u4e2d\uff0c\u5f88\u591a\u7684 periodical event handler\u90fd\u662f\u8003\u8651\u4e86idempotent\uff0c\u5b9e\u73b0\u65b9\u5f0f\u6709: state machine: sentinel file event readQueryFromClient \u5728\u521b\u5efaclient\u7684\u65f6\u5019\u8bbe\u7f6e client * createClient ( int fd ) { client * c = zmalloc ( sizeof ( client )); /* passing -1 as fd it is possible to create a non connected client. * This is useful since all the commands needs to be executed * in the context of a client. When commands are executed in other * contexts (for instance a Lua script) we need a non connected client. */ if ( fd != -1 ) { anetNonBlock ( NULL , fd ); anetEnableTcpNoDelay ( NULL , fd ); if ( server . tcpkeepalive ) anetKeepAlive ( NULL , fd , server . tcpkeepalive ); if ( aeCreateFileEvent ( server . el , fd , AE_READABLE , readQueryFromClient , c ) == AE_ERR ) { close ( fd ); zfree ( c ); return NULL ; } } } Redis aeFileEvent and struct client server\u7aef\u63a5\u53d7\u5230\u4e86client\u7aef\u7684\u8bf7\u6c42\u540e\u624d\u4f1a\u521b\u5efa struct client instance\uff1bclient\u7aef\u7684socket file descriptor\u6709\u5bf9\u5e94\u7684 aeFileEvent \u4e0a\u8ff0\u5c31\u5c55\u793a\u4e86client\u548cevent\u4e4b\u95f4\u7684\u5173\u8054 readQueryFromClient \u7684\u5b9e\u73b0\u5206\u6790 readQueryFromClient \u5b9e\u73b0Redis Protocol readQueryFromClient \u2192 processInputBufferAndReplicate processInputBufferAndReplicate \u2192 processInputBuffer \u2192 replicationFeedSlavesFromMasterStream processInputBuffer \u2192 processInlineBuffer \u2192 processMultibulkBuffer acceptTcpHandler void initServer ( void ) { /* Create an event handler for accepting new connections in TCP and Unix * domain sockets. */ for ( j = 0 ; j < server . ipfd_count ; j ++ ) { if ( aeCreateFileEvent ( server . el , server . ipfd [ j ], AE_READABLE , acceptTcpHandler , NULL ) == AE_ERR ) { serverPanic ( \"Unrecoverable error creating server.ipfd file event.\" ); } } if ( server . sofd > 0 && aeCreateFileEvent ( server . el , server . sofd , AE_READABLE , acceptUnixHandler , NULL ) == AE_ERR ) serverPanic ( \"Unrecoverable error creating server.sofd file event.\" ); /* Register a readable event for the pipe used to awake the event loop * when a blocked client in a module needs attention. */ if ( aeCreateFileEvent ( server . el , server . module_blocked_pipe [ 0 ], AE_READABLE , moduleBlockedClientPipeReadable , NULL ) == AE_ERR ) { serverPanic ( \"Error registering the readable event for the module \" \"blocked clients subsystem.\" ); } } acceptUnixHandler moduleBlockedClientPipeReadable replication replication.c:syncWithMaster int fd; fd = anetTcpNonBlockBestEffortBindConnect(NULL, server.masterhost,server.masterport,NET_FIRST_BIND_ADDR); if (fd == -1) { serverLog(LL_WARNING,\"Unable to connect to MASTER: %s\", strerror(errno)); return C_ERR; } if (aeCreateFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE,syncWithMaster,NULL) == AE_ERR) { close(fd); serverLog(LL_WARNING,\"Can't create readable event for SYNC\"); return C_ERR; } time event \u4ece\u76ee\u524dredis\u7684\u5b9e\u73b0\u6765\u770b\uff0c aeCreateTimeEvent \u6240\u521b\u5efa\u7684time event\u6709\uff1a server.c:serverCron module.c:moduleTimerHandler serverCron serverCron \u5b9e\u73b0\u4e3b\u8981\u7684\u6309\u7167time\u8fdb\u884cpoll\uff0c\u5b83\u5305\u542b\u5982\u4e0b\u7684\u4e00\u4e9bpoll\uff1a replicationCron poll backgroud process\u7684\u72b6\u6001 \u5b83\u4f1apoll background saving process\u7684\u72b6\u6001\uff0c\u5728\u5b83\u5b8c\u6210\u7684\u65f6\u5019\uff0c\u4f1a\u8c03\u7528 backgroundSaveDoneHandler \u6216\u8005 backgroundRewriteDoneHandler \u3002\u53c2\u89c1\u300a redis-code-analysis-background process.md \u300b\u3002 clusterCron databasesCron This function handles 'background' operations we are required to do incrementally in Redis databases, such as active key expiring, resizing, rehashing clientsCron This function is called by serverCron() and is used in order to perform operations on clients that are important to perform constantly. For instancewe use this function in order to disconnect clients after a timeout, including clients blocked in some blocking command with a non-zero timeout.The function makes some effort to process all the clients every second, even if this cannot be strictly guaranteed, since serverCron() may be called with an actual frequency lower than server.hz in case of latency events like slowcommands.It is very important for this function, and the functions it calls, to be very fast: sometimes Redis has tens of hundreds of connected clients, and the default server.hz value is 10, so sometimes here we need to process thousandsof clients per second, turning this function into a source of latency.","title":"Introduction"},{"location":"Basic/main/Event%26%26handler/#event#and#handler","text":"\u4e00\u3001\u5b8c\u5168event-driven\uff0c\u6bcf\u79cdevent\u90fd\u6709\u5bf9\u5e94\u7684handler\u3001callback \u4e8c\u3001state machine\u3001idempotent\u5e42\u7b49 1\u3001\u8fd9\u662f\u5728\u9605\u8bfb Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b \u65f6\uff0c\u5176\u4e2d sentinelFailoverStateMachine 2\u3001\u5e42\u7b49\uff0c\u7531\u4e8e\u662f periodical event\uff0c\u56e0\u6b64\u4f1a\u88ab\u4e0d\u65ad\u5730\u8c03\u7528\uff0c\u56e0\u6b64\u5c31\u9700\u8981\u8003\u8651idempotent\u5e42\u7b49\uff0c\u53ef\u4ee5\u770b\u5230\uff0cRedis\u4e2d\uff0c\u5f88\u591a\u7684 periodical event handler\u90fd\u662f\u8003\u8651\u4e86idempotent\uff0c\u5b9e\u73b0\u65b9\u5f0f\u6709: state machine: sentinel","title":"Event and handler"},{"location":"Basic/main/Event%26%26handler/#file#event","text":"","title":"file event"},{"location":"Basic/main/Event%26%26handler/#readqueryfromclient","text":"\u5728\u521b\u5efaclient\u7684\u65f6\u5019\u8bbe\u7f6e client * createClient ( int fd ) { client * c = zmalloc ( sizeof ( client )); /* passing -1 as fd it is possible to create a non connected client. * This is useful since all the commands needs to be executed * in the context of a client. When commands are executed in other * contexts (for instance a Lua script) we need a non connected client. */ if ( fd != -1 ) { anetNonBlock ( NULL , fd ); anetEnableTcpNoDelay ( NULL , fd ); if ( server . tcpkeepalive ) anetKeepAlive ( NULL , fd , server . tcpkeepalive ); if ( aeCreateFileEvent ( server . el , fd , AE_READABLE , readQueryFromClient , c ) == AE_ERR ) { close ( fd ); zfree ( c ); return NULL ; } } }","title":"readQueryFromClient"},{"location":"Basic/main/Event%26%26handler/#redis#aefileevent#and#struct#client","text":"server\u7aef\u63a5\u53d7\u5230\u4e86client\u7aef\u7684\u8bf7\u6c42\u540e\u624d\u4f1a\u521b\u5efa struct client instance\uff1bclient\u7aef\u7684socket file descriptor\u6709\u5bf9\u5e94\u7684 aeFileEvent \u4e0a\u8ff0\u5c31\u5c55\u793a\u4e86client\u548cevent\u4e4b\u95f4\u7684\u5173\u8054","title":"Redis aeFileEvent and struct client"},{"location":"Basic/main/Event%26%26handler/#readqueryfromclient_1","text":"","title":"readQueryFromClient\u7684\u5b9e\u73b0\u5206\u6790"},{"location":"Basic/main/Event%26%26handler/#readqueryfromclientredis#protocol","text":"readQueryFromClient \u2192 processInputBufferAndReplicate processInputBufferAndReplicate \u2192 processInputBuffer \u2192 replicationFeedSlavesFromMasterStream processInputBuffer \u2192 processInlineBuffer \u2192 processMultibulkBuffer","title":"readQueryFromClient\u5b9e\u73b0Redis Protocol"},{"location":"Basic/main/Event%26%26handler/#accepttcphandler","text":"void initServer ( void ) { /* Create an event handler for accepting new connections in TCP and Unix * domain sockets. */ for ( j = 0 ; j < server . ipfd_count ; j ++ ) { if ( aeCreateFileEvent ( server . el , server . ipfd [ j ], AE_READABLE , acceptTcpHandler , NULL ) == AE_ERR ) { serverPanic ( \"Unrecoverable error creating server.ipfd file event.\" ); } } if ( server . sofd > 0 && aeCreateFileEvent ( server . el , server . sofd , AE_READABLE , acceptUnixHandler , NULL ) == AE_ERR ) serverPanic ( \"Unrecoverable error creating server.sofd file event.\" ); /* Register a readable event for the pipe used to awake the event loop * when a blocked client in a module needs attention. */ if ( aeCreateFileEvent ( server . el , server . module_blocked_pipe [ 0 ], AE_READABLE , moduleBlockedClientPipeReadable , NULL ) == AE_ERR ) { serverPanic ( \"Error registering the readable event for the module \" \"blocked clients subsystem.\" ); } }","title":"acceptTcpHandler"},{"location":"Basic/main/Event%26%26handler/#acceptunixhandler","text":"","title":"acceptUnixHandler"},{"location":"Basic/main/Event%26%26handler/#moduleblockedclientpipereadable","text":"","title":"moduleBlockedClientPipeReadable"},{"location":"Basic/main/Event%26%26handler/#replication","text":"","title":"replication"},{"location":"Basic/main/Event%26%26handler/#replicationcsyncwithmaster","text":"int fd; fd = anetTcpNonBlockBestEffortBindConnect(NULL, server.masterhost,server.masterport,NET_FIRST_BIND_ADDR); if (fd == -1) { serverLog(LL_WARNING,\"Unable to connect to MASTER: %s\", strerror(errno)); return C_ERR; } if (aeCreateFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE,syncWithMaster,NULL) == AE_ERR) { close(fd); serverLog(LL_WARNING,\"Can't create readable event for SYNC\"); return C_ERR; }","title":"replication.c:syncWithMaster"},{"location":"Basic/main/Event%26%26handler/#time#event","text":"\u4ece\u76ee\u524dredis\u7684\u5b9e\u73b0\u6765\u770b\uff0c aeCreateTimeEvent \u6240\u521b\u5efa\u7684time event\u6709\uff1a server.c:serverCron module.c:moduleTimerHandler","title":"time event"},{"location":"Basic/main/Event%26%26handler/#servercron","text":"serverCron \u5b9e\u73b0\u4e3b\u8981\u7684\u6309\u7167time\u8fdb\u884cpoll\uff0c\u5b83\u5305\u542b\u5982\u4e0b\u7684\u4e00\u4e9bpoll\uff1a","title":"serverCron"},{"location":"Basic/main/Event%26%26handler/#replicationcron","text":"","title":"replicationCron"},{"location":"Basic/main/Event%26%26handler/#poll#backgroud#process","text":"\u5b83\u4f1apoll background saving process\u7684\u72b6\u6001\uff0c\u5728\u5b83\u5b8c\u6210\u7684\u65f6\u5019\uff0c\u4f1a\u8c03\u7528 backgroundSaveDoneHandler \u6216\u8005 backgroundRewriteDoneHandler \u3002\u53c2\u89c1\u300a redis-code-analysis-background process.md \u300b\u3002","title":"poll backgroud process\u7684\u72b6\u6001"},{"location":"Basic/main/Event%26%26handler/#clustercron","text":"","title":"clusterCron"},{"location":"Basic/main/Event%26%26handler/#databasescron","text":"This function handles 'background' operations we are required to do incrementally in Redis databases, such as active key expiring, resizing, rehashing","title":"databasesCron"},{"location":"Basic/main/Event%26%26handler/#clientscron","text":"This function is called by serverCron() and is used in order to perform operations on clients that are important to perform constantly. For instancewe use this function in order to disconnect clients after a timeout, including clients blocked in some blocking command with a non-zero timeout.The function makes some effort to process all the clients every second, even if this cannot be strictly guaranteed, since serverCron() may be called with an actual frequency lower than server.hz in case of latency events like slowcommands.It is very important for this function, and the functions it calls, to be very fast: sometimes Redis has tens of hundreds of connected clients, and the default server.hz value is 10, so sometimes here we need to process thousandsof clients per second, turning this function into a source of latency.","title":"clientsCron"},{"location":"Basic/main/redis-doc-Redis-internals/","text":"Redis internals If you are reading this README you are likely in front of a Github page or you just untarred the Redis distribution tar ball. In both the cases you are basically one step away from the source code, so here we explain the Redis source code layout, what is in each file as a general idea, the most important functions and structures inside the Redis server and so forth. We keep all the discussion at a high level without digging into the details since this document would be huge otherwise and our code base changes continuously, but a general idea should be a good starting point to understand more. Moreover most of the code is heavily commented and easy to follow. Source code layout The Redis root directory just contains this README , the Makefile which calls the real Makefile inside the src directory and an example configuration for Redis and Sentinel. You can find a few shell scripts that are used in order to execute the Redis, Redis Cluster and Redis Sentinel unit tests, which are implemented inside the tests directory. Inside the root are the following important directories: src : contains the Redis implementation, written in C. tests : contains the unit tests, implemented in Tcl. deps : contains libraries Redis uses. Everything needed to compile Redis is inside this directory; your system just needs to provide libc , a POSIX compatible interface and a C compiler. Notably deps contains a copy of jemalloc , which is the default allocator of Redis under Linux. Note that under deps there are also things which started with the Redis project, but for which the main repository is not antirez/redis . SUMMARY : \u901a\u8fc7\u6253\u5370\u51faMakefile\u4e2d\u7684 MALLOC \u53d8\u91cf\uff0c\u53ef\u4ee5\u77e5\u9053\u786e\u5b9e\u4f7f\u7528\u7684\u662f jemalloc \u3002 There are a few more directories but they are not very important for our goals here. We'll focus mostly on src , where the Redis implementation is contained, exploring what there is inside each file. The order in which files are exposed is the logical one to follow in order to disclose\uff08\u516c\u5f00\uff09 different layers of complexity incrementally. TRANSLATION : \u6587\u4ef6\u88ab\u516c\u5f00\u7684\u987a\u5e8f\u662f\u9075\u5faa\u7684\u903b\u8f91\u987a\u5e8f\uff0c\u4ee5\u4fbf\u9010\u6b65\u5730\u516c\u5f00\u4e0d\u540c\u7684\u590d\u6742\u5c42\u3002 Note: lately Redis was refactored quite a bit. Function names and file names have been changed, so you may find that this documentation reflects the unstable branch more closely. For instance in Redis 3.0 the server.c and server.h files were named redis.c and redis.h . However the overall structure is the same. Keep in mind that all the new developments and pull requests should be performed against the unstable branch. server.h The simplest way to understand how a program works is to understand the data structures it uses. So we'll start from the main header file of Redis, which is server.h . SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63d0\u53ca\u7684\u89c2\u70b9\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u73b0\u5728\u60f3\u60f3\uff0c\u5bf9\u4e8elinux OS\uff0c\u5728\u6211\u77e5\u9053\u4e86\u5176kernel\u4e2d\u4f7f\u7528process control block\u6765\u7ba1\u7406\u63cf\u8ff0\u8fdb\u7a0b\u540e\uff0c\u5f88\u591a\u76f8\u5173\u7684\u6982\u5ff5\u7406\u89e3\u8d77\u6765\u5c31\u975e\u5e38\u8f7b\u677e\u4e86\uff1b All the server configuration and in general all the shared state is defined in a global structure called server , of type struct redisServer . A few important fields in this structure are: server.db is an array of Redis databases, where data is stored. server.commands is the command table. server.clients is a linked list of clients connected to the server. server.master is a special client, the master, if the instance is a replica. There are tons of other fields. Most fields are commented directly inside the structure definition. Another important Redis data structure is the one defining a client. In the past it was called redisClient , now just client . The structure has many fields, here we'll just show the main ones: struct client { int fd ; sds querybuf ; int argc ; robj ** argv ; redisDb * db ; int flags ; list * reply ; char buf [ PROTO_REPLY_CHUNK_BYTES ]; ... many other fields ... } The client structure defines a connected client : The fd field is the client socket file descriptor . argc and argv are populated with the command the client is executing, so that functions implementing a given Redis command can read the arguments. querybuf accumulates the requests from the client , which are parsed by the Redis server according to the Redis protocol and executed by calling the implementations of the commands the client is executing. reply and buf are dynamic and static buffers that accumulate the replies the server sends to the client. These buffers are incrementally written to the socket as soon as the file descriptor is writable. As you can see in the client structure above, arguments in a command are described as robj structures. The following is the full robj structure, which defines a Redis object : typedef struct redisObject { unsigned type : 4 ; unsigned encoding : 4 ; unsigned lru : LRU_BITS ; /* lru time (relative to server.lruclock) */ int refcount ; void * ptr ; } robj ; Basically this structure can represent all the basic Redis data types like strings, lists, sets, sorted sets and so forth. The interesting thing is that it has a type field, so that it is possible to know what type a given object has, and a refcount , so that the same object can be referenced in multiple places without allocating it multiple times. Finally the ptr field points to the actual representation of the object, which might vary even for the same type, depending on the encoding used. Redis objects are used extensively in the Redis internals, however in order to avoid the overhead of indirect accesses, recently in many places we just use plain dynamic strings not wrapped inside a Redis object . server.c This is the entry point of the Redis server , where the main() function is defined. The following are the most important steps in order to startup the Redis server. initServerConfig() setups the default values of the server structure. initServer() allocates the data structures needed to operate, setup the listening socket , and so forth. aeMain() starts the event loop which listens for new connections. There are two special functions called periodically by the event loop: serverCron() is called periodically (according to server.hz frequency), and performs tasks that must be performed from time to time, like checking for timedout clients . beforeSleep() is called every time the event loop fired, Redis served a few requests, and is returning back into the event loop. Inside server.c you can find code that handles other vital things of the Redis server: call() is used in order to call a given command in the context of a given client. SUMMARY : command\u7684\u6267\u884c\u662fin the context of a given client\uff0c\u8fd9\u8bf4\u660ecommand\u662f\u548cclient\u5173\u8054\u7684\uff1b activeExpireCycle() handles eviciton(\u9a71\u9010) of keys with a time to live set via the EXPIRE command. freeMemoryIfNeeded() is called when a new write command should be performed but Redis is out of memory according to the maxmemory directive. The global variable redisCommandTable defines all the Redis commands, specifying the name of the command, the function implementing the command, the number of arguments required, and other properties of each command. networking.c This file defines all the I/O functions with clients, masters and replicas (which in Redis are just special clients): createClient() allocates and initializes a new client. the addReply*() family of functions are used by commands implementations in order to append data to the client structure , that will be transmitted to the client as a reply for a given command executed. writeToClient() transmits the data pending in the output buffers to the client and is called by the writable event handler sendReplyToClient() . readQueryFromClient() is the readable event handler and accumulates data from read from the client into the query buffer. processInputBuffer() is the entry point in order to parse the client query buffer according to the Redis protocol. Once commands are ready to be processed, it calls processCommand() which is defined inside server.c in order to actually execute the command. freeClient() deallocates, disconnects and removes a client. SUMMARY : \u663e\u7136\uff0c\u8fd9\u4e2a\u6a21\u5757\u6240\u63cf\u8ff0\u7684\u90fd\u662f\u548cnetwork\u76f8\u5173\u7684\u5b9e\u73b0\uff0c\u5b83\u4eec\u5e94\u8be5\u90fd\u662f\u7531event loop\u6765\u8fdb\u884c\u8c03\u7528\uff1b\u5728 server.h \u7684 struct client \u662f\u652f\u6301\u4e0a\u8ff0\u64cd\u4f5c\u7684data structure\uff1b SUMMARY : \u65b0\u7248\u672c\u7684redis\u4e2d\u7684networking\u5df2\u7ecf\u6ca1\u6709\u4f7f\u7528\u8be5\u6587\u4ef6\u4e86\uff0c\u800c\u662f\u4f7f\u7528\u7684 anet.c aof.c and rdb.c As you can guess from the names these files implement the RDB and AOF persistence for Redis. Redis uses a persistence model based on the fork() system call in order to create a thread with the same (shared) memory content of the main Redis thread . This secondary thread dumps the content of the memory on disk. This is used by rdb.c to create the snapshots on disk and by aof.c in order to perform the AOF rewrite when the append only file gets too big. The implementation inside aof.c has additional functions in order to implement an API that allows commands to append new commands into the AOF file as clients execute them. The call() function defined inside server.c is responsible to call the functions that in turn will write the commands into the AOF. db.c Certain Redis commands operate on specific data types, others are general. Examples of generic commands are DEL and EXPIRE . They operate on keys and not on their values specifically. All those generic commands are defined inside db.c . Moreover db.c implements an API in order to perform certain operations on the Redis dataset without directly accessing the internal data structures. The most important functions inside db.c which are used in many commands implementations are the following: lookupKeyRead() and lookupKeyWrite() are used in order to get a pointer to the value associated to a given key, or NULL if the key does not exist. dbAdd() and its higher level counterpart setKey() create a new key in a Redis database. dbDelete() removes a key and its associated value. emptyDb() removes an entire single database or all the databases defined. The rest of the file implements the generic commands exposed to the client. object.c The robj structure defining Redis objects was already described. Inside object.c there are all the functions that operate with Redis objects at a basic level, like functions to allocate new objects, handle the reference counting and so forth. Notable functions inside this file: incrRefcount() and decrRefCount() are used in order to increment or decrement an object reference count. When it drops to 0 the object is finally freed. createObject() allocates a new object. There are also specialized functions to allocate string objects having a specific content, like createStringObjectFromLongLong() and similar functions. This file also implements the OBJECT command. \u200b replication.c This is one of the most complex files inside Redis, it is recommended to approach it only after getting a bit familiar with the rest of the code base. In this file there is the implementation of both the master and replica role of Redis. One of the most important functions inside this file is replicationFeedSlaves() that writes commands to the clients representing replica instances connected to our master, so that the replicas can get the writes performed by the clients: this way their data set will remain synchronized with the one in the master. This file also implements both the SYNC and PSYNC commands that are used in order to perform the first synchronization between masters and replicas, or to continue the replication after a disconnection. Other C files t_hash.c , t_list.c , t_set.c , t_string.c and t_zset.c contains the implementation of the Redis data types. They implement both an API to access a given data type, and the client commands implementations for these data types. ae.c implements the Redis event loop, it's a self contained library which is simple to read and understand. sds.c is the Redis string library, check http://github.com/antirez/sds for more information. anet.c is a library to use POSIX networking in a simpler way compared to the raw interface exposed by the kernel. dict.c is an implementation of a non-blocking hash table which rehashes incrementally. scripting.c implements Lua scripting. It is completely self contained from the rest of the Redis implementation and is simple enough to understand if you are familar with the Lua API. cluster.c implements the Redis Cluster. Probably a good read only after being very familiar with the rest of the Redis code base. If you want to read cluster.c make sure to read the Redis Cluster specification . Anatomy of a Redis command All the Redis commands are defined in the following way: void foobarCommand ( client * c ) { printf ( \"%s\" , c -> argv [ 1 ] -> ptr ); /* Do something with the argument. */ addReply ( c , shared . ok ); /* Reply something to the client. */ } The command is then referenced inside server.c in the command table: { \"foobar\" , foobarCommand , 2 , \"rtF\" , 0 , NULL , 0 , 0 , 0 , 0 , 0 }, In the above example 2 is the number of arguments the command takes, while \"rtF\" are the command flags , as documented in the command table top comment inside server.c . After the command operates in some way, it returns a reply to the client, usually using addReply() or a similar function defined inside networking.c . There are tons of commands implementations inside the Redis source code that can serve as examples of actual commands implementations. To write a few toy commands can be a good exercise to familiarize with the code base. There are also many other files not described here, but it is useless to cover everything. We want to just help you with the first steps. Eventually you'll find your way inside the Redis code base :-) Enjoy!","title":"Introduction"},{"location":"Basic/main/redis-doc-Redis-internals/#redis#internals","text":"If you are reading this README you are likely in front of a Github page or you just untarred the Redis distribution tar ball. In both the cases you are basically one step away from the source code, so here we explain the Redis source code layout, what is in each file as a general idea, the most important functions and structures inside the Redis server and so forth. We keep all the discussion at a high level without digging into the details since this document would be huge otherwise and our code base changes continuously, but a general idea should be a good starting point to understand more. Moreover most of the code is heavily commented and easy to follow.","title":"Redis internals"},{"location":"Basic/main/redis-doc-Redis-internals/#source#code#layout","text":"The Redis root directory just contains this README , the Makefile which calls the real Makefile inside the src directory and an example configuration for Redis and Sentinel. You can find a few shell scripts that are used in order to execute the Redis, Redis Cluster and Redis Sentinel unit tests, which are implemented inside the tests directory. Inside the root are the following important directories: src : contains the Redis implementation, written in C. tests : contains the unit tests, implemented in Tcl. deps : contains libraries Redis uses. Everything needed to compile Redis is inside this directory; your system just needs to provide libc , a POSIX compatible interface and a C compiler. Notably deps contains a copy of jemalloc , which is the default allocator of Redis under Linux. Note that under deps there are also things which started with the Redis project, but for which the main repository is not antirez/redis . SUMMARY : \u901a\u8fc7\u6253\u5370\u51faMakefile\u4e2d\u7684 MALLOC \u53d8\u91cf\uff0c\u53ef\u4ee5\u77e5\u9053\u786e\u5b9e\u4f7f\u7528\u7684\u662f jemalloc \u3002 There are a few more directories but they are not very important for our goals here. We'll focus mostly on src , where the Redis implementation is contained, exploring what there is inside each file. The order in which files are exposed is the logical one to follow in order to disclose\uff08\u516c\u5f00\uff09 different layers of complexity incrementally. TRANSLATION : \u6587\u4ef6\u88ab\u516c\u5f00\u7684\u987a\u5e8f\u662f\u9075\u5faa\u7684\u903b\u8f91\u987a\u5e8f\uff0c\u4ee5\u4fbf\u9010\u6b65\u5730\u516c\u5f00\u4e0d\u540c\u7684\u590d\u6742\u5c42\u3002 Note: lately Redis was refactored quite a bit. Function names and file names have been changed, so you may find that this documentation reflects the unstable branch more closely. For instance in Redis 3.0 the server.c and server.h files were named redis.c and redis.h . However the overall structure is the same. Keep in mind that all the new developments and pull requests should be performed against the unstable branch.","title":"Source code layout"},{"location":"Basic/main/redis-doc-Redis-internals/#serverh","text":"The simplest way to understand how a program works is to understand the data structures it uses. So we'll start from the main header file of Redis, which is server.h . SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63d0\u53ca\u7684\u89c2\u70b9\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u73b0\u5728\u60f3\u60f3\uff0c\u5bf9\u4e8elinux OS\uff0c\u5728\u6211\u77e5\u9053\u4e86\u5176kernel\u4e2d\u4f7f\u7528process control block\u6765\u7ba1\u7406\u63cf\u8ff0\u8fdb\u7a0b\u540e\uff0c\u5f88\u591a\u76f8\u5173\u7684\u6982\u5ff5\u7406\u89e3\u8d77\u6765\u5c31\u975e\u5e38\u8f7b\u677e\u4e86\uff1b All the server configuration and in general all the shared state is defined in a global structure called server , of type struct redisServer . A few important fields in this structure are: server.db is an array of Redis databases, where data is stored. server.commands is the command table. server.clients is a linked list of clients connected to the server. server.master is a special client, the master, if the instance is a replica. There are tons of other fields. Most fields are commented directly inside the structure definition. Another important Redis data structure is the one defining a client. In the past it was called redisClient , now just client . The structure has many fields, here we'll just show the main ones: struct client { int fd ; sds querybuf ; int argc ; robj ** argv ; redisDb * db ; int flags ; list * reply ; char buf [ PROTO_REPLY_CHUNK_BYTES ]; ... many other fields ... } The client structure defines a connected client : The fd field is the client socket file descriptor . argc and argv are populated with the command the client is executing, so that functions implementing a given Redis command can read the arguments. querybuf accumulates the requests from the client , which are parsed by the Redis server according to the Redis protocol and executed by calling the implementations of the commands the client is executing. reply and buf are dynamic and static buffers that accumulate the replies the server sends to the client. These buffers are incrementally written to the socket as soon as the file descriptor is writable. As you can see in the client structure above, arguments in a command are described as robj structures. The following is the full robj structure, which defines a Redis object : typedef struct redisObject { unsigned type : 4 ; unsigned encoding : 4 ; unsigned lru : LRU_BITS ; /* lru time (relative to server.lruclock) */ int refcount ; void * ptr ; } robj ; Basically this structure can represent all the basic Redis data types like strings, lists, sets, sorted sets and so forth. The interesting thing is that it has a type field, so that it is possible to know what type a given object has, and a refcount , so that the same object can be referenced in multiple places without allocating it multiple times. Finally the ptr field points to the actual representation of the object, which might vary even for the same type, depending on the encoding used. Redis objects are used extensively in the Redis internals, however in order to avoid the overhead of indirect accesses, recently in many places we just use plain dynamic strings not wrapped inside a Redis object .","title":"server.h"},{"location":"Basic/main/redis-doc-Redis-internals/#serverc","text":"This is the entry point of the Redis server , where the main() function is defined. The following are the most important steps in order to startup the Redis server. initServerConfig() setups the default values of the server structure. initServer() allocates the data structures needed to operate, setup the listening socket , and so forth. aeMain() starts the event loop which listens for new connections. There are two special functions called periodically by the event loop: serverCron() is called periodically (according to server.hz frequency), and performs tasks that must be performed from time to time, like checking for timedout clients . beforeSleep() is called every time the event loop fired, Redis served a few requests, and is returning back into the event loop. Inside server.c you can find code that handles other vital things of the Redis server: call() is used in order to call a given command in the context of a given client. SUMMARY : command\u7684\u6267\u884c\u662fin the context of a given client\uff0c\u8fd9\u8bf4\u660ecommand\u662f\u548cclient\u5173\u8054\u7684\uff1b activeExpireCycle() handles eviciton(\u9a71\u9010) of keys with a time to live set via the EXPIRE command. freeMemoryIfNeeded() is called when a new write command should be performed but Redis is out of memory according to the maxmemory directive. The global variable redisCommandTable defines all the Redis commands, specifying the name of the command, the function implementing the command, the number of arguments required, and other properties of each command.","title":"server.c"},{"location":"Basic/main/redis-doc-Redis-internals/#networkingc","text":"This file defines all the I/O functions with clients, masters and replicas (which in Redis are just special clients): createClient() allocates and initializes a new client. the addReply*() family of functions are used by commands implementations in order to append data to the client structure , that will be transmitted to the client as a reply for a given command executed. writeToClient() transmits the data pending in the output buffers to the client and is called by the writable event handler sendReplyToClient() . readQueryFromClient() is the readable event handler and accumulates data from read from the client into the query buffer. processInputBuffer() is the entry point in order to parse the client query buffer according to the Redis protocol. Once commands are ready to be processed, it calls processCommand() which is defined inside server.c in order to actually execute the command. freeClient() deallocates, disconnects and removes a client. SUMMARY : \u663e\u7136\uff0c\u8fd9\u4e2a\u6a21\u5757\u6240\u63cf\u8ff0\u7684\u90fd\u662f\u548cnetwork\u76f8\u5173\u7684\u5b9e\u73b0\uff0c\u5b83\u4eec\u5e94\u8be5\u90fd\u662f\u7531event loop\u6765\u8fdb\u884c\u8c03\u7528\uff1b\u5728 server.h \u7684 struct client \u662f\u652f\u6301\u4e0a\u8ff0\u64cd\u4f5c\u7684data structure\uff1b SUMMARY : \u65b0\u7248\u672c\u7684redis\u4e2d\u7684networking\u5df2\u7ecf\u6ca1\u6709\u4f7f\u7528\u8be5\u6587\u4ef6\u4e86\uff0c\u800c\u662f\u4f7f\u7528\u7684 anet.c","title":"networking.c"},{"location":"Basic/main/redis-doc-Redis-internals/#aofc#and#rdbc","text":"As you can guess from the names these files implement the RDB and AOF persistence for Redis. Redis uses a persistence model based on the fork() system call in order to create a thread with the same (shared) memory content of the main Redis thread . This secondary thread dumps the content of the memory on disk. This is used by rdb.c to create the snapshots on disk and by aof.c in order to perform the AOF rewrite when the append only file gets too big. The implementation inside aof.c has additional functions in order to implement an API that allows commands to append new commands into the AOF file as clients execute them. The call() function defined inside server.c is responsible to call the functions that in turn will write the commands into the AOF.","title":"aof.c and rdb.c"},{"location":"Basic/main/redis-doc-Redis-internals/#dbc","text":"Certain Redis commands operate on specific data types, others are general. Examples of generic commands are DEL and EXPIRE . They operate on keys and not on their values specifically. All those generic commands are defined inside db.c . Moreover db.c implements an API in order to perform certain operations on the Redis dataset without directly accessing the internal data structures. The most important functions inside db.c which are used in many commands implementations are the following: lookupKeyRead() and lookupKeyWrite() are used in order to get a pointer to the value associated to a given key, or NULL if the key does not exist. dbAdd() and its higher level counterpart setKey() create a new key in a Redis database. dbDelete() removes a key and its associated value. emptyDb() removes an entire single database or all the databases defined. The rest of the file implements the generic commands exposed to the client.","title":"db.c"},{"location":"Basic/main/redis-doc-Redis-internals/#objectc","text":"The robj structure defining Redis objects was already described. Inside object.c there are all the functions that operate with Redis objects at a basic level, like functions to allocate new objects, handle the reference counting and so forth. Notable functions inside this file: incrRefcount() and decrRefCount() are used in order to increment or decrement an object reference count. When it drops to 0 the object is finally freed. createObject() allocates a new object. There are also specialized functions to allocate string objects having a specific content, like createStringObjectFromLongLong() and similar functions. This file also implements the OBJECT command. \u200b","title":"object.c"},{"location":"Basic/main/redis-doc-Redis-internals/#replicationc","text":"This is one of the most complex files inside Redis, it is recommended to approach it only after getting a bit familiar with the rest of the code base. In this file there is the implementation of both the master and replica role of Redis. One of the most important functions inside this file is replicationFeedSlaves() that writes commands to the clients representing replica instances connected to our master, so that the replicas can get the writes performed by the clients: this way their data set will remain synchronized with the one in the master. This file also implements both the SYNC and PSYNC commands that are used in order to perform the first synchronization between masters and replicas, or to continue the replication after a disconnection.","title":"replication.c"},{"location":"Basic/main/redis-doc-Redis-internals/#other#c#files","text":"t_hash.c , t_list.c , t_set.c , t_string.c and t_zset.c contains the implementation of the Redis data types. They implement both an API to access a given data type, and the client commands implementations for these data types. ae.c implements the Redis event loop, it's a self contained library which is simple to read and understand. sds.c is the Redis string library, check http://github.com/antirez/sds for more information. anet.c is a library to use POSIX networking in a simpler way compared to the raw interface exposed by the kernel. dict.c is an implementation of a non-blocking hash table which rehashes incrementally. scripting.c implements Lua scripting. It is completely self contained from the rest of the Redis implementation and is simple enough to understand if you are familar with the Lua API. cluster.c implements the Redis Cluster. Probably a good read only after being very familiar with the rest of the Redis code base. If you want to read cluster.c make sure to read the Redis Cluster specification .","title":"Other C files"},{"location":"Basic/main/redis-doc-Redis-internals/#anatomy#of#a#redis#command","text":"All the Redis commands are defined in the following way: void foobarCommand ( client * c ) { printf ( \"%s\" , c -> argv [ 1 ] -> ptr ); /* Do something with the argument. */ addReply ( c , shared . ok ); /* Reply something to the client. */ } The command is then referenced inside server.c in the command table: { \"foobar\" , foobarCommand , 2 , \"rtF\" , 0 , NULL , 0 , 0 , 0 , 0 , 0 }, In the above example 2 is the number of arguments the command takes, while \"rtF\" are the command flags , as documented in the command table top comment inside server.c . After the command operates in some way, it returns a reply to the client, usually using addReply() or a similar function defined inside networking.c . There are tons of commands implementations inside the Redis source code that can serve as examples of actual commands implementations. To write a few toy commands can be a good exercise to familiarize with the code base. There are also many other files not described here, but it is useless to cover everything. We want to just help you with the first steps. Eventually you'll find your way inside the Redis code base :-) Enjoy!","title":"Anatomy of a Redis command"},{"location":"Books/","text":"Books Redis in Action Redis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0","title":"Introduction"},{"location":"Books/#books","text":"Redis in Action Redis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0","title":"Books"},{"location":"CSDN-redis%E6%BA%90%E7%A0%81%E6%B5%85%E6%9E%90/","text":"redis\u6e90\u7801\u9605\u8bfb-\u4e00--sds\u7b80\u5355\u52a8\u6001\u5b57\u7b26\u4e32 redis\u6e90\u7801\u9605\u8bfb--\u4e8c-\u94fe\u8868 redis\u6e90\u7801\u6d45\u6790--\u4e8c\u5341\u4e8c.\u76d1\u89c6\u5668\u7684\u5b9e\u73b0","title":"Introduction"},{"location":"Clients/Redis-cli/","text":"redis-cli redis-cli, the Redis command line interface redis-cli is the Redis command line interface, a simple program that allows to send commands to Redis, and read the replies sent by the server, directly from the terminal. It has two main modes: an interactive mode where there is a REPL ( Read Eval Print Loop ) where the user types commands and get replies; and another mode where the command is sent as arguments of redis-cli , executed, and printed on the standard output. NOTE: \u7b2c\u4e8c\u4e2amode\u6240\u6307\u7684\u662f\u5982\u4e0b\u8fd9\u79cd\u4f7f\u7528\u65b9\u5f0f\uff1a redis-cli KEYS \"prefix:*\" | xargs redis-cli DEL In interactive mode, redis-cli has basic line editing capabilities to provide a good typing experience. However redis-cli is not just that. There are options you can use to launch the program in order to put it into special modes, so that redis-cli can definitely do more complex tasks, like simulate a slave and print the replication stream it receives from the master , check the latency of a Redis server and show statistics or even an ASCII-art spectrogram of latency samples and frequencies, and many other things. This guide will cover the different aspects of redis-cli , starting from the simplest and ending with the more advanced ones. SUMMARY : redis-cli\u4e0d\u4ec5\u4ec5\u53ea\u6709\u4e0a\u8ff0\u7684mode\uff0c\u8fd8\u6709\u5176\u4ed6\u7684mode\uff0c\u53c2\u89c1 Special modes of operation If you are going to use Redis extensively, or if you already do, chances are you happen to use redis-cli a lot. Spending some time to familiarize with it is likely a very good idea, you'll see that you'll work more effectively with Redis once you know all the tricks of its command line interface. Command line usage Host, port, password and database Getting input from other programs Continuously run the same command Mass insertion of data using redis-cli CSV output Running Lua scripts Interactive mode Handling connections and reconnections Editing, history and completion Running the same command N times Showing help about Redis commands Special modes of operation So far we saw two main modes of redis-cli . Command line execution of Redis commands. Interactive \"REPL-like\" usage. However the CLI performs other auxiliary tasks related to Redis that are explained in the next sections: Monitoring tool to show continuous stats about a Redis server. Scanning a Redis database for very large keys. Key space scanner with pattern matching. Acting as a Pub/Sub client to subscribe to channels. Monitoring the commands executed into a Redis instance. Checking the latency of a Redis server in different ways. Checking the scheduler latency of the local computer. Transferring RDB backups from a remote Redis server locally. Acting as a Redis slave for showing what a slave receives. Simulating LRU workloads for showing stats about keys hits. A client for the Lua debugger. \u5b9e\u73b0\u5206\u6790 \u201cinteractive mode \u201d\u7684\u5165\u53e3\u51fd\u6570\u662f\uff1a repl \"Command line execution of Redis commands\"\u7684 \"Running Lua scripts\"\u7684\u5165\u53e3\u51fd\u6570\u662f: evalMode redis\u6240\u652f\u6301\u7684\u6240\u6709\u7684command\u90fd\u5728\"help.h\"\u4e2d\u5b9a\u4e49. \u5982\u4f55\u7ec4\u7ec7\u6240\u6709\u7684command\uff1f \u5982\u4f55\u7ec4\u7ec7\u6240\u6709\u7684command\u548ccommand\u5bf9\u5e94\u7684\u6267\u884c\u51fd\u6570\uff1f","title":"Introduction"},{"location":"Clients/Redis-cli/#redis-cli","text":"","title":"redis-cli"},{"location":"Clients/Redis-cli/#redis-cli#the#redis#command#line#interface","text":"redis-cli is the Redis command line interface, a simple program that allows to send commands to Redis, and read the replies sent by the server, directly from the terminal. It has two main modes: an interactive mode where there is a REPL ( Read Eval Print Loop ) where the user types commands and get replies; and another mode where the command is sent as arguments of redis-cli , executed, and printed on the standard output. NOTE: \u7b2c\u4e8c\u4e2amode\u6240\u6307\u7684\u662f\u5982\u4e0b\u8fd9\u79cd\u4f7f\u7528\u65b9\u5f0f\uff1a redis-cli KEYS \"prefix:*\" | xargs redis-cli DEL In interactive mode, redis-cli has basic line editing capabilities to provide a good typing experience. However redis-cli is not just that. There are options you can use to launch the program in order to put it into special modes, so that redis-cli can definitely do more complex tasks, like simulate a slave and print the replication stream it receives from the master , check the latency of a Redis server and show statistics or even an ASCII-art spectrogram of latency samples and frequencies, and many other things. This guide will cover the different aspects of redis-cli , starting from the simplest and ending with the more advanced ones. SUMMARY : redis-cli\u4e0d\u4ec5\u4ec5\u53ea\u6709\u4e0a\u8ff0\u7684mode\uff0c\u8fd8\u6709\u5176\u4ed6\u7684mode\uff0c\u53c2\u89c1 Special modes of operation If you are going to use Redis extensively, or if you already do, chances are you happen to use redis-cli a lot. Spending some time to familiarize with it is likely a very good idea, you'll see that you'll work more effectively with Redis once you know all the tricks of its command line interface.","title":"redis-cli, the Redis command line interface"},{"location":"Clients/Redis-cli/#command#line#usage","text":"","title":"Command line usage"},{"location":"Clients/Redis-cli/#host#port#password#and#database","text":"","title":"Host, port, password and database"},{"location":"Clients/Redis-cli/#getting#input#from#other#programs","text":"","title":"Getting input from other programs"},{"location":"Clients/Redis-cli/#continuously#run#the#same#command","text":"","title":"Continuously run the same command"},{"location":"Clients/Redis-cli/#mass#insertion#of#data#using#redis-cli","text":"","title":"Mass insertion of data using redis-cli"},{"location":"Clients/Redis-cli/#csv#output","text":"","title":"CSV output"},{"location":"Clients/Redis-cli/#running#lua#scripts","text":"","title":"Running Lua scripts"},{"location":"Clients/Redis-cli/#interactive#mode","text":"","title":"Interactive mode"},{"location":"Clients/Redis-cli/#handling#connections#and#reconnections","text":"","title":"Handling connections and reconnections"},{"location":"Clients/Redis-cli/#editing#history#and#completion","text":"","title":"Editing, history and completion"},{"location":"Clients/Redis-cli/#running#the#same#command#n#times","text":"","title":"Running the same command N times"},{"location":"Clients/Redis-cli/#showing#help#about#redis#commands","text":"","title":"Showing help about Redis commands"},{"location":"Clients/Redis-cli/#special#modes#of#operation","text":"So far we saw two main modes of redis-cli . Command line execution of Redis commands. Interactive \"REPL-like\" usage. However the CLI performs other auxiliary tasks related to Redis that are explained in the next sections: Monitoring tool to show continuous stats about a Redis server. Scanning a Redis database for very large keys. Key space scanner with pattern matching. Acting as a Pub/Sub client to subscribe to channels. Monitoring the commands executed into a Redis instance. Checking the latency of a Redis server in different ways. Checking the scheduler latency of the local computer. Transferring RDB backups from a remote Redis server locally. Acting as a Redis slave for showing what a slave receives. Simulating LRU workloads for showing stats about keys hits. A client for the Lua debugger.","title":"Special modes of operation"},{"location":"Clients/Redis-cli/#_1","text":"\u201cinteractive mode \u201d\u7684\u5165\u53e3\u51fd\u6570\u662f\uff1a repl \"Command line execution of Redis commands\"\u7684 \"Running Lua scripts\"\u7684\u5165\u53e3\u51fd\u6570\u662f: evalMode redis\u6240\u652f\u6301\u7684\u6240\u6709\u7684command\u90fd\u5728\"help.h\"\u4e2d\u5b9a\u4e49.","title":"\u5b9e\u73b0\u5206\u6790"},{"location":"Clients/Redis-cli/#command","text":"\u5982\u4f55\u7ec4\u7ec7\u6240\u6709\u7684command\u548ccommand\u5bf9\u5e94\u7684\u6267\u884c\u51fd\u6570\uff1f","title":"\u5982\u4f55\u7ec4\u7ec7\u6240\u6709\u7684command\uff1f"},{"location":"Clients/Redis-cli/linenoise/","text":"linenoise redis-cli\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8elinenoise\uff0c\u9664\u6b64\u4e4b\u5916\uff0c\u4f7f\u7528linenoise\u6211\u4eec\u53ef\u4ee5\u6784\u5efa\u81ea\u5df1\u7684server\u7684command line interface\u3002 linenoise A small self-contained alternative to readline and libedit. The API The library returns a buffer with the line composed by the user, or NULL on end of file or when there is an out of memory condition. NOTE: \u8fd9\u6bb5\u8bdd\u600e\u4e48\u7406\u89e3\uff1f","title":"Introduction"},{"location":"Clients/Redis-cli/linenoise/#linenoise","text":"redis-cli\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8elinenoise\uff0c\u9664\u6b64\u4e4b\u5916\uff0c\u4f7f\u7528linenoise\u6211\u4eec\u53ef\u4ee5\u6784\u5efa\u81ea\u5df1\u7684server\u7684command line interface\u3002","title":"linenoise"},{"location":"Clients/Redis-cli/linenoise/#linenoise_1","text":"A small self-contained alternative to readline and libedit.","title":"linenoise"},{"location":"Clients/Redis-cli/linenoise/#the#api","text":"The library returns a buffer with the line composed by the user, or NULL on end of file or when there is an out of memory condition. NOTE: \u8fd9\u6bb5\u8bdd\u600e\u4e48\u7406\u89e3\uff1f","title":"The API"},{"location":"Clients/library-Redis-plus-plus/","text":"redis-plus-plus \u5bf9 hiredis \u7684wrapper\uff0c\u5199\u7684\u975e\u5e38\u597d\u3002 Connection class Connection If the connection is broken, Redis reconnects to Redis server automatically. \u5982\u4f55\u5b9e\u73b0\u7684\u81ea\u52a8\u91cd\u8fde\uff1f Connection Failure Exception See errors.h for details Redis Sentinel NOTE: \u901a\u8fc7sentinel\u53ef\u4ee5\u83b7\u53d6master\u3001slave\u7684\u5730\u5740\uff0c\u7136\u540e\u53ef\u4ee5\u6307\u5b9a Redis \u8fde\u63a5\u5230\u5b83\u4eec\u3002 \u7c7b\u56fe `class Redis` has a `ConnectionPool _pool`","title":"Introduction"},{"location":"Clients/library-Redis-plus-plus/#redis-plus-plus","text":"\u5bf9 hiredis \u7684wrapper\uff0c\u5199\u7684\u975e\u5e38\u597d\u3002","title":"redis-plus-plus"},{"location":"Clients/library-Redis-plus-plus/#connection","text":"","title":"Connection"},{"location":"Clients/library-Redis-plus-plus/#class#connection","text":"If the connection is broken, Redis reconnects to Redis server automatically.","title":"class Connection"},{"location":"Clients/library-Redis-plus-plus/#_1","text":"","title":"\u5982\u4f55\u5b9e\u73b0\u7684\u81ea\u52a8\u91cd\u8fde\uff1f"},{"location":"Clients/library-Redis-plus-plus/#connection#failure","text":"","title":"Connection Failure"},{"location":"Clients/library-Redis-plus-plus/#exception","text":"See errors.h for details","title":"Exception"},{"location":"Clients/library-Redis-plus-plus/#redis#sentinel","text":"NOTE: \u901a\u8fc7sentinel\u53ef\u4ee5\u83b7\u53d6master\u3001slave\u7684\u5730\u5740\uff0c\u7136\u540e\u53ef\u4ee5\u6307\u5b9a Redis \u8fde\u63a5\u5230\u5b83\u4eec\u3002","title":"Redis Sentinel"},{"location":"Clients/library-Redis-plus-plus/#_2","text":"`class Redis` has a `ConnectionPool _pool`","title":"\u7c7b\u56fe"},{"location":"Command/","text":"Commands client\u901a\u8fc7command\u6765\u64cd\u4f5credis server\uff0c\u672c\u6587\u5bf9redis\u7684command\u7684\u5b9e\u73b0\u8fdb\u884c\u5206\u6790\u3002 prototype of redis command function redis\u4e2d\u6240\u6709\u7528\u4e8e\u5904\u7406command\u7684\u51fd\u6570\u7684prototype\u5982\u4e0b\uff1a void Command(client *c); \u56e0\u4e3a\u5728\u5904\u7406command\u4e4b\u524d\uff0c readQueryFromClient \u51fd\u6570\u5df2\u7ecf\u5c06\u7528\u6237\u53d1\u9001\u8fc7\u6765\u7684command\u4fe1\u606f\u5199\u5165\u5230\u4e86client\u7684 argv \u6210\u5458\u53d8\u91cf\u4e2d\u3002 redis cluster command void clusterCommand(client *c) void dumpCommand(client *c) void restoreCommand(client *c) redis replication void syncCommand(client *c) void replconfCommand(client *c) redis config void configCommand(client *c)","title":"Introduction"},{"location":"Command/#commands","text":"client\u901a\u8fc7command\u6765\u64cd\u4f5credis server\uff0c\u672c\u6587\u5bf9redis\u7684command\u7684\u5b9e\u73b0\u8fdb\u884c\u5206\u6790\u3002","title":"Commands"},{"location":"Command/#prototype#of#redis#command#function","text":"redis\u4e2d\u6240\u6709\u7528\u4e8e\u5904\u7406command\u7684\u51fd\u6570\u7684prototype\u5982\u4e0b\uff1a void Command(client *c); \u56e0\u4e3a\u5728\u5904\u7406command\u4e4b\u524d\uff0c readQueryFromClient \u51fd\u6570\u5df2\u7ecf\u5c06\u7528\u6237\u53d1\u9001\u8fc7\u6765\u7684command\u4fe1\u606f\u5199\u5165\u5230\u4e86client\u7684 argv \u6210\u5458\u53d8\u91cf\u4e2d\u3002","title":"prototype of redis command function"},{"location":"Command/#redis#cluster#command","text":"void clusterCommand(client *c) void dumpCommand(client *c) void restoreCommand(client *c)","title":"redis cluster command"},{"location":"Command/#redis#replication","text":"void syncCommand(client *c) void replconfCommand(client *c)","title":"redis replication"},{"location":"Command/#redis#config","text":"void configCommand(client *c)","title":"redis config"},{"location":"Distributed-computing/","text":"Distributed computing: sentinel\u3001cluster Redis sentinel\u3001Redis cluster\u90fd\u5c5e\u4e8edistributed system\uff0c\u5176\u4e2d\u4f7f\u7528\u4e86\u5f88\u591adistributed computing\u4e2d\u7684\u6280\u672f\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06Redis\u4f5c\u4e3a\u5b66\u4e60distributed computing\u7684\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u6848\u4f8b\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528distributedcomputing\u7684\u7406\u8bba\u6765\u5bf9\u5b83\u8fdb\u884c\u5206\u6790\uff1b Redis sentinel\u3001Redis cluster Redis sentinel\u3001Redis cluster\u90fd\u5c5e\u4e8edistributed system\u5b58\u5728\u7740\u8bf8\u591a\u5171\u540c\u4e4b\u5904\uff0c\u53ef\u4ee5\u8ba4\u4e3a: Redis cluster\u96c6\u6210\u4e86Redis sentinel\u7684\u529f\u80fd\u3002 Redis sentinel Redis cluster Consensus protocol raft raft current epoch\u3001config epoch current epoch\u3001config epoch \u4e0b\u7ebf \u4e3b\u89c2\u4e0b\u7ebf\u3001\u5ba2\u89c2\u4e0b\u7ebf \u7591\u4f3c\u4e0b\u7ebf\u3001\u4e0b\u7ebf system/topology state sentinelState clusterState auto discover HELLO channel gossip stackoverflow Redis sentinel vs clustering fnordig Redis Sentinel & Redis Cluster - what? Redis Sentinel It will monitor your master & slave instances, notify you about changed behaviour, handle automatic failover in case a master is down and act as a configuration provider , so your clients can find the current master instance. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u4ecb\u7ecd\u4e86\"configuration provider\"\u7684\u529f\u80fd Redis Cluster Redis Cluster is a data sharding solution with automatic management , handling failover and replication . See also amazon Amazon ElastiCache for Redis \u5176\u4e2d\u603b\u7ed3\u4e86Redis sentinel\u3001Redis cluster CAP Redis\u7684\u4e24\u79cd\u90e8\u7f72\u65b9\u5f0f(sentinel\u3001cluster)\u90fd\u5c5e\u4e8edistributed system\uff0c\u90fd\u6d89\u53caCAP: consistency\u3001availability\u3001partition\uff1b","title":"Introduction"},{"location":"Distributed-computing/#distributed#computing#sentinelcluster","text":"Redis sentinel\u3001Redis cluster\u90fd\u5c5e\u4e8edistributed system\uff0c\u5176\u4e2d\u4f7f\u7528\u4e86\u5f88\u591adistributed computing\u4e2d\u7684\u6280\u672f\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06Redis\u4f5c\u4e3a\u5b66\u4e60distributed computing\u7684\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u6848\u4f8b\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528distributedcomputing\u7684\u7406\u8bba\u6765\u5bf9\u5b83\u8fdb\u884c\u5206\u6790\uff1b","title":"Distributed computing: sentinel\u3001cluster"},{"location":"Distributed-computing/#redis#sentinelredis#cluster","text":"Redis sentinel\u3001Redis cluster\u90fd\u5c5e\u4e8edistributed system\u5b58\u5728\u7740\u8bf8\u591a\u5171\u540c\u4e4b\u5904\uff0c\u53ef\u4ee5\u8ba4\u4e3a: Redis cluster\u96c6\u6210\u4e86Redis sentinel\u7684\u529f\u80fd\u3002 Redis sentinel Redis cluster Consensus protocol raft raft current epoch\u3001config epoch current epoch\u3001config epoch \u4e0b\u7ebf \u4e3b\u89c2\u4e0b\u7ebf\u3001\u5ba2\u89c2\u4e0b\u7ebf \u7591\u4f3c\u4e0b\u7ebf\u3001\u4e0b\u7ebf system/topology state sentinelState clusterState auto discover HELLO channel gossip","title":"Redis sentinel\u3001Redis cluster"},{"location":"Distributed-computing/#stackoverflow#redis#sentinel#vs#clustering","text":"","title":"stackoverflow Redis sentinel vs clustering"},{"location":"Distributed-computing/#fnordig#redis#sentinel#redis#cluster#-#what","text":"","title":"fnordig Redis Sentinel &amp; Redis Cluster - what?"},{"location":"Distributed-computing/#redis#sentinel","text":"It will monitor your master & slave instances, notify you about changed behaviour, handle automatic failover in case a master is down and act as a configuration provider , so your clients can find the current master instance. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u4ecb\u7ecd\u4e86\"configuration provider\"\u7684\u529f\u80fd","title":"Redis Sentinel"},{"location":"Distributed-computing/#redis#cluster","text":"Redis Cluster is a data sharding solution with automatic management , handling failover and replication .","title":"Redis Cluster"},{"location":"Distributed-computing/#see#also","text":"amazon Amazon ElastiCache for Redis \u5176\u4e2d\u603b\u7ed3\u4e86Redis sentinel\u3001Redis cluster","title":"See also"},{"location":"Distributed-computing/#cap","text":"Redis\u7684\u4e24\u79cd\u90e8\u7f72\u65b9\u5f0f(sentinel\u3001cluster)\u90fd\u5c5e\u4e8edistributed system\uff0c\u90fd\u6d89\u53caCAP: consistency\u3001availability\u3001partition\uff1b","title":"CAP"},{"location":"Distributed-computing/Consensus-raft/","text":"Redis Consensus protocol: Raft \u672c\u7ae0\u5bf9Redis\u7684\"consensus protocol\"\u8fdb\u884c\u8ba8\u8bba\u3002 \u5728\u9605\u8bfb Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb \u7684\u65f6\u5019\uff0c\u5176\u4e2d\u63d0\u53ca\uff1a \u7406\u89e3Redis\u96c6\u7fa4\u4e2d\u7684\u6545\u969c\u8f6c\u79fb\uff0c\u5fc5\u987b\u8981\u7406\u89e3\u7eaa\u5143(epoch)\u5728\u5206\u5e03\u5f0fRedis\u96c6\u7fa4\u4e2d\u7684\u4f5c\u7528\uff0cRedis\u96c6\u7fa4\u4f7f\u7528RAFT\u7b97\u6cd5\u4e2d\u7c7b\u4f3cterm\u7684\u6982\u5ff5\uff0c\u5728Redis\u96c6\u7fa4\u4e2d\u8fd9\u88ab\u79f0\u4e4b\u4e3a\u7eaa\u5143(epoch)\u3002\u7eaa\u5143\u7684\u6982\u5ff5\u5728\u4ecb\u7ecd\u54e8\u5175\u65f6\u5df2\u7ecf\u4ecb\u7ecd\u8fc7\u4e86\uff0c\u5728Redis\u96c6\u7fa4\u4e2d\uff0c\u7eaa\u5143\u7684\u6982\u5ff5\u548c\u4f5c\u7528\u4e0e\u54e8\u5175\u4e2d\u7684\u7eaa\u5143\u7c7b\u4f3c\u3002Redis\u96c6\u7fa4\u4e2d\u7684\u7eaa\u5143\u4e3b\u8981\u662f\u4e24\u79cd\uff1acurrentEpoch\u548cconfigEpoch\u3002 \u4e0a\u9762\u63d0\u5230\u4e86redis\u54e8\u5175\uff0credis\u54e8\u5175\u7684\u6587\u6863\u5728 Redis Sentinel Documentation \u4e2d\u4ecb\u7ecd\uff0c\u5176\u4e2d\u7684\u786e\u6709epoch\u7684\u6982\u5ff5\uff0c\u5e76\u4e14redis\u54e8\u5175\u6240\u5b9e\u73b0\u7684high availability\uff0c\u5176\u5b9e\u662f\u4f9d\u8d56\u4e8e\u6545\u969c\u8f6c\u79fb\u529f\u80fd\u7684\uff0c\u800credis cluster\u4e5f\u80fd\u591f\u63d0\u4f9b\u6545\u969c\u8f6c\u79fb(failover)\u529f\u80fd\uff0c\u5e76\u4e14\u5b83\u7684\u5b9e\u73b0\u4e5f\u501f\u9274\u4e86redis sentinel\u7684\u5b9e\u73b0\uff0c\u8fd9\u8bf4\u660eredis cluster\u96c6\u6210\u4e86redis sentinel\u7684\u90e8\u5206\u529f\u80fd\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e0a\u4e5f\u662f\u501f\u9274\u7684redis sentinel\uff1b \u9605\u8bfb gqtc \u7684 redis\u7cfb\u5217\u535a\u5ba2 \u4e2d\uff0c\u5173\u4e8esentinel\u7684\u6587\u7ae0\uff0c\u6211\u80fd\u591f\u4ece\u4e2d\u770b\u5230\u4e00\u4e9bredis cluster\u5b9e\u73b0\u4e2d\u540c\u6837\u4f7f\u7528\u7684\u6280\u672f\uff1a sentinel cluster Redis\u6e90\u7801\u89e3\u6790\uff1a21sentinel(\u4e8c)\u5b9a\u671f\u53d1\u9001\u6d88\u606f\u3001\u68c0\u6d4b\u4e3b\u89c2\u4e0b\u7ebf Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b Redis\u6e90\u7801\u89e3\u6790\uff1a22sentinel(\u4e09)\u5ba2\u89c2\u4e0b\u7ebf\u4ee5\u53ca\u6545\u969c\u8f6c\u79fb\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9 Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b \u5176\u5b9e\u73b0\u5728\u60f3\u60f3\u672c\u8d28\u4e0aredis sentinel\u548credis cluster\u90fd\u662fdistribution system\uff0c\u5e76\u4e14\u5b83\u4eec\u9700\u8981\u5b9e\u73b0\u7684\u529f\u80fd\u662f\u5b58\u5728\u4e00\u5b9a\u7684\u91cd\u590d\u7684\uff0c\u6240\u4ee5\u5b9e\u73b0\u4e0a\u7684\u91cd\u590d\u662f\u975e\u5e38\u6b63\u5e38\u7684\uff1b TODO redis\u96c6\u7fa4\u5b9e\u73b0(\u4e94) sentinel\u7684\u67b6\u6784\u4e0eraft\u534f\u8bae redis\u96c6\u7fa4\u5b9e\u73b0(\u4e94) \u96c6\u7fa4\u4e00\u81f4\u6027\u7684\u4fdd\u8bc1-raft\u534f\u8bae \u628a raft \u505a\u8fdb redis \u7684\u601d\u8003 Redis\u4e2d\u7b97\u6cd5\u4e4b\u2014\u2014Raft\u7b97\u6cd5 \u8fd9\u7bc7\u6587\u7ae0\u6536\u5f55\u4e86","title":"Introduction"},{"location":"Distributed-computing/Consensus-raft/#redis#consensus#protocol#raft","text":"\u672c\u7ae0\u5bf9Redis\u7684\"consensus protocol\"\u8fdb\u884c\u8ba8\u8bba\u3002 \u5728\u9605\u8bfb Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb \u7684\u65f6\u5019\uff0c\u5176\u4e2d\u63d0\u53ca\uff1a \u7406\u89e3Redis\u96c6\u7fa4\u4e2d\u7684\u6545\u969c\u8f6c\u79fb\uff0c\u5fc5\u987b\u8981\u7406\u89e3\u7eaa\u5143(epoch)\u5728\u5206\u5e03\u5f0fRedis\u96c6\u7fa4\u4e2d\u7684\u4f5c\u7528\uff0cRedis\u96c6\u7fa4\u4f7f\u7528RAFT\u7b97\u6cd5\u4e2d\u7c7b\u4f3cterm\u7684\u6982\u5ff5\uff0c\u5728Redis\u96c6\u7fa4\u4e2d\u8fd9\u88ab\u79f0\u4e4b\u4e3a\u7eaa\u5143(epoch)\u3002\u7eaa\u5143\u7684\u6982\u5ff5\u5728\u4ecb\u7ecd\u54e8\u5175\u65f6\u5df2\u7ecf\u4ecb\u7ecd\u8fc7\u4e86\uff0c\u5728Redis\u96c6\u7fa4\u4e2d\uff0c\u7eaa\u5143\u7684\u6982\u5ff5\u548c\u4f5c\u7528\u4e0e\u54e8\u5175\u4e2d\u7684\u7eaa\u5143\u7c7b\u4f3c\u3002Redis\u96c6\u7fa4\u4e2d\u7684\u7eaa\u5143\u4e3b\u8981\u662f\u4e24\u79cd\uff1acurrentEpoch\u548cconfigEpoch\u3002 \u4e0a\u9762\u63d0\u5230\u4e86redis\u54e8\u5175\uff0credis\u54e8\u5175\u7684\u6587\u6863\u5728 Redis Sentinel Documentation \u4e2d\u4ecb\u7ecd\uff0c\u5176\u4e2d\u7684\u786e\u6709epoch\u7684\u6982\u5ff5\uff0c\u5e76\u4e14redis\u54e8\u5175\u6240\u5b9e\u73b0\u7684high availability\uff0c\u5176\u5b9e\u662f\u4f9d\u8d56\u4e8e\u6545\u969c\u8f6c\u79fb\u529f\u80fd\u7684\uff0c\u800credis cluster\u4e5f\u80fd\u591f\u63d0\u4f9b\u6545\u969c\u8f6c\u79fb(failover)\u529f\u80fd\uff0c\u5e76\u4e14\u5b83\u7684\u5b9e\u73b0\u4e5f\u501f\u9274\u4e86redis sentinel\u7684\u5b9e\u73b0\uff0c\u8fd9\u8bf4\u660eredis cluster\u96c6\u6210\u4e86redis sentinel\u7684\u90e8\u5206\u529f\u80fd\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e0a\u4e5f\u662f\u501f\u9274\u7684redis sentinel\uff1b \u9605\u8bfb gqtc \u7684 redis\u7cfb\u5217\u535a\u5ba2 \u4e2d\uff0c\u5173\u4e8esentinel\u7684\u6587\u7ae0\uff0c\u6211\u80fd\u591f\u4ece\u4e2d\u770b\u5230\u4e00\u4e9bredis cluster\u5b9e\u73b0\u4e2d\u540c\u6837\u4f7f\u7528\u7684\u6280\u672f\uff1a sentinel cluster Redis\u6e90\u7801\u89e3\u6790\uff1a21sentinel(\u4e8c)\u5b9a\u671f\u53d1\u9001\u6d88\u606f\u3001\u68c0\u6d4b\u4e3b\u89c2\u4e0b\u7ebf Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b Redis\u6e90\u7801\u89e3\u6790\uff1a22sentinel(\u4e09)\u5ba2\u89c2\u4e0b\u7ebf\u4ee5\u53ca\u6545\u969c\u8f6c\u79fb\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9 Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b \u5176\u5b9e\u73b0\u5728\u60f3\u60f3\u672c\u8d28\u4e0aredis sentinel\u548credis cluster\u90fd\u662fdistribution system\uff0c\u5e76\u4e14\u5b83\u4eec\u9700\u8981\u5b9e\u73b0\u7684\u529f\u80fd\u662f\u5b58\u5728\u4e00\u5b9a\u7684\u91cd\u590d\u7684\uff0c\u6240\u4ee5\u5b9e\u73b0\u4e0a\u7684\u91cd\u590d\u662f\u975e\u5e38\u6b63\u5e38\u7684\uff1b","title":"Redis Consensus protocol: Raft"},{"location":"Distributed-computing/Consensus-raft/#todo","text":"redis\u96c6\u7fa4\u5b9e\u73b0(\u4e94) sentinel\u7684\u67b6\u6784\u4e0eraft\u534f\u8bae redis\u96c6\u7fa4\u5b9e\u73b0(\u4e94) \u96c6\u7fa4\u4e00\u81f4\u6027\u7684\u4fdd\u8bc1-raft\u534f\u8bae \u628a raft \u505a\u8fdb redis \u7684\u601d\u8003 Redis\u4e2d\u7b97\u6cd5\u4e4b\u2014\u2014Raft\u7b97\u6cd5 \u8fd9\u7bc7\u6587\u7ae0\u6536\u5f55\u4e86","title":"TODO"},{"location":"Distributed-computing/Consensus-raft/zhihu-Redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E7%BA%AA%E5%85%83-epoch/","text":"zhihu Redis \u96c6\u7fa4\u4e2d\u7684\u7eaa\u5143(epoch) \u7eaa\u5143\uff08epoch\uff09 Redis Cluster \u4f7f\u7528\u4e86\u7c7b\u4f3c\u4e8e Raft \u7b97\u6cd5 term \uff08\u4efb\u671f\uff09\u7684\u6982\u5ff5\u79f0\u4e3a epoch \uff08\u7eaa\u5143\uff09\uff0c\u7528\u6765\u7ed9**\u4e8b\u4ef6**\u589e\u52a0**\u7248\u672c\u53f7**\u3002Redis \u96c6\u7fa4\u4e2d\u7684\u7eaa\u5143\u4e3b\u8981\u662f\u4e24\u79cd\uff1a currentEpoch \u548c configEpoch \u3002 currentEpoch \u8fd9\u662f\u4e00\u4e2a**\u96c6\u7fa4\u72b6\u6001**\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u53ef\u4ee5\u5f53\u505a\u8bb0\u5f55**\u96c6\u7fa4\u72b6\u6001**\u53d8\u66f4\u7684\u9012\u589e\u7248\u672c\u53f7\u3002\u6bcf\u4e2a\u96c6\u7fa4\u8282\u70b9\uff0c\u90fd\u4f1a\u901a\u8fc7 server.cluster->currentEpoch \u8bb0\u5f55\u5f53\u524d\u7684 currentEpoch \u3002 \u96c6\u7fa4\u8282\u70b9\u521b\u5efa\u65f6\uff0c\u4e0d\u7ba1\u662f master \u8fd8\u662f slave \uff0c\u90fd\u7f6e currentEpoch \u4e3a 0\u3002\u5f53\u524d\u8282\u70b9\u63a5\u6536\u5230\u6765\u81ea\u5176\u4ed6\u8282\u70b9\u7684\u5305\u65f6\uff0c\u5982\u679c\u53d1\u9001\u8005\u7684 currentEpoch \uff08\u6d88\u606f\u5934\u90e8\u4f1a\u5305\u542b\u53d1\u9001\u8005\u7684 currentEpoch \uff09\u5927\u4e8e\u5f53\u524d\u8282\u70b9\u7684***currentEpoch***\uff0c\u90a3\u4e48\u5f53\u524d\u8282\u70b9\u4f1a\u66f4\u65b0 currentEpoch \u4e3a\u53d1\u9001\u8005\u7684 currentEpoch \u3002\u56e0\u6b64\uff0c\u96c6\u7fa4\u4e2d\u6240\u6709\u8282\u70b9\u7684 currentEpoch \u6700\u7ec8\u4f1a\u8fbe\u6210\u4e00\u81f4\uff0c\u76f8\u5f53\u4e8e\u5bf9**\u96c6\u7fa4\u72b6\u6001**\u7684\u8ba4\u77e5\u8fbe\u6210\u4e86\u4e00\u81f4\u3002 currentEpoch \u4f5c\u7528 currentEpoch \u4f5c\u7528\u5728\u4e8e\uff0c\u5f53\u96c6\u7fa4\u7684\u72b6\u6001\u53d1\u751f\u6539\u53d8\uff0c\u67d0\u4e2a\u8282\u70b9\u4e3a\u4e86\u6267\u884c\u4e00\u4e9b\u52a8\u4f5c\u9700\u8981\u5bfb\u6c42\u5176\u4ed6\u8282\u70b9\u7684\u540c\u610f\u65f6\uff0c\u5c31\u4f1a\u589e\u52a0 currentEpoch \u7684\u503c\u3002\u76ee\u524d currentEpoch \u53ea\u7528\u4e8e slave \u7684\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\uff0c\u8fd9\u5c31\u8ddf**\u54e8\u5175**\u4e2d\u7684 sentinel.current_epoch \u4f5c\u7528\u662f\u4e00\u6a21\u4e00\u6837\u7684\u3002\u5f53 slave A \u53d1\u73b0\u5176\u6240\u5c5e\u7684 master***\u4e0b\u7ebf\u65f6\uff0c\u5c31\u4f1a\u8bd5\u56fe\u53d1\u8d77\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\u3002\u9996\u5148\u5c31\u662f\u589e\u52a0 currentEpoch \u7684\u503c\uff0c\u8fd9\u4e2a\u589e\u52a0\u540e\u7684 currentEpoch \u662f\u6240\u6709\u96c6\u7fa4\u8282\u70b9\u4e2d\u6700\u5927\u7684\u3002\u7136\u540e**slave A** \u5411\u6240\u6709\u8282\u70b9\u53d1\u8d77\u62c9\u7968\u8bf7\u6c42\uff0c\u8bf7\u6c42\u5176\u4ed6 ***master \u6295\u7968\u7ed9\u81ea\u5df1\uff0c\u4f7f\u81ea\u5df1\u80fd\u6210\u4e3a\u65b0\u7684 master \u3002\u5176\u4ed6\u8282\u70b9\u6536\u5230\u5305\u540e\uff0c\u53d1\u73b0\u53d1\u9001\u8005\u7684 currentEpoch \u6bd4\u81ea\u5df1\u7684 currentEpoch \u5927\uff0c\u5c31\u4f1a\u66f4\u65b0\u81ea\u5df1\u7684 currentEpoch \uff0c\u5e76\u5728\u5c1a\u672a\u6295\u7968\u7684\u60c5\u51b5\u4e0b\uff0c\u6295\u7968\u7ed9 slave A \uff0c\u8868\u793a\u540c\u610f\u4f7f\u5176\u6210\u4e3a\u65b0\u7684 master \u3002 configEpoch \u8fd9\u662f\u4e00\u4e2a\u96c6\u7fa4**\u8282\u70b9\u914d\u7f6e**\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u8282\u70b9\u90fd\u6709\u81ea\u5df1\u72ec\u4e00\u65e0\u4e8c\u7684 configepoch\u3002\u6240\u8c13\u7684**\u8282\u70b9\u914d\u7f6e**\uff0c\u5b9e\u9645\u4e0a\u662f\u6307\u8282\u70b9\u6240\u8d1f\u8d23\u7684\u69fd\u4f4d\u4fe1\u606f\u3002 \u6bcf\u4e00\u4e2a master \u5728\u5411\u5176\u4ed6\u8282\u70b9\u53d1\u9001\u5305\u65f6\uff0c\u90fd\u4f1a\u9644\u5e26\u5176 configEpoch \u4fe1\u606f\uff0c\u4ee5\u53ca\u4e00\u4efd\u8868\u793a\u5b83\u6240\u8d1f\u8d23\u7684 slots \u4fe1\u606f\u3002\u800c slave \u5411\u5176\u4ed6\u8282\u70b9\u53d1\u9001\u5305\u65f6\uff0c\u5176\u5305\u4e2d\u7684 configEpoch \u548c\u8d1f\u8d23\u69fd\u4f4d\u4fe1\u606f\uff0c\u662f\u5176 master \u7684 configEpoch \u548c\u8d1f\u8d23\u7684 slot \u4fe1\u606f\u3002\u8282\u70b9\u6536\u5230\u5305\u4e4b\u540e\uff0c\u5c31\u4f1a\u6839\u636e\u5305\u4e2d\u7684 configEpoch***\u548c\u8d1f\u8d23\u7684 ***slots \u4fe1\u606f\uff0c\u8bb0\u5f55\u5230\u76f8\u5e94\u8282\u70b9\u5c5e\u6027\u4e2d\u3002 configEpoch \u4f5c\u7528 configEpoch \u4e3b\u8981\u7528\u4e8e\u89e3\u51b3\u4e0d\u540c\u7684\u8282\u70b9\u7684\u914d\u7f6e\u53d1\u751f\u51b2\u7a81\u7684\u60c5\u51b5\u3002\u4e3e\u4e2a\u4f8b\u5b50\u5c31\u660e\u767d\u4e86\uff1a\u8282\u70b9A \u5ba3\u79f0\u8d1f\u8d23 slot 1 \uff0c\u5176\u5411\u5916\u53d1\u9001\u7684\u5305\u4e2d\uff0c\u5305\u542b\u4e86\u81ea\u5df1\u7684 configEpoch \u548c\u8d1f\u8d23\u7684 slots \u4fe1\u606f\u3002\u8282\u70b9 C \u6536\u5230 A \u53d1\u6765\u7684\u5305\u540e\uff0c\u53d1\u73b0\u81ea\u5df1\u5f53\u524d\u6ca1\u6709\u8bb0\u5f55 slot 1 \u7684\u8d1f\u8d23\u8282\u70b9\uff08\u4e5f\u5c31\u662f server.cluster->slots[1] \u4e3a NULL\uff09\uff0c\u5c31\u4f1a\u5c06 A \u7f6e\u4e3a slot 1 \u7684\u8d1f\u8d23\u8282\u70b9\uff08server.cluster->slots[1] = A\uff09\uff0c\u5e76\u8bb0\u5f55\u8282\u70b9 A \u7684 configEpoch \u3002\u540e\u6765\uff0c\u8282\u70b9 C \u53c8\u6536\u5230\u4e86 B \u53d1\u6765\u7684\u5305\uff0c\u5b83\u4e5f\u5ba3\u79f0\u8d1f\u8d23 slot 1 \uff0c\u6b64\u65f6\uff0c\u5982\u4f55\u5224\u65ad slot 1 \u5230\u5e95\u7531\u8c01\u8d1f\u8d23\u5462\uff1f \u8fd9\u5c31\u662f configEpoch \u8d77\u4f5c\u7528\u7684\u65f6\u5019\u4e86\uff0cC \u5728 B \u53d1\u6765\u7684\u5305\u4e2d\uff0c\u53d1\u73b0\u5b83\u7684 configEpoch \uff0c\u8981\u6bd4 A \u7684\u5927\uff0c\u8bf4\u660e B \u662f\u66f4\u65b0\u7684\u914d\u7f6e\u3002\u56e0\u6b64\uff0c\u5c31\u5c06 slot 1 \u7684\u8d1f\u8d23\u8282\u70b9\u8bbe\u7f6e\u4e3a B\uff08server.cluster->slots[1] = B\uff09\u3002\u5728 slave \u53d1\u8d77\u9009\u4e3e\uff0c\u83b7\u5f97\u8db3\u591f\u591a\u7684\u9009\u7968\u4e4b\u540e\uff0c\u6210\u529f\u5f53\u9009\u65f6\uff0c\u4e5f\u5c31\u662f slave \u8bd5\u56fe\u66ff\u4ee3\u5176\u5df2\u7ecf\u4e0b\u7ebf\u7684\u65e7 master \uff0c\u6210\u4e3a\u65b0\u7684 master \u65f6\uff0c\u4f1a\u589e\u52a0\u5b83\u81ea\u5df1\u7684 configEpoch \uff0c\u4f7f\u5176\u6210\u4e3a\u5f53\u524d\u6240\u6709\u96c6\u7fa4\u8282\u70b9\u7684 configEpoch \u4e2d\u7684\u6700\u5927\u503c\u3002\u8fd9\u6837\uff0c\u8be5 slave \u6210\u4e3a master \u540e\uff0c\u5c31\u4f1a\u5411\u6240\u6709\u8282\u70b9\u53d1\u9001\u5e7f\u64ad\u5305\uff0c\u5f3a\u5236\u5176\u4ed6\u8282\u70b9\u66f4\u65b0\u76f8\u5173 slots \u7684\u8d1f\u8d23\u8282\u70b9\u4e3a\u81ea\u5df1\u3002 \u53c2\u8003\u8d44\u6599 Redis Cluster Specification Redis cluster tutorial Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528 Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb","title":"Introduction"},{"location":"Distributed-computing/Consensus-raft/zhihu-Redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E7%BA%AA%E5%85%83-epoch/#zhihu#redis#epoch","text":"","title":"zhihu Redis \u96c6\u7fa4\u4e2d\u7684\u7eaa\u5143(epoch)"},{"location":"Distributed-computing/Consensus-raft/zhihu-Redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E7%BA%AA%E5%85%83-epoch/#epoch","text":"Redis Cluster \u4f7f\u7528\u4e86\u7c7b\u4f3c\u4e8e Raft \u7b97\u6cd5 term \uff08\u4efb\u671f\uff09\u7684\u6982\u5ff5\u79f0\u4e3a epoch \uff08\u7eaa\u5143\uff09\uff0c\u7528\u6765\u7ed9**\u4e8b\u4ef6**\u589e\u52a0**\u7248\u672c\u53f7**\u3002Redis \u96c6\u7fa4\u4e2d\u7684\u7eaa\u5143\u4e3b\u8981\u662f\u4e24\u79cd\uff1a currentEpoch \u548c configEpoch \u3002","title":"\u7eaa\u5143\uff08epoch\uff09"},{"location":"Distributed-computing/Consensus-raft/zhihu-Redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E7%BA%AA%E5%85%83-epoch/#currentepoch","text":"\u8fd9\u662f\u4e00\u4e2a**\u96c6\u7fa4\u72b6\u6001**\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u53ef\u4ee5\u5f53\u505a\u8bb0\u5f55**\u96c6\u7fa4\u72b6\u6001**\u53d8\u66f4\u7684\u9012\u589e\u7248\u672c\u53f7\u3002\u6bcf\u4e2a\u96c6\u7fa4\u8282\u70b9\uff0c\u90fd\u4f1a\u901a\u8fc7 server.cluster->currentEpoch \u8bb0\u5f55\u5f53\u524d\u7684 currentEpoch \u3002 \u96c6\u7fa4\u8282\u70b9\u521b\u5efa\u65f6\uff0c\u4e0d\u7ba1\u662f master \u8fd8\u662f slave \uff0c\u90fd\u7f6e currentEpoch \u4e3a 0\u3002\u5f53\u524d\u8282\u70b9\u63a5\u6536\u5230\u6765\u81ea\u5176\u4ed6\u8282\u70b9\u7684\u5305\u65f6\uff0c\u5982\u679c\u53d1\u9001\u8005\u7684 currentEpoch \uff08\u6d88\u606f\u5934\u90e8\u4f1a\u5305\u542b\u53d1\u9001\u8005\u7684 currentEpoch \uff09\u5927\u4e8e\u5f53\u524d\u8282\u70b9\u7684***currentEpoch***\uff0c\u90a3\u4e48\u5f53\u524d\u8282\u70b9\u4f1a\u66f4\u65b0 currentEpoch \u4e3a\u53d1\u9001\u8005\u7684 currentEpoch \u3002\u56e0\u6b64\uff0c\u96c6\u7fa4\u4e2d\u6240\u6709\u8282\u70b9\u7684 currentEpoch \u6700\u7ec8\u4f1a\u8fbe\u6210\u4e00\u81f4\uff0c\u76f8\u5f53\u4e8e\u5bf9**\u96c6\u7fa4\u72b6\u6001**\u7684\u8ba4\u77e5\u8fbe\u6210\u4e86\u4e00\u81f4\u3002","title":"currentEpoch"},{"location":"Distributed-computing/Consensus-raft/zhihu-Redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E7%BA%AA%E5%85%83-epoch/#currentepoch_1","text":"currentEpoch \u4f5c\u7528\u5728\u4e8e\uff0c\u5f53\u96c6\u7fa4\u7684\u72b6\u6001\u53d1\u751f\u6539\u53d8\uff0c\u67d0\u4e2a\u8282\u70b9\u4e3a\u4e86\u6267\u884c\u4e00\u4e9b\u52a8\u4f5c\u9700\u8981\u5bfb\u6c42\u5176\u4ed6\u8282\u70b9\u7684\u540c\u610f\u65f6\uff0c\u5c31\u4f1a\u589e\u52a0 currentEpoch \u7684\u503c\u3002\u76ee\u524d currentEpoch \u53ea\u7528\u4e8e slave \u7684\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\uff0c\u8fd9\u5c31\u8ddf**\u54e8\u5175**\u4e2d\u7684 sentinel.current_epoch \u4f5c\u7528\u662f\u4e00\u6a21\u4e00\u6837\u7684\u3002\u5f53 slave A \u53d1\u73b0\u5176\u6240\u5c5e\u7684 master***\u4e0b\u7ebf\u65f6\uff0c\u5c31\u4f1a\u8bd5\u56fe\u53d1\u8d77\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\u3002\u9996\u5148\u5c31\u662f\u589e\u52a0 currentEpoch \u7684\u503c\uff0c\u8fd9\u4e2a\u589e\u52a0\u540e\u7684 currentEpoch \u662f\u6240\u6709\u96c6\u7fa4\u8282\u70b9\u4e2d\u6700\u5927\u7684\u3002\u7136\u540e**slave A** \u5411\u6240\u6709\u8282\u70b9\u53d1\u8d77\u62c9\u7968\u8bf7\u6c42\uff0c\u8bf7\u6c42\u5176\u4ed6 ***master \u6295\u7968\u7ed9\u81ea\u5df1\uff0c\u4f7f\u81ea\u5df1\u80fd\u6210\u4e3a\u65b0\u7684 master \u3002\u5176\u4ed6\u8282\u70b9\u6536\u5230\u5305\u540e\uff0c\u53d1\u73b0\u53d1\u9001\u8005\u7684 currentEpoch \u6bd4\u81ea\u5df1\u7684 currentEpoch \u5927\uff0c\u5c31\u4f1a\u66f4\u65b0\u81ea\u5df1\u7684 currentEpoch \uff0c\u5e76\u5728\u5c1a\u672a\u6295\u7968\u7684\u60c5\u51b5\u4e0b\uff0c\u6295\u7968\u7ed9 slave A \uff0c\u8868\u793a\u540c\u610f\u4f7f\u5176\u6210\u4e3a\u65b0\u7684 master \u3002","title":"currentEpoch \u4f5c\u7528"},{"location":"Distributed-computing/Consensus-raft/zhihu-Redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E7%BA%AA%E5%85%83-epoch/#configepoch","text":"\u8fd9\u662f\u4e00\u4e2a\u96c6\u7fa4**\u8282\u70b9\u914d\u7f6e**\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u8282\u70b9\u90fd\u6709\u81ea\u5df1\u72ec\u4e00\u65e0\u4e8c\u7684 configepoch\u3002\u6240\u8c13\u7684**\u8282\u70b9\u914d\u7f6e**\uff0c\u5b9e\u9645\u4e0a\u662f\u6307\u8282\u70b9\u6240\u8d1f\u8d23\u7684\u69fd\u4f4d\u4fe1\u606f\u3002 \u6bcf\u4e00\u4e2a master \u5728\u5411\u5176\u4ed6\u8282\u70b9\u53d1\u9001\u5305\u65f6\uff0c\u90fd\u4f1a\u9644\u5e26\u5176 configEpoch \u4fe1\u606f\uff0c\u4ee5\u53ca\u4e00\u4efd\u8868\u793a\u5b83\u6240\u8d1f\u8d23\u7684 slots \u4fe1\u606f\u3002\u800c slave \u5411\u5176\u4ed6\u8282\u70b9\u53d1\u9001\u5305\u65f6\uff0c\u5176\u5305\u4e2d\u7684 configEpoch \u548c\u8d1f\u8d23\u69fd\u4f4d\u4fe1\u606f\uff0c\u662f\u5176 master \u7684 configEpoch \u548c\u8d1f\u8d23\u7684 slot \u4fe1\u606f\u3002\u8282\u70b9\u6536\u5230\u5305\u4e4b\u540e\uff0c\u5c31\u4f1a\u6839\u636e\u5305\u4e2d\u7684 configEpoch***\u548c\u8d1f\u8d23\u7684 ***slots \u4fe1\u606f\uff0c\u8bb0\u5f55\u5230\u76f8\u5e94\u8282\u70b9\u5c5e\u6027\u4e2d\u3002","title":"configEpoch"},{"location":"Distributed-computing/Consensus-raft/zhihu-Redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E7%BA%AA%E5%85%83-epoch/#configepoch_1","text":"configEpoch \u4e3b\u8981\u7528\u4e8e\u89e3\u51b3\u4e0d\u540c\u7684\u8282\u70b9\u7684\u914d\u7f6e\u53d1\u751f\u51b2\u7a81\u7684\u60c5\u51b5\u3002\u4e3e\u4e2a\u4f8b\u5b50\u5c31\u660e\u767d\u4e86\uff1a\u8282\u70b9A \u5ba3\u79f0\u8d1f\u8d23 slot 1 \uff0c\u5176\u5411\u5916\u53d1\u9001\u7684\u5305\u4e2d\uff0c\u5305\u542b\u4e86\u81ea\u5df1\u7684 configEpoch \u548c\u8d1f\u8d23\u7684 slots \u4fe1\u606f\u3002\u8282\u70b9 C \u6536\u5230 A \u53d1\u6765\u7684\u5305\u540e\uff0c\u53d1\u73b0\u81ea\u5df1\u5f53\u524d\u6ca1\u6709\u8bb0\u5f55 slot 1 \u7684\u8d1f\u8d23\u8282\u70b9\uff08\u4e5f\u5c31\u662f server.cluster->slots[1] \u4e3a NULL\uff09\uff0c\u5c31\u4f1a\u5c06 A \u7f6e\u4e3a slot 1 \u7684\u8d1f\u8d23\u8282\u70b9\uff08server.cluster->slots[1] = A\uff09\uff0c\u5e76\u8bb0\u5f55\u8282\u70b9 A \u7684 configEpoch \u3002\u540e\u6765\uff0c\u8282\u70b9 C \u53c8\u6536\u5230\u4e86 B \u53d1\u6765\u7684\u5305\uff0c\u5b83\u4e5f\u5ba3\u79f0\u8d1f\u8d23 slot 1 \uff0c\u6b64\u65f6\uff0c\u5982\u4f55\u5224\u65ad slot 1 \u5230\u5e95\u7531\u8c01\u8d1f\u8d23\u5462\uff1f \u8fd9\u5c31\u662f configEpoch \u8d77\u4f5c\u7528\u7684\u65f6\u5019\u4e86\uff0cC \u5728 B \u53d1\u6765\u7684\u5305\u4e2d\uff0c\u53d1\u73b0\u5b83\u7684 configEpoch \uff0c\u8981\u6bd4 A \u7684\u5927\uff0c\u8bf4\u660e B \u662f\u66f4\u65b0\u7684\u914d\u7f6e\u3002\u56e0\u6b64\uff0c\u5c31\u5c06 slot 1 \u7684\u8d1f\u8d23\u8282\u70b9\u8bbe\u7f6e\u4e3a B\uff08server.cluster->slots[1] = B\uff09\u3002\u5728 slave \u53d1\u8d77\u9009\u4e3e\uff0c\u83b7\u5f97\u8db3\u591f\u591a\u7684\u9009\u7968\u4e4b\u540e\uff0c\u6210\u529f\u5f53\u9009\u65f6\uff0c\u4e5f\u5c31\u662f slave \u8bd5\u56fe\u66ff\u4ee3\u5176\u5df2\u7ecf\u4e0b\u7ebf\u7684\u65e7 master \uff0c\u6210\u4e3a\u65b0\u7684 master \u65f6\uff0c\u4f1a\u589e\u52a0\u5b83\u81ea\u5df1\u7684 configEpoch \uff0c\u4f7f\u5176\u6210\u4e3a\u5f53\u524d\u6240\u6709\u96c6\u7fa4\u8282\u70b9\u7684 configEpoch \u4e2d\u7684\u6700\u5927\u503c\u3002\u8fd9\u6837\uff0c\u8be5 slave \u6210\u4e3a master \u540e\uff0c\u5c31\u4f1a\u5411\u6240\u6709\u8282\u70b9\u53d1\u9001\u5e7f\u64ad\u5305\uff0c\u5f3a\u5236\u5176\u4ed6\u8282\u70b9\u66f4\u65b0\u76f8\u5173 slots \u7684\u8d1f\u8d23\u8282\u70b9\u4e3a\u81ea\u5df1\u3002","title":"configEpoch \u4f5c\u7528"},{"location":"Distributed-computing/Consensus-raft/zhihu-Redis%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E7%BA%AA%E5%85%83-epoch/#_1","text":"Redis Cluster Specification Redis cluster tutorial Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528 Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb","title":"\u53c2\u8003\u8d44\u6599"},{"location":"Distributed-computing/Consistency/","text":"Redis \u6700\u7ec8\u4e00\u81f4\u6027 \u7d20\u6750: \u4e00\u3001cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b \u5176\u4e2d\u63d0\u53caGossip\u662f\u4e00\u4e2a \u6700\u7ec8\u4e00\u81f4\u6027\u7b97\u6cd5 \u3002 \u4e8c\u3001 Redis Sentinel Documentation \u4e2d\u63d0\u53ca: In general Redis + Sentinel as a whole are a an eventually consistent system where the merge function is last failover wins , and the data from old masters are discarded to replicate the data of the current master, so there is always a window for losing acknowledged writes. This is due to Redis asynchronous replication and the discarding nature of the \"virtual\" merge function of the system. Note that this is not a limitation of Sentinel itself, and if you orchestrate the failover with a strongly consistent replicated state machine, the same properties will still apply. There are only two ways to avoid losing acknowledged writes: \u4e09\u3001Redis replication\u7684consistency model \u5728redis Replication \u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u975e\u5e38\u8be6\u7ec6\u7684\u4ecb\u7ecd: Synchronous replication of certain data can be requested by the clients using the WAIT command. However WAIT is only able to ensure that there are the specified number of acknowledged copies in the other Redis instances, it does not turn a set of Redis instances into a CP system with strong consistency: acknowledged writes can still be lost during a failover, depending on the exact configuration of the Redis persistence. However with WAIT the probability of losing a write after a failure event is greatly reduced to certain hard to trigger failure modes. \u56db\u3001raft eventual consistency","title":"Introduction"},{"location":"Distributed-computing/Consistency/#redis","text":"\u7d20\u6750: \u4e00\u3001cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b \u5176\u4e2d\u63d0\u53caGossip\u662f\u4e00\u4e2a \u6700\u7ec8\u4e00\u81f4\u6027\u7b97\u6cd5 \u3002 \u4e8c\u3001 Redis Sentinel Documentation \u4e2d\u63d0\u53ca: In general Redis + Sentinel as a whole are a an eventually consistent system where the merge function is last failover wins , and the data from old masters are discarded to replicate the data of the current master, so there is always a window for losing acknowledged writes. This is due to Redis asynchronous replication and the discarding nature of the \"virtual\" merge function of the system. Note that this is not a limitation of Sentinel itself, and if you orchestrate the failover with a strongly consistent replicated state machine, the same properties will still apply. There are only two ways to avoid losing acknowledged writes: \u4e09\u3001Redis replication\u7684consistency model \u5728redis Replication \u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u975e\u5e38\u8be6\u7ec6\u7684\u4ecb\u7ecd: Synchronous replication of certain data can be requested by the clients using the WAIT command. However WAIT is only able to ensure that there are the specified number of acknowledged copies in the other Redis instances, it does not turn a set of Redis instances into a CP system with strong consistency: acknowledged writes can still be lost during a failover, depending on the exact configuration of the Redis persistence. However with WAIT the probability of losing a write after a failure event is greatly reduced to certain hard to trigger failure modes. \u56db\u3001raft eventual consistency","title":"Redis \u6700\u7ec8\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Write-safety-analysis/","text":"Redis write safety\u5206\u6790 \u4e00\u3001\u5728 redis Redis Cluster Specification \u4e2d\uff0c\u5bf9\u6b64\u6709\u7740\u6bd4\u8f83\u597d\u7684\u5206\u6790 \u4e8c\u3001csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f NOTE: \u8fd9\u7bc7\u6587\u7ae0\u603b\u7ed3\u5730\u4e0d\u9519\uff0c\u53ef\u4ee5\u4f5c\u4e3a redis Redis Cluster Specification \u7684\u8865\u5145\u6750\u6599 Redis Cluster \u4e0d\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\uff0c\u5728\u4e00\u4e9b\u7279\u6b8a\u573a\u666f\uff0c\u5ba2\u6237\u7aef\u5373\u4f7f\u6536\u5230\u4e86\u5199\u5165\u786e\u8ba4\uff0c\u8fd8\u662f\u53ef\u80fd\u4e22\u6570\u636e\u7684\u3002 \u573a\u666f1\uff1a\u5f02\u6b65\u590d\u5236 NOTE: \u8fd9\u79cd\u662f\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\u7684 client \u5199\u5165 master B master B \u56de\u590d OK master B \u540c\u6b65\u81f3 slave B1 B2 B3 B \u6ca1\u6709\u7b49\u5f85 B1 B2 B3 \u7684\u786e\u8ba4\u5c31\u56de\u590d\u4e86 client\uff0c\u5982\u679c\u5728 slave \u540c\u6b65\u5b8c\u6210\u4e4b\u524d\uff0cmaster \u5b95\u673a\u4e86\uff0c\u5176\u4e2d\u4e00\u4e2a slave \u4f1a\u88ab\u9009\u4e3a master\uff0c\u8fd9\u65f6\u4e4b\u524d client \u5199\u5165\u7684\u6570\u636e\u5c31\u4e22\u4e86\u3002 wait \u547d\u4ee4\u53ef\u4ee5\u589e\u5f3a\u8fd9\u79cd\u573a\u666f\u7684\u6570\u636e\u5b89\u5168\u6027\u3002 wait \u4f1a\u963b\u585e\u5f53\u524d client \u76f4\u5230\u4e4b\u524d\u7684\u5199\u64cd\u4f5c\u88ab\u6307\u5b9a\u6570\u91cf\u7684 slave \u540c\u6b65\u6210\u529f\u3002 wait \u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u7684\u5b89\u5168\u6027\uff0c\u4f46\u5e76\u4e0d\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\u3002 \u56e0\u4e3a\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u79cd\u540c\u6b65\u590d\u5236\u65b9\u5f0f\uff0c\u4e5f\u5b58\u5728\u7279\u6b8a\u60c5\u51b5\uff1a\u4e00\u4e2a\u6ca1\u6709\u5b8c\u6210\u540c\u6b65\u7684 slave \u88ab\u9009\u4e3e\u4e3a\u4e86 master\u3002 \u573a\u666f2\uff1a\u7f51\u7edc\u5206\u533a 6\u4e2a\u8282\u70b9 A, B, C, A1, B1, C1 \uff0c3\u4e2amaster\uff0c3\u4e2aslave\uff0c\u8fd8\u6709\u4e00\u4e2aclient\uff0c Z1 \u3002 \u53d1\u751f\u7f51\u7edc\u5206\u533a\u4e4b\u540e\uff0c\u5f62\u6210\u4e862\u4e2a\u533a\uff0c A, C, A1, B1, C1 \u548c B Z1 \u3002 \u8fd9\u65f6 Z1 \u8fd8\u662f\u53ef\u4ee5\u5411 B \u5199\u5165\u7684\uff0c\u5982\u679c\u77ed\u65f6\u95f4\u5185\u5206\u533a\u5c31\u6062\u590d\u4e86\uff0c\u90a3\u5c31\u6ca1\u95ee\u9898\uff0c\u6574\u4e2a\u96c6\u7fa4\u7ee7\u7eed\u6b63\u5e38\u5de5\u4f5c\uff0c\u4f46\u5982\u679c\u65f6\u95f4\u4e00\u957f\uff0cB1 \u5c31\u4f1a\u6210\u4e3a\u6240\u5728\u5206\u533a\u7684 master\uff0cZ1 \u5199\u5165 B \u7684\u6570\u636e\u5c31\u4e22\u4e86\u3002 maximum window\uff08\u6700\u5927\u65f6\u95f4\u7a97\u53e3\uff09 \u53ef\u4ee5\u51cf\u5c11\u6570\u636e\u635f\u5931\uff0c\u53ef\u4ee5\u63a7\u5236 Z1 \u5411 B \u5199\u5165\u7684\u603b\u6570\uff1a \u8fc7\u53bb\u4e00\u5b9a\u65f6\u95f4\u540e\uff0c\u5206\u533a\u7684\u591a\u6570\u8fb9\u5c31\u4f1a\u8fdb\u884c\u9009\u4e3e\uff0cslave \u6210\u4e3a master\uff0c\u8fd9\u65f6\u5206\u533a\u5c11\u6570\u8fb9\u7684 master \u5c31\u4f1a\u62d2\u7edd\u63a5\u6536\u5199\u8bf7\u6c42\u3002 \u8fd9\u4e2a**\u65f6\u95f4\u91cf**\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u79f0\u4e3a**\u8282\u70b9\u8fc7\u671f\u65f6\u95f4**\u3002 \u4e00\u4e2a master \u5728\u8fbe\u5230\u8fc7\u671f\u65f6\u95f4\u540e\uff0c\u5c31\u88ab\u8ba4\u4e3a\u662f\u6545\u969c\u7684\uff0c\u8fdb\u5165 error \u72b6\u6001\uff0c\u505c\u6b62\u63a5\u6536\u5199\u8bf7\u6c42\uff0c\u53ef\u4ee5\u88ab slave \u53d6\u4ee3\u3002 \u5c0f\u7ed3 Redis Cluster \u4e0d\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\uff0c\u5b58\u5728\u4e22\u5931\u6570\u636e\u7684\u573a\u666f\uff1a \u5f02\u6b65\u590d\u5236 \u5728 master \u5199\u6210\u529f\uff0c\u4f46 slave \u540c\u6b65\u5b8c\u6210\u4e4b\u524d\uff0cmaster \u5b95\u673a\u4e86\uff0cslave \u53d8\u4e3a master\uff0c\u6570\u636e\u4e22\u5931\u3002 wait \u547d\u4ee4\u53ef\u4ee5\u7ed9\u4e3a\u540c\u6b65\u590d\u5236\uff0c\u4f46\u4e5f\u65e0\u6cd5\u5b8c\u5168\u4fdd\u8bc1\u6570\u636e\u4e0d\u4e22\uff0c\u800c\u4e14\u5f71\u54cd\u6027\u80fd\u3002 \u7f51\u7edc\u5206\u533a \u5206\u533a\u540e\u4e00\u4e2a master \u7ee7\u7eed\u63a5\u6536\u5199\u8bf7\u6c42\uff0c\u5206\u533a\u6062\u590d\u540e\u8fd9\u4e2a master \u53ef\u80fd\u4f1a\u53d8\u4e3a slave\uff0c\u90a3\u4e48\u4e4b\u524d\u5199\u5165\u7684\u6570\u636e\u5c31\u4e22\u4e86\u3002 \u53ef\u4ee5\u8bbe\u7f6e\u8282\u70b9\u8fc7\u671f\u65f6\u95f4\uff0c\u51cf\u5c11 master \u5728\u5206\u533a\u671f\u95f4\u63a5\u6536\u7684\u5199\u5165\u6570\u91cf\uff0c\u964d\u4f4e\u6570\u636e\u4e22\u5931\u7684\u635f\u5931\u3002","title":"Introduction"},{"location":"Distributed-computing/Write-safety-analysis/#redis#write#safety","text":"\u4e00\u3001\u5728 redis Redis Cluster Specification \u4e2d\uff0c\u5bf9\u6b64\u6709\u7740\u6bd4\u8f83\u597d\u7684\u5206\u6790 \u4e8c\u3001csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f","title":"Redis write safety\u5206\u6790"},{"location":"Distributed-computing/Write-safety-analysis/#csdn#redis#cluster","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u603b\u7ed3\u5730\u4e0d\u9519\uff0c\u53ef\u4ee5\u4f5c\u4e3a redis Redis Cluster Specification \u7684\u8865\u5145\u6750\u6599 Redis Cluster \u4e0d\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\uff0c\u5728\u4e00\u4e9b\u7279\u6b8a\u573a\u666f\uff0c\u5ba2\u6237\u7aef\u5373\u4f7f\u6536\u5230\u4e86\u5199\u5165\u786e\u8ba4\uff0c\u8fd8\u662f\u53ef\u80fd\u4e22\u6570\u636e\u7684\u3002","title":"csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f"},{"location":"Distributed-computing/Write-safety-analysis/#1","text":"NOTE: \u8fd9\u79cd\u662f\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\u7684 client \u5199\u5165 master B master B \u56de\u590d OK master B \u540c\u6b65\u81f3 slave B1 B2 B3 B \u6ca1\u6709\u7b49\u5f85 B1 B2 B3 \u7684\u786e\u8ba4\u5c31\u56de\u590d\u4e86 client\uff0c\u5982\u679c\u5728 slave \u540c\u6b65\u5b8c\u6210\u4e4b\u524d\uff0cmaster \u5b95\u673a\u4e86\uff0c\u5176\u4e2d\u4e00\u4e2a slave \u4f1a\u88ab\u9009\u4e3a master\uff0c\u8fd9\u65f6\u4e4b\u524d client \u5199\u5165\u7684\u6570\u636e\u5c31\u4e22\u4e86\u3002 wait \u547d\u4ee4\u53ef\u4ee5\u589e\u5f3a\u8fd9\u79cd\u573a\u666f\u7684\u6570\u636e\u5b89\u5168\u6027\u3002 wait \u4f1a\u963b\u585e\u5f53\u524d client \u76f4\u5230\u4e4b\u524d\u7684\u5199\u64cd\u4f5c\u88ab\u6307\u5b9a\u6570\u91cf\u7684 slave \u540c\u6b65\u6210\u529f\u3002 wait \u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u7684\u5b89\u5168\u6027\uff0c\u4f46\u5e76\u4e0d\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\u3002 \u56e0\u4e3a\u5373\u4f7f\u4f7f\u7528\u4e86\u8fd9\u79cd\u540c\u6b65\u590d\u5236\u65b9\u5f0f\uff0c\u4e5f\u5b58\u5728\u7279\u6b8a\u60c5\u51b5\uff1a\u4e00\u4e2a\u6ca1\u6709\u5b8c\u6210\u540c\u6b65\u7684 slave \u88ab\u9009\u4e3e\u4e3a\u4e86 master\u3002","title":"\u573a\u666f1\uff1a\u5f02\u6b65\u590d\u5236"},{"location":"Distributed-computing/Write-safety-analysis/#2","text":"6\u4e2a\u8282\u70b9 A, B, C, A1, B1, C1 \uff0c3\u4e2amaster\uff0c3\u4e2aslave\uff0c\u8fd8\u6709\u4e00\u4e2aclient\uff0c Z1 \u3002 \u53d1\u751f\u7f51\u7edc\u5206\u533a\u4e4b\u540e\uff0c\u5f62\u6210\u4e862\u4e2a\u533a\uff0c A, C, A1, B1, C1 \u548c B Z1 \u3002 \u8fd9\u65f6 Z1 \u8fd8\u662f\u53ef\u4ee5\u5411 B \u5199\u5165\u7684\uff0c\u5982\u679c\u77ed\u65f6\u95f4\u5185\u5206\u533a\u5c31\u6062\u590d\u4e86\uff0c\u90a3\u5c31\u6ca1\u95ee\u9898\uff0c\u6574\u4e2a\u96c6\u7fa4\u7ee7\u7eed\u6b63\u5e38\u5de5\u4f5c\uff0c\u4f46\u5982\u679c\u65f6\u95f4\u4e00\u957f\uff0cB1 \u5c31\u4f1a\u6210\u4e3a\u6240\u5728\u5206\u533a\u7684 master\uff0cZ1 \u5199\u5165 B \u7684\u6570\u636e\u5c31\u4e22\u4e86\u3002 maximum window\uff08\u6700\u5927\u65f6\u95f4\u7a97\u53e3\uff09 \u53ef\u4ee5\u51cf\u5c11\u6570\u636e\u635f\u5931\uff0c\u53ef\u4ee5\u63a7\u5236 Z1 \u5411 B \u5199\u5165\u7684\u603b\u6570\uff1a \u8fc7\u53bb\u4e00\u5b9a\u65f6\u95f4\u540e\uff0c\u5206\u533a\u7684\u591a\u6570\u8fb9\u5c31\u4f1a\u8fdb\u884c\u9009\u4e3e\uff0cslave \u6210\u4e3a master\uff0c\u8fd9\u65f6\u5206\u533a\u5c11\u6570\u8fb9\u7684 master \u5c31\u4f1a\u62d2\u7edd\u63a5\u6536\u5199\u8bf7\u6c42\u3002 \u8fd9\u4e2a**\u65f6\u95f4\u91cf**\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u79f0\u4e3a**\u8282\u70b9\u8fc7\u671f\u65f6\u95f4**\u3002 \u4e00\u4e2a master \u5728\u8fbe\u5230\u8fc7\u671f\u65f6\u95f4\u540e\uff0c\u5c31\u88ab\u8ba4\u4e3a\u662f\u6545\u969c\u7684\uff0c\u8fdb\u5165 error \u72b6\u6001\uff0c\u505c\u6b62\u63a5\u6536\u5199\u8bf7\u6c42\uff0c\u53ef\u4ee5\u88ab slave \u53d6\u4ee3\u3002","title":"\u573a\u666f2\uff1a\u7f51\u7edc\u5206\u533a"},{"location":"Distributed-computing/Write-safety-analysis/#_1","text":"Redis Cluster \u4e0d\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\uff0c\u5b58\u5728\u4e22\u5931\u6570\u636e\u7684\u573a\u666f\uff1a \u5f02\u6b65\u590d\u5236 \u5728 master \u5199\u6210\u529f\uff0c\u4f46 slave \u540c\u6b65\u5b8c\u6210\u4e4b\u524d\uff0cmaster \u5b95\u673a\u4e86\uff0cslave \u53d8\u4e3a master\uff0c\u6570\u636e\u4e22\u5931\u3002 wait \u547d\u4ee4\u53ef\u4ee5\u7ed9\u4e3a\u540c\u6b65\u590d\u5236\uff0c\u4f46\u4e5f\u65e0\u6cd5\u5b8c\u5168\u4fdd\u8bc1\u6570\u636e\u4e0d\u4e22\uff0c\u800c\u4e14\u5f71\u54cd\u6027\u80fd\u3002 \u7f51\u7edc\u5206\u533a \u5206\u533a\u540e\u4e00\u4e2a master \u7ee7\u7eed\u63a5\u6536\u5199\u8bf7\u6c42\uff0c\u5206\u533a\u6062\u590d\u540e\u8fd9\u4e2a master \u53ef\u80fd\u4f1a\u53d8\u4e3a slave\uff0c\u90a3\u4e48\u4e4b\u524d\u5199\u5165\u7684\u6570\u636e\u5c31\u4e22\u4e86\u3002 \u53ef\u4ee5\u8bbe\u7f6e\u8282\u70b9\u8fc7\u671f\u65f6\u95f4\uff0c\u51cf\u5c11 master \u5728\u5206\u533a\u671f\u95f4\u63a5\u6536\u7684\u5199\u5165\u6570\u91cf\uff0c\u964d\u4f4e\u6570\u636e\u4e22\u5931\u7684\u635f\u5931\u3002","title":"\u5c0f\u7ed3"},{"location":"Expert-antirez/","text":"","title":"Introduction"},{"location":"Expert-antirez/An-update-about-Redis-developments-in-2019/","text":"antirez An update about Redis developments in 2019 Multi threading There are two possible multi threading supports that Redis could get. I believe the user is referring to \u201cmemcached alike\u201d multithreading, that is the ability to scale a single Redis instance to multiple threads in order to increase the operations per second it can deliver in things like GET or SET and other simple commands. This involves making the I/O, command parsing and so forth multi threaded. So let\u2019s call this thing \u201c I/O threading \u201d. Another multi threaded approach is to, instead, allow slow commands to be executed in a different thread, so that other clients are not blocked. We\u2019ll call this threading model \u201c Slow commands threading \u201d. Well, that\u2019s the plan: I/O threading is not going to happen in Redis AFAIK, because after much consideration I think it\u2019s a lot of complexity without a good reason. Many Redis setups are network or memory bound actually. Additionally I really believe in a share-nothing setup, so the way I want to scale Redis is by improving the support for multiple Redis instances to be executed in the same host, especially via Redis Cluster. The things that will happen in 2019 about that are two: A) Redis Cluster multiple instances will be able to orchestrate(\u7cbe\u5fc3\u7f16\u5236) to make a judicious use of the disk of the local instance, that is, let\u2019s avoid an AOF rewrite at the same time. B) We are going to ship a Redis Cluster proxy as part of the Redis project, so that users are able to abstract away a cluster without having a good implementation of the Cluster protocol client side. Redis VS Memcached Another thing to note is that Redis is not Memcached, but, like memcached, is an in-memory system. To make multithreaded an in-memory system like memcached, with a very simple data model, makes a lot of sense. A multi-threaded on-disk store is mandatory. A multi-threaded complex in-memory system is in the middle where things become ugly: Redis clients are not isolated, and data structures are complex. A thread doing LPUSH need to serve other threads doing LPOP. There is less to gain, and a lot of complexity to add. NOTE: \u4e00\u3001\u7ffb\u8bd1: \"\u53e6\u5916\u9700\u8981\u6ce8\u610f\u7684\u662fRedis\u4e0d\u662fMemcached\uff0c\u4f46\u662f\u50cfmemcached\u4e00\u6837\uff0c\u662f\u4e00\u4e2a\u5185\u5b58\u7cfb\u7edf\u3002 \u4f7f\u7528\u975e\u5e38\u7b80\u5355\u7684\u6570\u636e\u6a21\u578b\u5236\u4f5c\u591a\u7ebf\u7a0b\u5185\u5b58\u7cfb\u7edf\uff08\u5982memcached\uff09\u975e\u5e38\u6709\u610f\u4e49\u3002 \u5fc5\u987b\u4f7f\u7528\u591a\u7ebf\u7a0b\u78c1\u76d8\u5b58\u50a8\u3002 \u4e00\u4e2a\u591a\u7ebf\u7a0b\u590d\u6742\u7684\u5185\u5b58\u7cfb\u7edf\u6b63\u5904\u4e8e\u4e11\u964b\u7684\u4e2d\u95f4\uff1aRedis\u5ba2\u6237\u7aef\u4e0d\u662f\u5b64\u7acb\u7684\uff0c\u6570\u636e\u7ed3\u6784\u4e5f\u5f88\u590d\u6742\u3002 \u6267\u884cLPUSH\u7684\u7ebf\u7a0b\u9700\u8981\u670d\u52a1\u4e8e\u6267\u884cLPOP\u7684\u5176\u4ed6\u7ebf\u7a0b\u3002 \u83b7\u5f97\u7684\u6536\u76ca\u8f83\u5c11\uff0c\u6dfb\u52a0\u7684\u590d\u6742\u6027\u4e5f\u5f88\u9ad8\u3002\" \u4e8c\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u4f5c\u8005\u5bf9Redis \u652f\u6301 multithread \u7684\u601d\u8003\u3001tradeoff\uff0c\u4f5c\u8005\u4f7f\u7528Memcached\u6765\u4f5c\u4e3a\u5bf9\u7167(Memcached\u662fmultithread\u7684) 1\u3001Redis\u7684data model\u662f\u6bd4Memcached\u8981\u590d\u6742\u7684\uff0c\u56e0\u6b64\uff0c\u5f53Redis\u8003\u8651\u652f\u6301multithread\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u8003\u8651\u53ef\u80fd\u51fa\u73b0\u7684\u5bf9data structures\u7684\u5e76\u53d1\u64cd\u4f5c\uff0c\u4f5c\u8005\u5217\u4e3e\u4e86\u4e00\u4e2a\u4f8b\u5b50: \"A thread doing LPUSH need to serve other threads doing LPOP\"\uff0c\u5373\u591a\u4e2athread\u540c\u65f6\u5bf9\u4e00\u4e2adata structure\u8fdb\u884c\u64cd\u4f5c\uff0c\u90a3\u5982\u4f55\u8fdb\u884c\u540c\u6b65\u5462\uff1f\u663e\u7136\u8fd9\u662f\u6bd4\u8f83\u590d\u6742\u7684\uff0c\u9700\u8981\u8003\u8651\u975e\u5e38\u591a\u7684\u60c5\u51b5\u3002 2\u3001\u56e0\u6b64\uff0c\u662f\u5426\u91c7\u7528multithread\u662f\u9700\u8981\u8fdb\u884ctradeoff\u7684: a\u3001\u80fd\u591f\u83b7\u5f97\u591a\u5927\u7684\u6027\u80fd\u63d0\u5347 b\u3001\u5b9e\u73b0\u7684\u590d\u6742\u5ea6\u3001\u7ef4\u62a4\u6027 \u4e09\u3001\u4ece\u76ee\u524d(2021-06-05)\u7684Redis\u7684\u5b9e\u73b0\u6765\u770b\uff0c\u5b83\u91c7\u7528\u7684\u662f\u4e00\u79cd\u6298\u4e2d\u7684\u65b9\u6848: \u5177\u4f53\u53c2\u89c1 zhihu Redis 6.0 \u591a\u7ebf\u7a0bIO\u5904\u7406\u8fc7\u7a0b\u8be6\u89e3 \u3002 What instead I really want a lot is slow operations threading, and with the Redis modules system we already are in the right direction. However in the future (not sure if in Redis 6 or 7) we\u2019ll get key-level locking in the module system so that threads can completely acquire control of a key to process slow operations. Now modules can implement commands and can create a reply for the client in a completely separated way, but still to access the shared data set a global lock is needed: this will go away. NOTE: \"\u76f8\u53cd\uff0c\u6211\u771f\u6b63\u60f3\u8981\u7684\u662f\u6162\u901f\u64cd\u4f5c\u7ebf\u7a0b\uff0c\u800c\u4f7f\u7528Redis\u6a21\u5757\u7cfb\u7edf\uff0c\u6211\u4eec\u5df2\u7ecf\u671d\u7740\u6b63\u786e\u7684\u65b9\u5411\u524d\u8fdb\u4e86\u3002 \u4f46\u662f\u5728\u5c06\u6765\uff08\u4e0d\u786e\u5b9a\u662f\u5426\u5728Redis 6\u62167\u4e2d\uff09\u6211\u4eec\u5c06\u5728\u6a21\u5757\u7cfb\u7edf\u4e2d\u83b7\u5f97key-level \u9501\u5b9a\uff0c\u4ee5\u4fbf\u7ebf\u7a0b\u53ef\u4ee5\u5b8c\u5168\u83b7\u5f97\u5bf9key\u7684\u63a7\u5236\u4ee5\u5904\u7406\u6162\u901f\u64cd\u4f5c\u3002 \u73b0\u5728\uff0c\u6a21\u5757\u53ef\u4ee5\u5b9e\u73b0\u547d\u4ee4\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ee5\u5b8c\u5168\u72ec\u7acb\u7684\u65b9\u5f0f\u4e3a\u5ba2\u6237\u7aef\u521b\u5efa\u56de\u590d\uff0c\u4f46\u4ecd\u7136\u9700\u8981\u8bbf\u95ee\u5171\u4eab\u6570\u636e\u96c6\uff0c\u9700\u8981\u5168\u5c40\u9501\u5b9a\uff1a\u8fd9\u5c06\u6d88\u5931\u3002\" Data structures Now Redis has Streams, starting with Redis 5. For Redis 6 and 7 what is planned is, to start, to make what we have much more memory efficient by changing the implementations of certain things. However to add new data structures there are a lot of considerations to do. It took me years to realize how to fill the gap, with streams, between lists, pub/sub and sorted sets, in the context of time series and streaming. I really want Redis to be a set of orthogonal data structures that the user can put together, and not a set of tools that are ready to use. Streams are an abstract log, so I think it\u2019s a very worthwhile addition. However other things I\u2019m not completely sure if they are worth to be inside the core without a very long consideration. Anyway in the latest years there was definitely more stress in adding new data structures. HyperLogLogs, more advanced bit operations, streams, blocking sorted set operations (ZPOP* and BZPOP*), and streams are good examples.","title":"Introduction"},{"location":"Expert-antirez/An-update-about-Redis-developments-in-2019/#antirez#an#update#about#redis#developments#in#2019","text":"","title":"antirez An update about Redis developments in 2019"},{"location":"Expert-antirez/An-update-about-Redis-developments-in-2019/#multi#threading","text":"There are two possible multi threading supports that Redis could get. I believe the user is referring to \u201cmemcached alike\u201d multithreading, that is the ability to scale a single Redis instance to multiple threads in order to increase the operations per second it can deliver in things like GET or SET and other simple commands. This involves making the I/O, command parsing and so forth multi threaded. So let\u2019s call this thing \u201c I/O threading \u201d. Another multi threaded approach is to, instead, allow slow commands to be executed in a different thread, so that other clients are not blocked. We\u2019ll call this threading model \u201c Slow commands threading \u201d. Well, that\u2019s the plan: I/O threading is not going to happen in Redis AFAIK, because after much consideration I think it\u2019s a lot of complexity without a good reason. Many Redis setups are network or memory bound actually. Additionally I really believe in a share-nothing setup, so the way I want to scale Redis is by improving the support for multiple Redis instances to be executed in the same host, especially via Redis Cluster. The things that will happen in 2019 about that are two: A) Redis Cluster multiple instances will be able to orchestrate(\u7cbe\u5fc3\u7f16\u5236) to make a judicious use of the disk of the local instance, that is, let\u2019s avoid an AOF rewrite at the same time. B) We are going to ship a Redis Cluster proxy as part of the Redis project, so that users are able to abstract away a cluster without having a good implementation of the Cluster protocol client side.","title":"Multi threading"},{"location":"Expert-antirez/An-update-about-Redis-developments-in-2019/#redis#vs#memcached","text":"Another thing to note is that Redis is not Memcached, but, like memcached, is an in-memory system. To make multithreaded an in-memory system like memcached, with a very simple data model, makes a lot of sense. A multi-threaded on-disk store is mandatory. A multi-threaded complex in-memory system is in the middle where things become ugly: Redis clients are not isolated, and data structures are complex. A thread doing LPUSH need to serve other threads doing LPOP. There is less to gain, and a lot of complexity to add. NOTE: \u4e00\u3001\u7ffb\u8bd1: \"\u53e6\u5916\u9700\u8981\u6ce8\u610f\u7684\u662fRedis\u4e0d\u662fMemcached\uff0c\u4f46\u662f\u50cfmemcached\u4e00\u6837\uff0c\u662f\u4e00\u4e2a\u5185\u5b58\u7cfb\u7edf\u3002 \u4f7f\u7528\u975e\u5e38\u7b80\u5355\u7684\u6570\u636e\u6a21\u578b\u5236\u4f5c\u591a\u7ebf\u7a0b\u5185\u5b58\u7cfb\u7edf\uff08\u5982memcached\uff09\u975e\u5e38\u6709\u610f\u4e49\u3002 \u5fc5\u987b\u4f7f\u7528\u591a\u7ebf\u7a0b\u78c1\u76d8\u5b58\u50a8\u3002 \u4e00\u4e2a\u591a\u7ebf\u7a0b\u590d\u6742\u7684\u5185\u5b58\u7cfb\u7edf\u6b63\u5904\u4e8e\u4e11\u964b\u7684\u4e2d\u95f4\uff1aRedis\u5ba2\u6237\u7aef\u4e0d\u662f\u5b64\u7acb\u7684\uff0c\u6570\u636e\u7ed3\u6784\u4e5f\u5f88\u590d\u6742\u3002 \u6267\u884cLPUSH\u7684\u7ebf\u7a0b\u9700\u8981\u670d\u52a1\u4e8e\u6267\u884cLPOP\u7684\u5176\u4ed6\u7ebf\u7a0b\u3002 \u83b7\u5f97\u7684\u6536\u76ca\u8f83\u5c11\uff0c\u6dfb\u52a0\u7684\u590d\u6742\u6027\u4e5f\u5f88\u9ad8\u3002\" \u4e8c\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u4f5c\u8005\u5bf9Redis \u652f\u6301 multithread \u7684\u601d\u8003\u3001tradeoff\uff0c\u4f5c\u8005\u4f7f\u7528Memcached\u6765\u4f5c\u4e3a\u5bf9\u7167(Memcached\u662fmultithread\u7684) 1\u3001Redis\u7684data model\u662f\u6bd4Memcached\u8981\u590d\u6742\u7684\uff0c\u56e0\u6b64\uff0c\u5f53Redis\u8003\u8651\u652f\u6301multithread\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u8003\u8651\u53ef\u80fd\u51fa\u73b0\u7684\u5bf9data structures\u7684\u5e76\u53d1\u64cd\u4f5c\uff0c\u4f5c\u8005\u5217\u4e3e\u4e86\u4e00\u4e2a\u4f8b\u5b50: \"A thread doing LPUSH need to serve other threads doing LPOP\"\uff0c\u5373\u591a\u4e2athread\u540c\u65f6\u5bf9\u4e00\u4e2adata structure\u8fdb\u884c\u64cd\u4f5c\uff0c\u90a3\u5982\u4f55\u8fdb\u884c\u540c\u6b65\u5462\uff1f\u663e\u7136\u8fd9\u662f\u6bd4\u8f83\u590d\u6742\u7684\uff0c\u9700\u8981\u8003\u8651\u975e\u5e38\u591a\u7684\u60c5\u51b5\u3002 2\u3001\u56e0\u6b64\uff0c\u662f\u5426\u91c7\u7528multithread\u662f\u9700\u8981\u8fdb\u884ctradeoff\u7684: a\u3001\u80fd\u591f\u83b7\u5f97\u591a\u5927\u7684\u6027\u80fd\u63d0\u5347 b\u3001\u5b9e\u73b0\u7684\u590d\u6742\u5ea6\u3001\u7ef4\u62a4\u6027 \u4e09\u3001\u4ece\u76ee\u524d(2021-06-05)\u7684Redis\u7684\u5b9e\u73b0\u6765\u770b\uff0c\u5b83\u91c7\u7528\u7684\u662f\u4e00\u79cd\u6298\u4e2d\u7684\u65b9\u6848: \u5177\u4f53\u53c2\u89c1 zhihu Redis 6.0 \u591a\u7ebf\u7a0bIO\u5904\u7406\u8fc7\u7a0b\u8be6\u89e3 \u3002 What instead I really want a lot is slow operations threading, and with the Redis modules system we already are in the right direction. However in the future (not sure if in Redis 6 or 7) we\u2019ll get key-level locking in the module system so that threads can completely acquire control of a key to process slow operations. Now modules can implement commands and can create a reply for the client in a completely separated way, but still to access the shared data set a global lock is needed: this will go away. NOTE: \"\u76f8\u53cd\uff0c\u6211\u771f\u6b63\u60f3\u8981\u7684\u662f\u6162\u901f\u64cd\u4f5c\u7ebf\u7a0b\uff0c\u800c\u4f7f\u7528Redis\u6a21\u5757\u7cfb\u7edf\uff0c\u6211\u4eec\u5df2\u7ecf\u671d\u7740\u6b63\u786e\u7684\u65b9\u5411\u524d\u8fdb\u4e86\u3002 \u4f46\u662f\u5728\u5c06\u6765\uff08\u4e0d\u786e\u5b9a\u662f\u5426\u5728Redis 6\u62167\u4e2d\uff09\u6211\u4eec\u5c06\u5728\u6a21\u5757\u7cfb\u7edf\u4e2d\u83b7\u5f97key-level \u9501\u5b9a\uff0c\u4ee5\u4fbf\u7ebf\u7a0b\u53ef\u4ee5\u5b8c\u5168\u83b7\u5f97\u5bf9key\u7684\u63a7\u5236\u4ee5\u5904\u7406\u6162\u901f\u64cd\u4f5c\u3002 \u73b0\u5728\uff0c\u6a21\u5757\u53ef\u4ee5\u5b9e\u73b0\u547d\u4ee4\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ee5\u5b8c\u5168\u72ec\u7acb\u7684\u65b9\u5f0f\u4e3a\u5ba2\u6237\u7aef\u521b\u5efa\u56de\u590d\uff0c\u4f46\u4ecd\u7136\u9700\u8981\u8bbf\u95ee\u5171\u4eab\u6570\u636e\u96c6\uff0c\u9700\u8981\u5168\u5c40\u9501\u5b9a\uff1a\u8fd9\u5c06\u6d88\u5931\u3002\"","title":"Redis VS Memcached"},{"location":"Expert-antirez/An-update-about-Redis-developments-in-2019/#data#structures","text":"Now Redis has Streams, starting with Redis 5. For Redis 6 and 7 what is planned is, to start, to make what we have much more memory efficient by changing the implementations of certain things. However to add new data structures there are a lot of considerations to do. It took me years to realize how to fill the gap, with streams, between lists, pub/sub and sorted sets, in the context of time series and streaming. I really want Redis to be a set of orthogonal data structures that the user can put together, and not a set of tools that are ready to use. Streams are an abstract log, so I think it\u2019s a very worthwhile addition. However other things I\u2019m not completely sure if they are worth to be inside the core without a very long consideration. Anyway in the latest years there was definitely more stress in adding new data structures. HyperLogLogs, more advanced bit operations, streams, blocking sorted set operations (ZPOP* and BZPOP*), and streams are good examples.","title":"Data structures"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/","text":"cnblogs gqtc # redis\u7cfb\u5217 \u4ed6\u7684Redis\u7cfb\u5217\u6587\u7ae0\u5199\u5f97\u975e\u5e38\u4e0d\u9519\u3002 Replication Redis\u6e90\u7801\u89e3\u6790\uff1a15Resis\u4e3b\u4ece\u590d\u5236\u4e4b\u4ece\u8282\u70b9\u6d41\u7a0b Redis\u6e90\u7801\u89e3\u6790\uff1a16Resis\u4e3b\u4ece\u590d\u5236\u4e4b\u4e3b\u8282\u70b9\u7684\u5b8c\u5168\u91cd\u540c\u6b65\u6d41\u7a0b Redis\u6e90\u7801\u89e3\u6790\uff1a17Resis\u4e3b\u4ece\u590d\u5236\u4e4b\u4e3b\u8282\u70b9\u7684\u90e8\u5206\u91cd\u540c\u6b65\u6d41\u7a0b\u53ca\u5176\u4ed6 Sentinel Redis\u6e90\u7801\u89e3\u6790\uff1a20sentinel(\u4e00)\u521d\u59cb\u5316\u3001\u5efa\u94fe Redis\u6e90\u7801\u89e3\u6790\uff1a21sentinel(\u4e8c)\u5b9a\u671f\u53d1\u9001\u6d88\u606f\u3001\u68c0\u6d4b\u4e3b\u89c2\u4e0b\u7ebf Redis\u6e90\u7801\u89e3\u6790\uff1a22sentinel(\u4e09)\u5ba2\u89c2\u4e0b\u7ebf\u4ee5\u53ca\u6545\u969c\u8f6c\u79fb\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9 Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b Redis\u6e90\u7801\u89e3\u6790\uff1a24sentinel(\u4e94)TLIT\u6a21\u5f0f\u3001\u6267\u884c\u811a\u672c Cluster Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b Redis\u6e90\u7801\u89e3\u6790\uff1a26\u96c6\u7fa4(\u4e8c)\u952e\u7684\u5206\u914d\u4e0e\u8fc1\u79fb Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb Redis\u6e90\u7801\u89e3\u6790\uff1a28\u96c6\u7fa4(\u56db)\u624b\u52a8\u6545\u969c\u8f6c\u79fb\u3001\u4ece\u8282\u70b9\u8fc1\u79fb","title":"Introduction"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/#cnblogs#gqtc#redis","text":"\u4ed6\u7684Redis\u7cfb\u5217\u6587\u7ae0\u5199\u5f97\u975e\u5e38\u4e0d\u9519\u3002","title":"cnblogs gqtc # redis\u7cfb\u5217"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/#replication","text":"Redis\u6e90\u7801\u89e3\u6790\uff1a15Resis\u4e3b\u4ece\u590d\u5236\u4e4b\u4ece\u8282\u70b9\u6d41\u7a0b Redis\u6e90\u7801\u89e3\u6790\uff1a16Resis\u4e3b\u4ece\u590d\u5236\u4e4b\u4e3b\u8282\u70b9\u7684\u5b8c\u5168\u91cd\u540c\u6b65\u6d41\u7a0b Redis\u6e90\u7801\u89e3\u6790\uff1a17Resis\u4e3b\u4ece\u590d\u5236\u4e4b\u4e3b\u8282\u70b9\u7684\u90e8\u5206\u91cd\u540c\u6b65\u6d41\u7a0b\u53ca\u5176\u4ed6","title":"Replication"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/#sentinel","text":"Redis\u6e90\u7801\u89e3\u6790\uff1a20sentinel(\u4e00)\u521d\u59cb\u5316\u3001\u5efa\u94fe Redis\u6e90\u7801\u89e3\u6790\uff1a21sentinel(\u4e8c)\u5b9a\u671f\u53d1\u9001\u6d88\u606f\u3001\u68c0\u6d4b\u4e3b\u89c2\u4e0b\u7ebf Redis\u6e90\u7801\u89e3\u6790\uff1a22sentinel(\u4e09)\u5ba2\u89c2\u4e0b\u7ebf\u4ee5\u53ca\u6545\u969c\u8f6c\u79fb\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9 Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b Redis\u6e90\u7801\u89e3\u6790\uff1a24sentinel(\u4e94)TLIT\u6a21\u5f0f\u3001\u6267\u884c\u811a\u672c","title":"Sentinel"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/#cluster","text":"Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b Redis\u6e90\u7801\u89e3\u6790\uff1a26\u96c6\u7fa4(\u4e8c)\u952e\u7684\u5206\u914d\u4e0e\u8fc1\u79fb Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb Redis\u6e90\u7801\u89e3\u6790\uff1a28\u96c6\u7fa4(\u56db)\u624b\u52a8\u6545\u969c\u8f6c\u79fb\u3001\u4ece\u8282\u70b9\u8fc1\u79fb","title":"Cluster"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Cluster/25-%E9%9B%86%E7%BE%A4%E4%B8%80%E6%8F%A1%E6%89%8B-%E5%BF%83%E8%B7%B3%E6%B6%88%E6%81%AF%E4%BB%A5%E5%8F%8A%E4%B8%8B%E7%BA%BF%E6%A3%80%E6%B5%8B/","text":"Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b","title":"Introduction"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Cluster/25-%E9%9B%86%E7%BE%A4%E4%B8%80%E6%8F%A1%E6%89%8B-%E5%BF%83%E8%B7%B3%E6%B6%88%E6%81%AF%E4%BB%A5%E5%8F%8A%E4%B8%8B%E7%BA%BF%E6%A3%80%E6%B5%8B/#redis25","text":"","title":"Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Cluster/26-%E9%9B%86%E7%BE%A4%E4%BA%8C%E9%94%AE%E7%9A%84%E5%88%86%E9%85%8D%E4%B8%8E%E8%BF%81%E7%A7%BB/","text":"Redis\u6e90\u7801\u89e3\u6790\uff1a26\u96c6\u7fa4(\u4e8c)\u952e\u7684\u5206\u914d\u4e0e\u8fc1\u79fb","title":"Introduction"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Cluster/26-%E9%9B%86%E7%BE%A4%E4%BA%8C%E9%94%AE%E7%9A%84%E5%88%86%E9%85%8D%E4%B8%8E%E8%BF%81%E7%A7%BB/#redis26","text":"","title":"Redis\u6e90\u7801\u89e3\u6790\uff1a26\u96c6\u7fa4(\u4e8c)\u952e\u7684\u5206\u914d\u4e0e\u8fc1\u79fb"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Cluster/27-%E9%9B%86%E7%BE%A4%E4%B8%89%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6-%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/","text":"Redis\u6e90\u7801\u89e3\u6790\uff1a26\u96c6\u7fa4(\u4e8c)\u952e\u7684\u5206\u914d\u4e0e\u8fc1\u79fb","title":"Introduction"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Cluster/27-%E9%9B%86%E7%BE%A4%E4%B8%89%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6-%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/#redis26","text":"","title":"Redis\u6e90\u7801\u89e3\u6790\uff1a26\u96c6\u7fa4(\u4e8c)\u952e\u7684\u5206\u914d\u4e0e\u8fc1\u79fb"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/","text":"cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a20sentinel(\u4e00)\u521d\u59cb\u5316\u3001\u5efa\u94fe NOTE: \u4e00\u3001\u9996\u5148\u9700\u8981\u4e86\u89e3\u62d3\u6251\u7ed3\u6784 jianshu Redis\u54e8\u5175\uff08Sentinel\uff09\u6a21\u5f0f sentinel(\u54e8\u5175)\u662fredis\u7684\u9ad8\u53ef\u7528\u89e3\u51b3\u65b9\u6848\u3002\u7531\u4e00\u4e2a\u6216\u591a\u4e2asentinel\u5b9e\u4f8b\u7ec4\u6210\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u53ef\u4ee5\u76d1\u63a7\u4efb\u610f\u591a\u4e2a\u4e3b\u8282\u70b9\uff0c\u4ee5\u53ca\u5b83\u4eec\u5c5e\u4e0b\u7684\u6240\u6709\u4ece\u8282\u70b9\u3002\u5f53\u67d0\u4e2a\u4e3b\u8282\u70b9\u4e0b\u7ebf\u65f6\uff0csentinel\u53ef\u4ee5\u5c06\u4e0b\u7ebf\u4e3b\u8282\u70b9\u5c5e\u4e0b\u7684\u67d0\u4e2a\u4ece\u8282\u70b9\u5347\u7ea7\u4e3a\u65b0\u7684\u4e3b\u8282\u70b9\u3002 \u4e00\uff1a\u54e8\u5175\u8fdb\u7a0b \u4e8c\uff1a\u6570\u636e\u7ed3\u6784 struct sentinelState \u5728\u54e8\u5175\u6a21\u5f0f\u4e2d\uff0c\u6700\u4e3b\u8981\u7684\u6570\u636e\u7ed3\u6784\u5c31\u662f sentinelState \u3002 NOTE: \u5728 sentinel.c \u4e2d\u5b9a\u4e49 /* Main state. */ struct sentinelState { uint64_t current_epoch ; /* Current epoch. */ dict * masters ; /* Dictionary of master sentinelRedisInstances. Key is the instance name, value is the sentinelRedisInstance structure pointer. */ int tilt ; /* Are we in TILT mode? */ int running_scripts ; /* Number of scripts in execution right now. */ mstime_t tilt_start_time ; /* When TITL started. */ mstime_t previous_time ; /* Last time we ran the time handler. */ list * scripts_queue ; /* Queue of user scripts to execute. */ char * announce_ip ; /* IP addr that is gossiped to other sentinels if not NULL. */ int announce_port ; /* Port that is gossiped to other sentinels if non zero. */ } sentinel ; sentinelState::masters \u5728 sentinelState \u7ed3\u6784\u4e2d\uff0c\u6700\u4e3b\u8981\u7684\u6210\u5458\u5c31\u662f\u5b57\u5178 masters \u3002\u8be5\u5b57\u5178\u4e2d\u8bb0\u5f55\u5f53\u524d\u54e8\u5175\u6240\u8981\u76d1\u63a7\u548c\u4ea4\u4e92\u7684\u6240\u6709\u5b9e\u4f8b\u3002\u8fd9\u4e9b\u5b9e\u4f8b\u5305\u62ec\u4e3b\u8282\u70b9\u3001\u4ece\u8282\u70b9\u548c\u5176\u4ed6\u54e8\u5175\u3002 masters \u5b57\u5178\u4ee5\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u4e3akey\uff0c\u4ee5\u4e3b\u8282\u70b9\u5b9e\u4f8b\u7ed3\u6784 sentinelRedisInstance \u4e3avalue\u3002\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u901a\u8fc7\u89e3\u6790\u914d\u7f6e\u6587\u4ef6\u5f97\u5230\u3002 NOTE: \u4e00\u3001\"\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u901a\u8fc7\u89e3\u6790\u914d\u7f6e\u6587\u4ef6\u5f97\u5230\"\u662f\u4ec0\u4e48\u610f\u601d\uff1f \u53c2\u89c1\u4e0b\u9762\u7684\"\u521d\u59cb\u5316\"\u7ae0\u8282\u53ef\u77e5: \u7528\u6237\u9700\u8981\u5728sentinel\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u6307\u5b9a\u7531\u5b83\u76d1\u63a7\u7684master\u8282\u70b9\uff1b struct sentinelRedisInstance \u800c sentinelRedisInstance \u7ed3\u6784\u7684\u5b9a\u4e49\u5982\u4e0b\uff1a typedef struct sentinelRedisInstance { int flags ; /* See SRI_... defines */ char * name ; /* Master name from the point of view of this sentinel. */ char * runid ; /* run ID of this instance. */ uint64_t config_epoch ; /* Configuration epoch. */ sentinelAddr * addr ; /* Master host. */ redisAsyncContext * cc ; /* Hiredis context for commands. */ redisAsyncContext * pc ; /* Hiredis context for Pub / Sub. */ int pending_commands ; /* Number of commands sent waiting for a reply. */ mstime_t cc_conn_time ; /* cc connection time. */ mstime_t pc_conn_time ; /* pc connection time. */ ... /* Master specific. */ dict * sentinels ; /* Other sentinels monitoring the same master. */ dict * slaves ; /* Slaves for this master instance. */ ... /* Slave specific. */ ... struct sentinelRedisInstance * master ; /* Master instance if it's slave. */ ... /* Failover */ ... struct sentinelRedisInstance * promoted_slave ; ... } sentinelRedisInstance ; NOTE: \u4e00\u3001\u5728 sentinel.c \u4e2d\u5b9a\u4e49\uff1b \u5728\u6700\u65b0\u7248\u7684Redis\u4e2d\uff0c\u4e0a\u8ff0\u7ed3\u6784\u4f53\u5df2\u7ecf\u6539\u53d8\u4e86 \u5728\u54e8\u5175\u6a21\u5f0f\u4e2d\uff0c\u6240\u6709\u7684\u4e3b\u8282\u70b9\u3001\u4ece\u8282\u70b9\u4ee5\u53ca\u54e8\u5175\u5b9e\u4f8b\uff0c\u90fd\u662f\u7531 sentinelRedisInstance \u7ed3\u6784\u8868\u793a\u7684\u3002 TCP\u8fde\u63a5 \u54e8\u5175\u4f1a\u4e0e\u5176\u76d1\u63a7\u7684\u6240\u6709\u4e3b\u8282\u70b9\u3001\u8be5\u4e3b\u8282\u70b9\u4e0b\u5c5e\u7684\u6240\u6709\u4ece\u8282\u70b9\uff0c\u4ee5\u53ca\u4e0e\u4e4b\u76d1\u63a7\u76f8\u540c\u4e3b\u8282\u70b9\u7684\u5176\u4ed6\u54e8\u5175\u4e4b\u95f4\u5efa\u7acbTCP\u8fde\u63a5\u3002 \u54e8\u5175\u4e0e\u4e3b\u8282\u70b9\u548c\u4ece\u8282\u70b9\u4e4b\u95f4\u4f1a\u5efa\u7acb\u4e24\u4e2aTCP\u8fde\u63a5\uff0c\u5206\u522b\u7528\u4e8e\u53d1\u9001\u547d\u4ee4\u548c\u8ba2\u9605HELLO\u9891\u9053\uff1b NOTE: \u4e00\u3001\u5bf9\u5e94\u4e86\u5982\u4e0b\u4e24\u4e2a redisAsyncContext redisAsyncContext * cc ; /* Hiredis context for commands. */ redisAsyncContext * pc ; /* Hiredis context for Pub / Sub. */ \u5728\u6700\u65b0\u7248Redis\u4e2d\uff0c\u5b83\u4eec\u5b9a\u4e49\u5728: struct instanceLink \u54e8\u5175\u4e0e\u5176\u4ed6\u54e8\u5175\u4e4b\u95f4\u53ea\u5efa\u7acb\u4e00\u4e2a\u53d1\u9001\u547d\u4ee4\u7684TCP\u8fde\u63a5\uff08\u56e0\u4e3a\u54e8\u5175\u672c\u8eab\u4e0d\u652f\u6301\u8ba2\u9605\u6a21\u5f0f\uff09\uff1b sentinelRedisInstance::cc \u3001 sentinelRedisInstance::pc \u54e8\u5175\u4e0e\u5176\u4ed6\u8282\u70b9\u8fdb\u884c\u901a\u4fe1\uff0c\u4f7f\u7528\u7684\u662fHiredis\u4e2d\u7684\u5f02\u6b65\u65b9\u5f0f\u3002\u56e0\u6b64\uff0c sentinelRedisInstance \u7ed3\u6784\u4e2d\u7684 cc \uff0c\u5c31\u662f\u7528\u4e8e\u547d\u4ee4\u8fde\u63a5\u7684\u5f02\u6b65\u4e0a\u4e0b\u6587\uff1b\u800c\u5176\u4e2d\u7684 pc \uff0c\u5c31\u662f\u7528\u4e8e\u8ba2\u9605\u8fde\u63a5\u7684\u5f02\u6b65\u4e0a\u4e0b\u6587\uff1b \u4e3b\u8282\u70b9\u5b9e\u4f8b: sentinelRedisInstance::sentinels \u3001 sentinelRedisInstance::slaves \u9664\u4e86\u516c\u5171\u90e8\u5206\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u5b9e\u4f8b\u8fd8\u4f1a\u6709\u81ea\u5df1\u7279\u6709\u7684\u5c5e\u6027\u3002\u6bd4\u5982\u5bf9\u4e8e**\u4e3b\u8282\u70b9\u5b9e\u4f8b**\u800c\u8a00\uff0c\u5b83\u7684\u7279\u6709\u5c5e\u6027\u6709\uff1a 1\u3001 sentinels \u5b57\u5178\uff1a\u7528\u4e8e\u8bb0\u5f55\u76d1\u63a7\u76f8\u540c\u4e3b\u8282\u70b9\u5176\u4ed6\u54e8\u5175\u5b9e\u4f8b\u3002\u8be5\u5b57\u5178\u4ee5\u54e8\u5175\u540d\u5b57\u4e3akey\uff0c\u4ee5\u54e8\u5175\u5b9e\u4f8b sentinelRedisInstance \u7ed3\u6784\u4e3akey\uff1b 2\u3001 slaves \u5b57\u5178\uff1a\u7528\u4e8e\u8bb0\u5f55\u8be5\u4e3b\u8282\u70b9\u5b9e\u4f8b\u7684\u6240\u6709\u4ece\u8282\u70b9\u5b9e\u4f8b\u3002\u8be5\u5b57\u5178\u4ee5\u4ece\u8282\u70b9\u540d\u5b57\u4e3akey\uff0c\u4ee5\u4ece\u8282\u70b9\u5b9e\u4f8b sentinelRedisInstance \u7ed3\u6784\u4e3akey\uff1b \u56e0\u6b64\u603b\u7ed3\u800c\u8a00\u5c31\u662f\uff1a sentinelState \u7ed3\u6784\u4e2d\u7684\u5b57\u5178 masters \u4e2d\uff0c\u8bb0\u5f55\u4e86\u672c\u54e8\u5175\u8981\u76d1\u63a7\u7684\u6240\u6709\u4e3b\u8282\u70b9\u5b9e\u4f8b\uff0c\u800c\u5728\u8868\u793a\u6bcf\u4e2a\u4e3b\u8282\u70b9\u5b9e\u4f8b\u7684 sentinelRedisInstance \u7ed3\u6784\u4e2d\uff0c\u5b57\u5178 sentinels \u4e2d\u8bb0\u5f55\u4e86\u76d1\u63a7\u8be5\u4e3b\u8282\u70b9\u7684\u5176\u4ed6\u54e8\u5175\u5b9e\u4f8b\uff0c\u5b57\u5178 slaves \u8bb0\u5f55\u4e86\u8be5\u4e3b\u8282\u70b9\u7684\u6240\u6709\u4e0b\u5c5e\u4ece\u8282\u70b9\u3002 \u8fd9\u79cd\u8bbe\u8ba1\u65b9\u5f0f\u975e\u5e38\u5de7\u5999\uff0c\u4ee5**\u4e3b\u8282\u70b9**\u4e3a\u6838\u5fc3\uff0c\u5c06\u5f53\u524d\u54e8\u5175\u6240\u76d1\u63a7\u7684\u5b9e\u4f8b\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u4e2a\u4e3b\u8282\u70b9\u53ca\u5176\u5c5e\u4e0b\u7684\u4ece\u8282\u70b9\u548c\u54e8\u5175\uff0c\u7ec4\u6210\u4e00\u4e2a\u76d1\u63a7\u5355\u4f4d\uff0c\u4e0d\u540c\u76d1\u63a7\u5355\u4f4d\u4e4b\u95f4\u7684\u6d41\u7a0b\u662f\u76f8\u4e92\u9694\u79bb\u7684\u3002 NOTE: \u4e00\u3001\u8fd9\u662f\u81ea\u7136\u800c\u7136\u7684\u8bbe\u8ba1\uff0c\u56e0\u4e3aRedis sentinel\u7684\u76ee\u7684\u5c31\u662f\u5bf9\u4e3b\u8282\u70b9\u76d1\u63a7\u3001automatic failover\uff1b\u56e0\u6b64\uff0c\u5c31\u9700\u8981\u77e5\u9053\u8fd9\u4e2a\u4e3b\u8282\u70b9\u7684\u4ece\u8282\u70b9\u3001\u5176\u4ed6\u76d1\u63a7\u8fd9\u4e2a\u4e3b\u8282\u70b9\u7684sentinel\uff1b\u663e\u7136\u5b83\u4eec\u5c31\u6784\u6210\u4e86\u4e00\u4e2a\u76d1\u63a7\u5355\u4f4d\uff1b\u663e\u7136\u8fd9\u4e2a\u76d1\u63a7\u5355\u4f4d\u7684\u6838\u5fc3\u662f\"\u4e3b\u8282\u70b9\"\uff0c\u8fd9\u4e2a\u76d1\u63a7\u5355\u4f4d\u5c31\u662f\u4e3a\u4e86\u4fdd\u8bc1\u8fd9\u4e2a\u4e3b\u8282\u70b9\u7684HA\u7684\uff1b \u4e8c\u3001\"\u4e0d\u540c\u76d1\u63a7\u5355\u4f4d\u4e4b\u95f4\u7684\u6d41\u7a0b\u662f\u76f8\u4e92\u9694\u79bb\u7684\" \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728Redis cluster\u88ab\u6807\u51c6\u5316\u4e4b\u524d\uff0c\u6c11\u95f4\u4e5f\u662f\u6709cluster\u65b9\u6848\u7684\uff1b\u5728\u6c11\u95f4\u7684cluster\u65b9\u6848\u4e2d\uff0c\u5c31\u4f1a\u540c\u65f6\u5b58\u5728\u591a\u4e2amaster\uff1b \u4ece\u8282\u70b9\u5b9e\u4f8b \u5bf9\u4e8e**\u4ece\u8282\u70b9\u5b9e\u4f8b**\u800c\u8a00\uff0c sentinelRedisInstance \u7ed3\u6784\u4e2d\u4e5f\u6709\u4e00\u4e9b\u5b83\u6240\u5404\u6709\u7684\u5c5e\u6027\uff0c\u6bd4\u5982 master \u6307\u9488\uff0c\u5c31\u6307\u5411\u4e86\u5b83\u7684\u4e3b\u8282\u70b9\u7684 sentinelRedisInstance \u7ed3\u6784\uff1b sentinelRedisInstance \u7ed3\u6784\u4e2d\u8fd8\u5305\u542b\u4e0e\u6545\u969c\u8f6c\u79fb\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u8fd9\u5728\u5206\u6790\u54e8\u5175\u7684\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\u7684\u4ee3\u7801\u65f6\u4f1a\u4ecb\u7ecd\u3002 \u4e09\uff1a\u521d\u59cb\u5316 \u5728\u54e8\u5175\u6a21\u5f0f\u4e0b\uff0c\u542f\u52a8\u65f6\u5fc5\u987b\u6307\u5b9a\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6\uff0c\u8fd9\u4e5f\u662f\u54e8\u5175\u6a21\u5f0f\u548credis\u670d\u52a1\u5668\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u54e8\u5175\u6a21\u5f0f\u4e0d\u652f\u6301\u547d\u4ee4\u884c\u65b9\u5f0f\u7684\u53c2\u6570\u914d\u7f6e\u3002 NOTE: \u4e3b\u8981\u662f\u6307\u5b9a\u9700\u8981\u7531\u5b83\u76d1\u63a7\u7684master sentinel monitor mymaster 127 .0.0.1 6379 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 sentinel monitor resque 192 .168.1.3 6380 4 sentinel down-after-milliseconds resque 10000 sentinel failover-timeout resque 180000 sentinel parallel-syncs resque 5 \u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u53ea\u9700\u8981\u6307\u5b9a\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u3001ip\u548cport\u4fe1\u606f\uff0c\u800c\u4ece\u8282\u70b9\u548c\u5176\u4ed6\u54e8\u5175\u7684\u4fe1\u606f\uff0c\u90fd\u662f\u5728\u4fe1\u606f\u4ea4\u4e92\u7684\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u53d1\u73b0\u7684\u3002 NOTE: Redis cluster\u4e2d\uff0c\u901a\u8fc7gossip\u6765\u8fdb\u884c\u81ea\u52a8\u53d1\u73b0 sentinel.c:sentinelHandleConfiguration \u5728\u6e90\u4ee3\u7801 sentinel.c \u4e2d\uff0c\u51fd\u6570 sentinelHandleConfiguration \u5c31\u662f\u7528\u4e8e\u89e3\u6790\u54e8\u5175\u914d\u7f6e\u9009\u9879\u7684\u51fd\u6570\u3002 char * sentinelHandleConfiguration ( char ** argv , int argc ) { sentinelRedisInstance * ri ; if ( ! strcasecmp ( argv [ 0 ], \"monitor\" ) && argc == 5 ) { /* monitor <name> <host> <port> <quorum> */ int quorum = atoi ( argv [ 4 ]); if ( quorum <= 0 ) return \"Quorum must be 1 or greater.\" ; if ( createSentinelRedisInstance ( argv [ 1 ], SRI_MASTER , argv [ 2 ], atoi ( argv [ 3 ]), quorum , NULL ) == NULL ) { switch ( errno ) { case EBUSY : return \"Duplicated master name.\" ; case ENOENT : return \"Can't resolve master instance hostname.\" ; case EINVAL : return \"Invalid port number\" ; } } } ... } \u4e0a\u9762\u7684\u4ee3\u7801\uff0c\u5c31\u662f\u6839\u636e\u53c2\u6570\u503c\uff0c\u76f4\u63a5\u8c03\u7528 createSentinelRedisInstance \u51fd\u6570\uff0c\u521b\u5efa\u4e00\u4e2a SRI_MASTER \u6807\u8bb0\u7684\u4e3b\u8282\u70b9\u5b9e\u4f8b\u3002 sentinel.c:createSentinelRedisInstance createSentinelRedisInstance \u51fd\u6570\u7684\u4ee3\u7801\u5982\u4e0b\uff1a sentinelRedisInstance * createSentinelRedisInstance ( char * name , int flags , char * hostname , int port , int quorum , sentinelRedisInstance * master ) { sentinelRedisInstance * ri ; sentinelAddr * addr ; dict * table = NULL ; char slavename [ 128 ], * sdsname ; redisAssert ( flags & ( SRI_MASTER | SRI_SLAVE | SRI_SENTINEL )); redisAssert (( flags & SRI_MASTER ) || master != NULL ); /* Check address validity. */ addr = createSentinelAddr ( hostname , port ); if ( addr == NULL ) return NULL ; /* For slaves and sentinel we use ip:port as name. */ if ( flags & ( SRI_SLAVE | SRI_SENTINEL )) { snprintf ( slavename , sizeof ( slavename ), strchr ( hostname , ':' ) ? \"[%s]:%d\" : \"%s:%d\" , hostname , port ); name = slavename ; } /* Make sure the entry is not duplicated. This may happen when the same * name for a master is used multiple times inside the configuration or * if we try to add multiple times a slave or sentinel with same ip/port * to a master. */ if ( flags & SRI_MASTER ) table = sentinel . masters ; else if ( flags & SRI_SLAVE ) table = master -> slaves ; else if ( flags & SRI_SENTINEL ) table = master -> sentinels ; sdsname = sdsnew ( name ); if ( dictFind ( table , sdsname )) { releaseSentinelAddr ( addr ); sdsfree ( sdsname ); errno = EBUSY ; return NULL ; } /* Create the instance object. */ ri = zmalloc ( sizeof ( * ri )); /* Note that all the instances are started in the disconnected state, * the event loop will take care of connecting them. */ ri -> flags = flags | SRI_DISCONNECTED ; ri -> name = sdsname ; ri -> runid = NULL ; ri -> config_epoch = 0 ; ri -> addr = addr ; ri -> cc = NULL ; ri -> pc = NULL ; ... dictAdd ( table , ri -> name , ri ); return ri ; } \u53c2\u6570 name \u8868\u793a\u8be5\u5b9e\u4f8b\u7684\u540d\u5b57\uff0c\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u914d\u7f6e\u7684\uff1b\u4ece\u8282\u70b9\u548c\u54e8\u5175\u7684\u540d\u5b57\u7531hostname\u548cport\u7ec4\u6210\uff1b \u5982\u679c\u8be5\u5b9e\u4f8b\u4e3a\u4e3b\u8282\u70b9\uff0c\u5219\u53c2\u6570 master \u4e3aNULL\uff0c\u6700\u7ec8\u4f1a\u5c06\u8be5\u5b9e\u4f8b\u5b58\u653e\u5230\u5b57\u5178 sentinel.masters \u4e2d\uff1b \u5982\u679c\u8be5\u5b9e\u4f8b\u4e3a\u4ece\u8282\u70b9\u6216\u54e8\u5175\uff0c\u5219\u53c2\u6570 master \u4e0d\u80fd\u4e3aNULL\uff0c\u5c06\u8be5\u5b9e\u4f8b\u5b58\u653e\u5230\u5b57\u5178 master->slaves \u6216 master->sentinels \u4e2d\uff1b NOTE: \u4e00\u3001\u53c2\u6570 master \u6307\u7684\u662f\u51fd\u6570\u7684\u5165\u53c2 master \u5982\u679c\u5b57\u5178\u4e2d\u5df2\u7ecf\u5b58\u5728\u540c\u540d\u5b9e\u4f8b\uff0c\u5219\u8bbe\u7f6eerrno\u4e3aEBUSY\uff0c\u5e76\u4e14\u8fd4\u56deNULL\uff0c\u8868\u793a\u521b\u5efa\u5b9e\u4f8b\u5931\u8d25\uff1b \u56e0\u6b64\uff0c\u89e3\u6790\u5b8c\u54e8\u5175\u7684\u914d\u7f6e\u6587\u4ef6\u4e4b\u540e\uff0c\u5c31\u5df2\u7ecf\u628a\u6240\u6709\u8981\u76d1\u63a7\u7684\u4e3b\u8282\u70b9\u5b9e\u4f8b\u63d2\u5165\u5230\u5b57\u5178 sentinel.masters \u4e2d\u4e86\u3002\u4e0b\u4e00\u6b65\uff0c\u5c31\u662f\u5f00\u59cb\u5411\u4e3b\u8282\u70b9\u8fdb\u884cTCP\u5efa\u94fe\u4e86\u3002 \u56db\uff1a\u54e8\u5175\u8fdb\u7a0b\u7684\u201c\u4e3b\u51fd\u6570\u201d \u5728\u4ecb\u7ecd\u54e8\u5175\u8fdb\u7a0b\u7684\u5404\u79cd\u6d41\u7a0b\u4e4b\u524d\uff0c\u9700\u8981\u5148\u4e86\u89e3\u4e00\u4e0b\u54e8\u5175\u8fdb\u7a0b\u7684\u201c\u4e3b\u51fd\u6570\u201d\u3002 NOTE: \u4ece\u540e\u9762\u53ef\u77e5\uff0c\"\u54e8\u5175\u8fdb\u7a0b\u7684\u201c\u4e3b\u51fd\u6570\u201d\"\u5c31\u662f sentinelHandleRedisInstance sentinelTimer \u5728redis\u670d\u52a1\u5668\u4e2d\u7684\u5b9a\u65f6\u5668\u51fd\u6570 serverCron \u4e2d\uff0c\u6bcf\u9694100ms\u5c31\u4f1a\u8c03\u7528\u4e00\u6b21 sentinelTimer \u51fd\u6570\u3002\u8be5\u51fd\u6570\u5c31\u662f\u54e8\u5175\u8fdb\u7a0b\u7684\u4e3b\u8981\u5904\u7406\u51fd\u6570\uff0c\u54e8\u5175\u4e2d\u7684\u6240\u6709\u6d41\u7a0b\u90fd\u662f\u5728\u8be5\u51fd\u6570\u4e2d\u5904\u7406\u7684\u3002 void sentinelTimer ( void ) { sentinelCheckTiltCondition (); sentinelHandleDictOfRedisInstances ( sentinel . masters ); sentinelRunPendingScripts (); sentinelCollectTerminatedScripts (); sentinelKillTimedoutScripts (); /* We continuously change the frequency of the Redis \"timer interrupt\" * in order to desynchronize every Sentinel from every other. * This non-determinism avoids that Sentinels started at the same time * exactly continue to stay synchronized asking to be voted at the * same time again and again (resulting in nobody likely winning the * election because of split brain voting). */ server . hz = REDIS_DEFAULT_HZ + rand () % REDIS_DEFAULT_HZ ; } \u6700\u540e\uff0c\u4fee\u6539 server.hz \uff0c\u589e\u52a0\u5176\u968f\u673a\u6027\uff0c\u4ee5\u907f\u514d\u6295\u7968\u9009\u4e3e\u65f6\u53d1\u751f\u51b2\u7a81\uff1b NOTE: /* We continuously change the frequency of the Redis \"timer interrupt\" * in order to desynchronize every Sentinel from every other. * This non-determinism avoids that Sentinels started at the same time * exactly continue to stay synchronized asking to be voted at the * same time again and again (resulting in nobody likely winning the * election because of split brain voting). */ server . hz = REDIS_DEFAULT_HZ + rand () % REDIS_DEFAULT_HZ ; \u8fd9\u662fraft\u7b97\u6cd5\u7684\"\u968f\u673a\u8d85\u65f6\"\u673a\u5236\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u8fdb\u884c\u4e86\u4ecb\u7ecd: 1\u3001csdn \u7528\u52a8\u56fe\u8bb2\u89e3\u5206\u5e03\u5f0f Raft sentinelHandleDictOfRedisInstances sentinelHandleDictOfRedisInstances \u51fd\u6570\uff0c\u662f\u5904\u7406\u8be5\u54e8\u5175\u4e2d\u4fdd\u5b58\u7684\u6240\u6709\u5b9e\u4f8b\u7684\u51fd\u6570\u3002\u5b83\u7684\u4ee3\u7801\u5982\u4e0b\uff1a void sentinelHandleDictOfRedisInstances ( dict * instances ) { dictIterator * di ; dictEntry * de ; sentinelRedisInstance * switch_to_promoted = NULL ; /* There are a number of things we need to perform against every master. */ di = dictGetIterator ( instances ); while (( de = dictNext ( di )) != NULL ) { sentinelRedisInstance * ri = dictGetVal ( de ); sentinelHandleRedisInstance ( ri ); if ( ri -> flags & SRI_MASTER ) { sentinelHandleDictOfRedisInstances ( ri -> slaves ); sentinelHandleDictOfRedisInstances ( ri -> sentinels ); if ( ri -> failover_state == SENTINEL_FAILOVER_STATE_UPDATE_CONFIG ) { switch_to_promoted = ri ; } } } if ( switch_to_promoted ) sentinelFailoverSwitchToPromotedSlave ( switch_to_promoted ); dictReleaseIterator ( di ); } \u51fd\u6570\u7684\u6700\u540e\uff0c\u5982\u679c\u9488\u5bf9\u67d0\u4e2a\u4e3b\u8282\u70b9\uff0c\u53d1\u8d77\u4e86\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\uff0c\u5e76\u4e14\u6d41\u7a0b\u5df2\u7ecf\u5230\u4e86\u6700\u540e\u4e00\u6b65\uff0c\u5219\u4f1a\u8c03\u7528\u51fd\u6570 sentinelFailoverSwitchToPromotedSlave \u8fdb\u884c\u5904\u7406\uff1b sentinelHandleRedisInstance sentinelHandleRedisInstance \u51fd\u6570\uff0c\u5c31\u662f\u76f8\u5f53\u4e8e\u54e8\u5175\u8fdb\u7a0b\u7684\u201c\u4e3b\u51fd\u6570\u201d\u3002\u6709\u5173\u5b9e\u4f8b\u7684\u51e0\u4e4e\u6240\u6709\u52a8\u4f5c\uff0c\u90fd\u5728\u8be5\u51fd\u6570\u4e2d\u8fdb\u884c\u7684\u3002\u8be5\u51fd\u6570\u7684\u4ee3\u7801\u5982\u4e0b\uff1a \u4e94\uff1a\u5efa\u94fe \u54e8\u5175\u5bf9\u4e8e\u5176\u6240\u76d1\u63a7\u7684\u6240\u6709**\u4e3b\u8282\u70b9**\uff0c\u53ca\u5176\u5c5e\u4e0b\u7684\u6240\u6709**\u4ece\u8282\u70b9**\uff0c\u90fd\u4f1a\u5efa\u7acb\u4e24\u4e2a**TCP\u8fde\u63a5**\u3002\u4e00\u4e2a\u7528\u4e8e\u53d1\u9001\u547d\u4ee4\uff0c\u4e00\u4e2a\u7528\u4e8e\u8ba2\u9605\u5176**HELLO\u9891\u9053**\u3002\u800c\u54e8\u5175\u5bf9\u4e8e\u76d1\u63a7**\u540c\u4e00\u4e3b\u8282\u70b9**\u7684\u5176\u4ed6\u54e8\u5175\u5b9e\u4f8b\uff0c\u53ea\u5efa\u7acb\u4e00\u4e2a**\u547d\u4ee4\u8fde\u63a5**\u3002 \u54e8\u5175\u5411\u4e3b\u8282\u70b9\u548c\u4ece\u8282\u70b9\u5efa\u7acb\u7684**\u8ba2\u9605\u8fde\u63a5**\uff0c\u4e3b\u8981\u662f\u4e3a\u4e86\u76d1\u63a7\u540c\u4e00\u4e3b\u8282\u70b9\u7684\u6240\u6709\u54e8\u5175\u4e4b\u95f4\uff0c\u80fd\u591f\u76f8\u4e92\u53d1\u73b0\uff0c\u4ee5\u53ca\u4ea4\u6362\u4fe1\u606f\u3002 NOTE: \u4e3a\u4ec0\u4e48\u8981\u4e13\u95e8\u4f7f\u7528\u4e00\u4e2a\u7279\u6b8a\u7684\u8fde\u63a5\uff1f","title":"Introduction"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#cnblogs#redis20sentinel","text":"NOTE: \u4e00\u3001\u9996\u5148\u9700\u8981\u4e86\u89e3\u62d3\u6251\u7ed3\u6784 jianshu Redis\u54e8\u5175\uff08Sentinel\uff09\u6a21\u5f0f sentinel(\u54e8\u5175)\u662fredis\u7684\u9ad8\u53ef\u7528\u89e3\u51b3\u65b9\u6848\u3002\u7531\u4e00\u4e2a\u6216\u591a\u4e2asentinel\u5b9e\u4f8b\u7ec4\u6210\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u53ef\u4ee5\u76d1\u63a7\u4efb\u610f\u591a\u4e2a\u4e3b\u8282\u70b9\uff0c\u4ee5\u53ca\u5b83\u4eec\u5c5e\u4e0b\u7684\u6240\u6709\u4ece\u8282\u70b9\u3002\u5f53\u67d0\u4e2a\u4e3b\u8282\u70b9\u4e0b\u7ebf\u65f6\uff0csentinel\u53ef\u4ee5\u5c06\u4e0b\u7ebf\u4e3b\u8282\u70b9\u5c5e\u4e0b\u7684\u67d0\u4e2a\u4ece\u8282\u70b9\u5347\u7ea7\u4e3a\u65b0\u7684\u4e3b\u8282\u70b9\u3002","title":"cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a20sentinel(\u4e00)\u521d\u59cb\u5316\u3001\u5efa\u94fe"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#_1","text":"","title":"\u4e00\uff1a\u54e8\u5175\u8fdb\u7a0b"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#_2","text":"","title":"\u4e8c\uff1a\u6570\u636e\u7ed3\u6784"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#struct#sentinelstate","text":"\u5728\u54e8\u5175\u6a21\u5f0f\u4e2d\uff0c\u6700\u4e3b\u8981\u7684\u6570\u636e\u7ed3\u6784\u5c31\u662f sentinelState \u3002 NOTE: \u5728 sentinel.c \u4e2d\u5b9a\u4e49 /* Main state. */ struct sentinelState { uint64_t current_epoch ; /* Current epoch. */ dict * masters ; /* Dictionary of master sentinelRedisInstances. Key is the instance name, value is the sentinelRedisInstance structure pointer. */ int tilt ; /* Are we in TILT mode? */ int running_scripts ; /* Number of scripts in execution right now. */ mstime_t tilt_start_time ; /* When TITL started. */ mstime_t previous_time ; /* Last time we ran the time handler. */ list * scripts_queue ; /* Queue of user scripts to execute. */ char * announce_ip ; /* IP addr that is gossiped to other sentinels if not NULL. */ int announce_port ; /* Port that is gossiped to other sentinels if non zero. */ } sentinel ;","title":"struct sentinelState"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#sentinelstatemasters","text":"\u5728 sentinelState \u7ed3\u6784\u4e2d\uff0c\u6700\u4e3b\u8981\u7684\u6210\u5458\u5c31\u662f\u5b57\u5178 masters \u3002\u8be5\u5b57\u5178\u4e2d\u8bb0\u5f55\u5f53\u524d\u54e8\u5175\u6240\u8981\u76d1\u63a7\u548c\u4ea4\u4e92\u7684\u6240\u6709\u5b9e\u4f8b\u3002\u8fd9\u4e9b\u5b9e\u4f8b\u5305\u62ec\u4e3b\u8282\u70b9\u3001\u4ece\u8282\u70b9\u548c\u5176\u4ed6\u54e8\u5175\u3002 masters \u5b57\u5178\u4ee5\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u4e3akey\uff0c\u4ee5\u4e3b\u8282\u70b9\u5b9e\u4f8b\u7ed3\u6784 sentinelRedisInstance \u4e3avalue\u3002\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u901a\u8fc7\u89e3\u6790\u914d\u7f6e\u6587\u4ef6\u5f97\u5230\u3002 NOTE: \u4e00\u3001\"\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u901a\u8fc7\u89e3\u6790\u914d\u7f6e\u6587\u4ef6\u5f97\u5230\"\u662f\u4ec0\u4e48\u610f\u601d\uff1f \u53c2\u89c1\u4e0b\u9762\u7684\"\u521d\u59cb\u5316\"\u7ae0\u8282\u53ef\u77e5: \u7528\u6237\u9700\u8981\u5728sentinel\u7684\u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u6307\u5b9a\u7531\u5b83\u76d1\u63a7\u7684master\u8282\u70b9\uff1b","title":"sentinelState::masters"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#struct#sentinelredisinstance","text":"\u800c sentinelRedisInstance \u7ed3\u6784\u7684\u5b9a\u4e49\u5982\u4e0b\uff1a typedef struct sentinelRedisInstance { int flags ; /* See SRI_... defines */ char * name ; /* Master name from the point of view of this sentinel. */ char * runid ; /* run ID of this instance. */ uint64_t config_epoch ; /* Configuration epoch. */ sentinelAddr * addr ; /* Master host. */ redisAsyncContext * cc ; /* Hiredis context for commands. */ redisAsyncContext * pc ; /* Hiredis context for Pub / Sub. */ int pending_commands ; /* Number of commands sent waiting for a reply. */ mstime_t cc_conn_time ; /* cc connection time. */ mstime_t pc_conn_time ; /* pc connection time. */ ... /* Master specific. */ dict * sentinels ; /* Other sentinels monitoring the same master. */ dict * slaves ; /* Slaves for this master instance. */ ... /* Slave specific. */ ... struct sentinelRedisInstance * master ; /* Master instance if it's slave. */ ... /* Failover */ ... struct sentinelRedisInstance * promoted_slave ; ... } sentinelRedisInstance ; NOTE: \u4e00\u3001\u5728 sentinel.c \u4e2d\u5b9a\u4e49\uff1b \u5728\u6700\u65b0\u7248\u7684Redis\u4e2d\uff0c\u4e0a\u8ff0\u7ed3\u6784\u4f53\u5df2\u7ecf\u6539\u53d8\u4e86 \u5728\u54e8\u5175\u6a21\u5f0f\u4e2d\uff0c\u6240\u6709\u7684\u4e3b\u8282\u70b9\u3001\u4ece\u8282\u70b9\u4ee5\u53ca\u54e8\u5175\u5b9e\u4f8b\uff0c\u90fd\u662f\u7531 sentinelRedisInstance \u7ed3\u6784\u8868\u793a\u7684\u3002","title":"struct sentinelRedisInstance"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#tcp","text":"\u54e8\u5175\u4f1a\u4e0e\u5176\u76d1\u63a7\u7684\u6240\u6709\u4e3b\u8282\u70b9\u3001\u8be5\u4e3b\u8282\u70b9\u4e0b\u5c5e\u7684\u6240\u6709\u4ece\u8282\u70b9\uff0c\u4ee5\u53ca\u4e0e\u4e4b\u76d1\u63a7\u76f8\u540c\u4e3b\u8282\u70b9\u7684\u5176\u4ed6\u54e8\u5175\u4e4b\u95f4\u5efa\u7acbTCP\u8fde\u63a5\u3002 \u54e8\u5175\u4e0e\u4e3b\u8282\u70b9\u548c\u4ece\u8282\u70b9\u4e4b\u95f4\u4f1a\u5efa\u7acb\u4e24\u4e2aTCP\u8fde\u63a5\uff0c\u5206\u522b\u7528\u4e8e\u53d1\u9001\u547d\u4ee4\u548c\u8ba2\u9605HELLO\u9891\u9053\uff1b NOTE: \u4e00\u3001\u5bf9\u5e94\u4e86\u5982\u4e0b\u4e24\u4e2a redisAsyncContext redisAsyncContext * cc ; /* Hiredis context for commands. */ redisAsyncContext * pc ; /* Hiredis context for Pub / Sub. */ \u5728\u6700\u65b0\u7248Redis\u4e2d\uff0c\u5b83\u4eec\u5b9a\u4e49\u5728: struct instanceLink \u54e8\u5175\u4e0e\u5176\u4ed6\u54e8\u5175\u4e4b\u95f4\u53ea\u5efa\u7acb\u4e00\u4e2a\u53d1\u9001\u547d\u4ee4\u7684TCP\u8fde\u63a5\uff08\u56e0\u4e3a\u54e8\u5175\u672c\u8eab\u4e0d\u652f\u6301\u8ba2\u9605\u6a21\u5f0f\uff09\uff1b","title":"TCP\u8fde\u63a5"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#sentinelredisinstanceccsentinelredisinstancepc","text":"\u54e8\u5175\u4e0e\u5176\u4ed6\u8282\u70b9\u8fdb\u884c\u901a\u4fe1\uff0c\u4f7f\u7528\u7684\u662fHiredis\u4e2d\u7684\u5f02\u6b65\u65b9\u5f0f\u3002\u56e0\u6b64\uff0c sentinelRedisInstance \u7ed3\u6784\u4e2d\u7684 cc \uff0c\u5c31\u662f\u7528\u4e8e\u547d\u4ee4\u8fde\u63a5\u7684\u5f02\u6b65\u4e0a\u4e0b\u6587\uff1b\u800c\u5176\u4e2d\u7684 pc \uff0c\u5c31\u662f\u7528\u4e8e\u8ba2\u9605\u8fde\u63a5\u7684\u5f02\u6b65\u4e0a\u4e0b\u6587\uff1b","title":"sentinelRedisInstance::cc\u3001sentinelRedisInstance::pc"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#sentinelredisinstancesentinelssentinelredisinstanceslaves","text":"\u9664\u4e86\u516c\u5171\u90e8\u5206\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u5b9e\u4f8b\u8fd8\u4f1a\u6709\u81ea\u5df1\u7279\u6709\u7684\u5c5e\u6027\u3002\u6bd4\u5982\u5bf9\u4e8e**\u4e3b\u8282\u70b9\u5b9e\u4f8b**\u800c\u8a00\uff0c\u5b83\u7684\u7279\u6709\u5c5e\u6027\u6709\uff1a 1\u3001 sentinels \u5b57\u5178\uff1a\u7528\u4e8e\u8bb0\u5f55\u76d1\u63a7\u76f8\u540c\u4e3b\u8282\u70b9\u5176\u4ed6\u54e8\u5175\u5b9e\u4f8b\u3002\u8be5\u5b57\u5178\u4ee5\u54e8\u5175\u540d\u5b57\u4e3akey\uff0c\u4ee5\u54e8\u5175\u5b9e\u4f8b sentinelRedisInstance \u7ed3\u6784\u4e3akey\uff1b 2\u3001 slaves \u5b57\u5178\uff1a\u7528\u4e8e\u8bb0\u5f55\u8be5\u4e3b\u8282\u70b9\u5b9e\u4f8b\u7684\u6240\u6709\u4ece\u8282\u70b9\u5b9e\u4f8b\u3002\u8be5\u5b57\u5178\u4ee5\u4ece\u8282\u70b9\u540d\u5b57\u4e3akey\uff0c\u4ee5\u4ece\u8282\u70b9\u5b9e\u4f8b sentinelRedisInstance \u7ed3\u6784\u4e3akey\uff1b \u56e0\u6b64\u603b\u7ed3\u800c\u8a00\u5c31\u662f\uff1a sentinelState \u7ed3\u6784\u4e2d\u7684\u5b57\u5178 masters \u4e2d\uff0c\u8bb0\u5f55\u4e86\u672c\u54e8\u5175\u8981\u76d1\u63a7\u7684\u6240\u6709\u4e3b\u8282\u70b9\u5b9e\u4f8b\uff0c\u800c\u5728\u8868\u793a\u6bcf\u4e2a\u4e3b\u8282\u70b9\u5b9e\u4f8b\u7684 sentinelRedisInstance \u7ed3\u6784\u4e2d\uff0c\u5b57\u5178 sentinels \u4e2d\u8bb0\u5f55\u4e86\u76d1\u63a7\u8be5\u4e3b\u8282\u70b9\u7684\u5176\u4ed6\u54e8\u5175\u5b9e\u4f8b\uff0c\u5b57\u5178 slaves \u8bb0\u5f55\u4e86\u8be5\u4e3b\u8282\u70b9\u7684\u6240\u6709\u4e0b\u5c5e\u4ece\u8282\u70b9\u3002 \u8fd9\u79cd\u8bbe\u8ba1\u65b9\u5f0f\u975e\u5e38\u5de7\u5999\uff0c\u4ee5**\u4e3b\u8282\u70b9**\u4e3a\u6838\u5fc3\uff0c\u5c06\u5f53\u524d\u54e8\u5175\u6240\u76d1\u63a7\u7684\u5b9e\u4f8b\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u4e2a\u4e3b\u8282\u70b9\u53ca\u5176\u5c5e\u4e0b\u7684\u4ece\u8282\u70b9\u548c\u54e8\u5175\uff0c\u7ec4\u6210\u4e00\u4e2a\u76d1\u63a7\u5355\u4f4d\uff0c\u4e0d\u540c\u76d1\u63a7\u5355\u4f4d\u4e4b\u95f4\u7684\u6d41\u7a0b\u662f\u76f8\u4e92\u9694\u79bb\u7684\u3002 NOTE: \u4e00\u3001\u8fd9\u662f\u81ea\u7136\u800c\u7136\u7684\u8bbe\u8ba1\uff0c\u56e0\u4e3aRedis sentinel\u7684\u76ee\u7684\u5c31\u662f\u5bf9\u4e3b\u8282\u70b9\u76d1\u63a7\u3001automatic failover\uff1b\u56e0\u6b64\uff0c\u5c31\u9700\u8981\u77e5\u9053\u8fd9\u4e2a\u4e3b\u8282\u70b9\u7684\u4ece\u8282\u70b9\u3001\u5176\u4ed6\u76d1\u63a7\u8fd9\u4e2a\u4e3b\u8282\u70b9\u7684sentinel\uff1b\u663e\u7136\u5b83\u4eec\u5c31\u6784\u6210\u4e86\u4e00\u4e2a\u76d1\u63a7\u5355\u4f4d\uff1b\u663e\u7136\u8fd9\u4e2a\u76d1\u63a7\u5355\u4f4d\u7684\u6838\u5fc3\u662f\"\u4e3b\u8282\u70b9\"\uff0c\u8fd9\u4e2a\u76d1\u63a7\u5355\u4f4d\u5c31\u662f\u4e3a\u4e86\u4fdd\u8bc1\u8fd9\u4e2a\u4e3b\u8282\u70b9\u7684HA\u7684\uff1b \u4e8c\u3001\"\u4e0d\u540c\u76d1\u63a7\u5355\u4f4d\u4e4b\u95f4\u7684\u6d41\u7a0b\u662f\u76f8\u4e92\u9694\u79bb\u7684\" \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728Redis cluster\u88ab\u6807\u51c6\u5316\u4e4b\u524d\uff0c\u6c11\u95f4\u4e5f\u662f\u6709cluster\u65b9\u6848\u7684\uff1b\u5728\u6c11\u95f4\u7684cluster\u65b9\u6848\u4e2d\uff0c\u5c31\u4f1a\u540c\u65f6\u5b58\u5728\u591a\u4e2amaster\uff1b","title":"\u4e3b\u8282\u70b9\u5b9e\u4f8b: sentinelRedisInstance::sentinels\u3001sentinelRedisInstance::slaves"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#_3","text":"\u5bf9\u4e8e**\u4ece\u8282\u70b9\u5b9e\u4f8b**\u800c\u8a00\uff0c sentinelRedisInstance \u7ed3\u6784\u4e2d\u4e5f\u6709\u4e00\u4e9b\u5b83\u6240\u5404\u6709\u7684\u5c5e\u6027\uff0c\u6bd4\u5982 master \u6307\u9488\uff0c\u5c31\u6307\u5411\u4e86\u5b83\u7684\u4e3b\u8282\u70b9\u7684 sentinelRedisInstance \u7ed3\u6784\uff1b sentinelRedisInstance \u7ed3\u6784\u4e2d\u8fd8\u5305\u542b\u4e0e\u6545\u969c\u8f6c\u79fb\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u8fd9\u5728\u5206\u6790\u54e8\u5175\u7684\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\u7684\u4ee3\u7801\u65f6\u4f1a\u4ecb\u7ecd\u3002","title":"\u4ece\u8282\u70b9\u5b9e\u4f8b"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#_4","text":"\u5728\u54e8\u5175\u6a21\u5f0f\u4e0b\uff0c\u542f\u52a8\u65f6\u5fc5\u987b\u6307\u5b9a\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6\uff0c\u8fd9\u4e5f\u662f\u54e8\u5175\u6a21\u5f0f\u548credis\u670d\u52a1\u5668\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u54e8\u5175\u6a21\u5f0f\u4e0d\u652f\u6301\u547d\u4ee4\u884c\u65b9\u5f0f\u7684\u53c2\u6570\u914d\u7f6e\u3002 NOTE: \u4e3b\u8981\u662f\u6307\u5b9a\u9700\u8981\u7531\u5b83\u76d1\u63a7\u7684master sentinel monitor mymaster 127 .0.0.1 6379 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 sentinel monitor resque 192 .168.1.3 6380 4 sentinel down-after-milliseconds resque 10000 sentinel failover-timeout resque 180000 sentinel parallel-syncs resque 5 \u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\uff0c\u53ea\u9700\u8981\u6307\u5b9a\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u3001ip\u548cport\u4fe1\u606f\uff0c\u800c\u4ece\u8282\u70b9\u548c\u5176\u4ed6\u54e8\u5175\u7684\u4fe1\u606f\uff0c\u90fd\u662f\u5728\u4fe1\u606f\u4ea4\u4e92\u7684\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u53d1\u73b0\u7684\u3002 NOTE: Redis cluster\u4e2d\uff0c\u901a\u8fc7gossip\u6765\u8fdb\u884c\u81ea\u52a8\u53d1\u73b0","title":"\u4e09\uff1a\u521d\u59cb\u5316"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#sentinelcsentinelhandleconfiguration","text":"\u5728\u6e90\u4ee3\u7801 sentinel.c \u4e2d\uff0c\u51fd\u6570 sentinelHandleConfiguration \u5c31\u662f\u7528\u4e8e\u89e3\u6790\u54e8\u5175\u914d\u7f6e\u9009\u9879\u7684\u51fd\u6570\u3002 char * sentinelHandleConfiguration ( char ** argv , int argc ) { sentinelRedisInstance * ri ; if ( ! strcasecmp ( argv [ 0 ], \"monitor\" ) && argc == 5 ) { /* monitor <name> <host> <port> <quorum> */ int quorum = atoi ( argv [ 4 ]); if ( quorum <= 0 ) return \"Quorum must be 1 or greater.\" ; if ( createSentinelRedisInstance ( argv [ 1 ], SRI_MASTER , argv [ 2 ], atoi ( argv [ 3 ]), quorum , NULL ) == NULL ) { switch ( errno ) { case EBUSY : return \"Duplicated master name.\" ; case ENOENT : return \"Can't resolve master instance hostname.\" ; case EINVAL : return \"Invalid port number\" ; } } } ... } \u4e0a\u9762\u7684\u4ee3\u7801\uff0c\u5c31\u662f\u6839\u636e\u53c2\u6570\u503c\uff0c\u76f4\u63a5\u8c03\u7528 createSentinelRedisInstance \u51fd\u6570\uff0c\u521b\u5efa\u4e00\u4e2a SRI_MASTER \u6807\u8bb0\u7684\u4e3b\u8282\u70b9\u5b9e\u4f8b\u3002","title":"sentinel.c:sentinelHandleConfiguration"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#sentinelccreatesentinelredisinstance","text":"createSentinelRedisInstance \u51fd\u6570\u7684\u4ee3\u7801\u5982\u4e0b\uff1a sentinelRedisInstance * createSentinelRedisInstance ( char * name , int flags , char * hostname , int port , int quorum , sentinelRedisInstance * master ) { sentinelRedisInstance * ri ; sentinelAddr * addr ; dict * table = NULL ; char slavename [ 128 ], * sdsname ; redisAssert ( flags & ( SRI_MASTER | SRI_SLAVE | SRI_SENTINEL )); redisAssert (( flags & SRI_MASTER ) || master != NULL ); /* Check address validity. */ addr = createSentinelAddr ( hostname , port ); if ( addr == NULL ) return NULL ; /* For slaves and sentinel we use ip:port as name. */ if ( flags & ( SRI_SLAVE | SRI_SENTINEL )) { snprintf ( slavename , sizeof ( slavename ), strchr ( hostname , ':' ) ? \"[%s]:%d\" : \"%s:%d\" , hostname , port ); name = slavename ; } /* Make sure the entry is not duplicated. This may happen when the same * name for a master is used multiple times inside the configuration or * if we try to add multiple times a slave or sentinel with same ip/port * to a master. */ if ( flags & SRI_MASTER ) table = sentinel . masters ; else if ( flags & SRI_SLAVE ) table = master -> slaves ; else if ( flags & SRI_SENTINEL ) table = master -> sentinels ; sdsname = sdsnew ( name ); if ( dictFind ( table , sdsname )) { releaseSentinelAddr ( addr ); sdsfree ( sdsname ); errno = EBUSY ; return NULL ; } /* Create the instance object. */ ri = zmalloc ( sizeof ( * ri )); /* Note that all the instances are started in the disconnected state, * the event loop will take care of connecting them. */ ri -> flags = flags | SRI_DISCONNECTED ; ri -> name = sdsname ; ri -> runid = NULL ; ri -> config_epoch = 0 ; ri -> addr = addr ; ri -> cc = NULL ; ri -> pc = NULL ; ... dictAdd ( table , ri -> name , ri ); return ri ; } \u53c2\u6570 name \u8868\u793a\u8be5\u5b9e\u4f8b\u7684\u540d\u5b57\uff0c\u4e3b\u8282\u70b9\u7684\u540d\u5b57\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u914d\u7f6e\u7684\uff1b\u4ece\u8282\u70b9\u548c\u54e8\u5175\u7684\u540d\u5b57\u7531hostname\u548cport\u7ec4\u6210\uff1b \u5982\u679c\u8be5\u5b9e\u4f8b\u4e3a\u4e3b\u8282\u70b9\uff0c\u5219\u53c2\u6570 master \u4e3aNULL\uff0c\u6700\u7ec8\u4f1a\u5c06\u8be5\u5b9e\u4f8b\u5b58\u653e\u5230\u5b57\u5178 sentinel.masters \u4e2d\uff1b \u5982\u679c\u8be5\u5b9e\u4f8b\u4e3a\u4ece\u8282\u70b9\u6216\u54e8\u5175\uff0c\u5219\u53c2\u6570 master \u4e0d\u80fd\u4e3aNULL\uff0c\u5c06\u8be5\u5b9e\u4f8b\u5b58\u653e\u5230\u5b57\u5178 master->slaves \u6216 master->sentinels \u4e2d\uff1b NOTE: \u4e00\u3001\u53c2\u6570 master \u6307\u7684\u662f\u51fd\u6570\u7684\u5165\u53c2 master \u5982\u679c\u5b57\u5178\u4e2d\u5df2\u7ecf\u5b58\u5728\u540c\u540d\u5b9e\u4f8b\uff0c\u5219\u8bbe\u7f6eerrno\u4e3aEBUSY\uff0c\u5e76\u4e14\u8fd4\u56deNULL\uff0c\u8868\u793a\u521b\u5efa\u5b9e\u4f8b\u5931\u8d25\uff1b \u56e0\u6b64\uff0c\u89e3\u6790\u5b8c\u54e8\u5175\u7684\u914d\u7f6e\u6587\u4ef6\u4e4b\u540e\uff0c\u5c31\u5df2\u7ecf\u628a\u6240\u6709\u8981\u76d1\u63a7\u7684\u4e3b\u8282\u70b9\u5b9e\u4f8b\u63d2\u5165\u5230\u5b57\u5178 sentinel.masters \u4e2d\u4e86\u3002\u4e0b\u4e00\u6b65\uff0c\u5c31\u662f\u5f00\u59cb\u5411\u4e3b\u8282\u70b9\u8fdb\u884cTCP\u5efa\u94fe\u4e86\u3002","title":"sentinel.c:createSentinelRedisInstance"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#_5","text":"\u5728\u4ecb\u7ecd\u54e8\u5175\u8fdb\u7a0b\u7684\u5404\u79cd\u6d41\u7a0b\u4e4b\u524d\uff0c\u9700\u8981\u5148\u4e86\u89e3\u4e00\u4e0b\u54e8\u5175\u8fdb\u7a0b\u7684\u201c\u4e3b\u51fd\u6570\u201d\u3002 NOTE: \u4ece\u540e\u9762\u53ef\u77e5\uff0c\"\u54e8\u5175\u8fdb\u7a0b\u7684\u201c\u4e3b\u51fd\u6570\u201d\"\u5c31\u662f sentinelHandleRedisInstance","title":"\u56db\uff1a\u54e8\u5175\u8fdb\u7a0b\u7684\u201c\u4e3b\u51fd\u6570\u201d"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#sentineltimer","text":"\u5728redis\u670d\u52a1\u5668\u4e2d\u7684\u5b9a\u65f6\u5668\u51fd\u6570 serverCron \u4e2d\uff0c\u6bcf\u9694100ms\u5c31\u4f1a\u8c03\u7528\u4e00\u6b21 sentinelTimer \u51fd\u6570\u3002\u8be5\u51fd\u6570\u5c31\u662f\u54e8\u5175\u8fdb\u7a0b\u7684\u4e3b\u8981\u5904\u7406\u51fd\u6570\uff0c\u54e8\u5175\u4e2d\u7684\u6240\u6709\u6d41\u7a0b\u90fd\u662f\u5728\u8be5\u51fd\u6570\u4e2d\u5904\u7406\u7684\u3002 void sentinelTimer ( void ) { sentinelCheckTiltCondition (); sentinelHandleDictOfRedisInstances ( sentinel . masters ); sentinelRunPendingScripts (); sentinelCollectTerminatedScripts (); sentinelKillTimedoutScripts (); /* We continuously change the frequency of the Redis \"timer interrupt\" * in order to desynchronize every Sentinel from every other. * This non-determinism avoids that Sentinels started at the same time * exactly continue to stay synchronized asking to be voted at the * same time again and again (resulting in nobody likely winning the * election because of split brain voting). */ server . hz = REDIS_DEFAULT_HZ + rand () % REDIS_DEFAULT_HZ ; } \u6700\u540e\uff0c\u4fee\u6539 server.hz \uff0c\u589e\u52a0\u5176\u968f\u673a\u6027\uff0c\u4ee5\u907f\u514d\u6295\u7968\u9009\u4e3e\u65f6\u53d1\u751f\u51b2\u7a81\uff1b NOTE: /* We continuously change the frequency of the Redis \"timer interrupt\" * in order to desynchronize every Sentinel from every other. * This non-determinism avoids that Sentinels started at the same time * exactly continue to stay synchronized asking to be voted at the * same time again and again (resulting in nobody likely winning the * election because of split brain voting). */ server . hz = REDIS_DEFAULT_HZ + rand () % REDIS_DEFAULT_HZ ; \u8fd9\u662fraft\u7b97\u6cd5\u7684\"\u968f\u673a\u8d85\u65f6\"\u673a\u5236\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u8fdb\u884c\u4e86\u4ecb\u7ecd: 1\u3001csdn \u7528\u52a8\u56fe\u8bb2\u89e3\u5206\u5e03\u5f0f Raft","title":"sentinelTimer"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#sentinelhandledictofredisinstances","text":"sentinelHandleDictOfRedisInstances \u51fd\u6570\uff0c\u662f\u5904\u7406\u8be5\u54e8\u5175\u4e2d\u4fdd\u5b58\u7684\u6240\u6709\u5b9e\u4f8b\u7684\u51fd\u6570\u3002\u5b83\u7684\u4ee3\u7801\u5982\u4e0b\uff1a void sentinelHandleDictOfRedisInstances ( dict * instances ) { dictIterator * di ; dictEntry * de ; sentinelRedisInstance * switch_to_promoted = NULL ; /* There are a number of things we need to perform against every master. */ di = dictGetIterator ( instances ); while (( de = dictNext ( di )) != NULL ) { sentinelRedisInstance * ri = dictGetVal ( de ); sentinelHandleRedisInstance ( ri ); if ( ri -> flags & SRI_MASTER ) { sentinelHandleDictOfRedisInstances ( ri -> slaves ); sentinelHandleDictOfRedisInstances ( ri -> sentinels ); if ( ri -> failover_state == SENTINEL_FAILOVER_STATE_UPDATE_CONFIG ) { switch_to_promoted = ri ; } } } if ( switch_to_promoted ) sentinelFailoverSwitchToPromotedSlave ( switch_to_promoted ); dictReleaseIterator ( di ); } \u51fd\u6570\u7684\u6700\u540e\uff0c\u5982\u679c\u9488\u5bf9\u67d0\u4e2a\u4e3b\u8282\u70b9\uff0c\u53d1\u8d77\u4e86\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\uff0c\u5e76\u4e14\u6d41\u7a0b\u5df2\u7ecf\u5230\u4e86\u6700\u540e\u4e00\u6b65\uff0c\u5219\u4f1a\u8c03\u7528\u51fd\u6570 sentinelFailoverSwitchToPromotedSlave \u8fdb\u884c\u5904\u7406\uff1b","title":"sentinelHandleDictOfRedisInstances"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#sentinelhandleredisinstance","text":"sentinelHandleRedisInstance \u51fd\u6570\uff0c\u5c31\u662f\u76f8\u5f53\u4e8e\u54e8\u5175\u8fdb\u7a0b\u7684\u201c\u4e3b\u51fd\u6570\u201d\u3002\u6709\u5173\u5b9e\u4f8b\u7684\u51e0\u4e4e\u6240\u6709\u52a8\u4f5c\uff0c\u90fd\u5728\u8be5\u51fd\u6570\u4e2d\u8fdb\u884c\u7684\u3002\u8be5\u51fd\u6570\u7684\u4ee3\u7801\u5982\u4e0b\uff1a","title":"sentinelHandleRedisInstance"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/20-sentinel%E4%B8%80%E5%88%9D%E5%A7%8B%E5%8C%96-%E5%BB%BA%E9%93%BE/#_6","text":"\u54e8\u5175\u5bf9\u4e8e\u5176\u6240\u76d1\u63a7\u7684\u6240\u6709**\u4e3b\u8282\u70b9**\uff0c\u53ca\u5176\u5c5e\u4e0b\u7684\u6240\u6709**\u4ece\u8282\u70b9**\uff0c\u90fd\u4f1a\u5efa\u7acb\u4e24\u4e2a**TCP\u8fde\u63a5**\u3002\u4e00\u4e2a\u7528\u4e8e\u53d1\u9001\u547d\u4ee4\uff0c\u4e00\u4e2a\u7528\u4e8e\u8ba2\u9605\u5176**HELLO\u9891\u9053**\u3002\u800c\u54e8\u5175\u5bf9\u4e8e\u76d1\u63a7**\u540c\u4e00\u4e3b\u8282\u70b9**\u7684\u5176\u4ed6\u54e8\u5175\u5b9e\u4f8b\uff0c\u53ea\u5efa\u7acb\u4e00\u4e2a**\u547d\u4ee4\u8fde\u63a5**\u3002 \u54e8\u5175\u5411\u4e3b\u8282\u70b9\u548c\u4ece\u8282\u70b9\u5efa\u7acb\u7684**\u8ba2\u9605\u8fde\u63a5**\uff0c\u4e3b\u8981\u662f\u4e3a\u4e86\u76d1\u63a7\u540c\u4e00\u4e3b\u8282\u70b9\u7684\u6240\u6709\u54e8\u5175\u4e4b\u95f4\uff0c\u80fd\u591f\u76f8\u4e92\u53d1\u73b0\uff0c\u4ee5\u53ca\u4ea4\u6362\u4fe1\u606f\u3002 NOTE: \u4e3a\u4ec0\u4e48\u8981\u4e13\u95e8\u4f7f\u7528\u4e00\u4e2a\u7279\u6b8a\u7684\u8fde\u63a5\uff1f","title":"\u4e94\uff1a\u5efa\u94fe"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/","text":"cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a22sentinel(\u4e09)\u5ba2\u89c2\u4e0b\u7ebf\u4ee5\u53ca\u6545\u969c\u8f6c\u79fb\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9 \u516b\uff1a\u5224\u65ad\u5b9e\u4f8b\u662f\u5426\u5ba2\u89c2\u4e0b\u7ebf \u5f53\u524d\u54e8\u5175\u4e00\u65e6\u76d1\u6d4b\u5230\u67d0\u4e2a\u4e3b\u8282\u70b9\u5b9e\u4f8b\u4e3b\u89c2\u4e0b\u7ebf\u4e4b\u540e\uff0c\u5c31\u4f1a\u5411\u5176\u4ed6\u54e8\u5175\u53d1\u9001\u201dis-master-down-by-addr\u201d\u547d\u4ee4\uff0c\u8be2\u95ee\u5176\u4ed6\u54e8\u5175\u662f\u5426\u4e5f\u8ba4\u4e3a\u8be5\u4e3b\u8282\u70b9\u4e3b\u89c2\u4e0b\u7ebf\u4e86\u3002\u5982\u679c\u6709\u8d85\u8fc7quorum\u4e2a\u54e8\u5175\uff08\u5305\u62ec\u5f53\u524d\u54e8\u5175\uff09\u53cd\u9988\uff0c\u90fd\u8ba4\u4e3a\u8be5\u4e3b\u8282\u70b9\u4e3b\u89c2\u4e0b\u7ebf\u4e86\uff0c\u5219\u5f53\u524d\u54e8\u5175\u5c31\u5c06\u8be5\u4e3b\u8282\u70b9\u5b9e\u4f8b\u6807\u8bb0\u4e3a\u5ba2\u89c2\u4e0b\u7ebf\u3002 1\uff1a\u53d1\u9001\u201dis-master-down-by-addr\u201d\u547d\u4ee4 2\uff1a\u5176\u4ed6\u54e8\u5175\u6536\u5230\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u540e\u7684\u5904\u7406 3\uff1a\u54e8\u5175\u6536\u5230\u5176\u4ed6\u54e8\u5175\u7684\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u56de\u590d\u4fe1\u606f\u540e\u7684\u5904\u7406 4\uff1a\u5224\u65ad\u5b9e\u4f8b\u662f\u5426\u5ba2\u89c2\u4e0b\u7ebf \u4e5d\uff1a\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9 1\uff1a\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b 2\uff1a\u9009\u4e3e\u9886\u5bfc\u8282\u70b9\u539f\u7406 3\uff1a\u5224\u65ad\u662f\u5426\u5f00\u59cb\u6545\u969c\u8f6c\u79fb 4\uff1a\u5f00\u59cb\u65b0\u4e00\u8f6e\u7684\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b 5\uff1a\u53d1\u9001\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u8fdb\u884c\u62c9\u7968 6\uff1a\u5176\u4ed6\u54e8\u5175\u6536\u5230\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u540e\u8fdb\u884c\u6295\u7968 7\uff1a\u54e8\u5175\u6536\u5230\u5176\u4ed6\u54e8\u5175\u7684\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u56de\u590d\u4fe1\u606f\u540e\u7684\u5904\u7406 8\uff1a\u7edf\u8ba1\u6295\u7968","title":"Introduction"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#cnblogs#redis22sentinel","text":"","title":"cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a22sentinel(\u4e09)\u5ba2\u89c2\u4e0b\u7ebf\u4ee5\u53ca\u6545\u969c\u8f6c\u79fb\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#_1","text":"\u5f53\u524d\u54e8\u5175\u4e00\u65e6\u76d1\u6d4b\u5230\u67d0\u4e2a\u4e3b\u8282\u70b9\u5b9e\u4f8b\u4e3b\u89c2\u4e0b\u7ebf\u4e4b\u540e\uff0c\u5c31\u4f1a\u5411\u5176\u4ed6\u54e8\u5175\u53d1\u9001\u201dis-master-down-by-addr\u201d\u547d\u4ee4\uff0c\u8be2\u95ee\u5176\u4ed6\u54e8\u5175\u662f\u5426\u4e5f\u8ba4\u4e3a\u8be5\u4e3b\u8282\u70b9\u4e3b\u89c2\u4e0b\u7ebf\u4e86\u3002\u5982\u679c\u6709\u8d85\u8fc7quorum\u4e2a\u54e8\u5175\uff08\u5305\u62ec\u5f53\u524d\u54e8\u5175\uff09\u53cd\u9988\uff0c\u90fd\u8ba4\u4e3a\u8be5\u4e3b\u8282\u70b9\u4e3b\u89c2\u4e0b\u7ebf\u4e86\uff0c\u5219\u5f53\u524d\u54e8\u5175\u5c31\u5c06\u8be5\u4e3b\u8282\u70b9\u5b9e\u4f8b\u6807\u8bb0\u4e3a\u5ba2\u89c2\u4e0b\u7ebf\u3002","title":"\u516b\uff1a\u5224\u65ad\u5b9e\u4f8b\u662f\u5426\u5ba2\u89c2\u4e0b\u7ebf"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#1is-master-down-by-addr","text":"","title":"1\uff1a\u53d1\u9001\u201dis-master-down-by-addr\u201d\u547d\u4ee4"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#2is-master-down-by-addr","text":"","title":"2\uff1a\u5176\u4ed6\u54e8\u5175\u6536\u5230\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u540e\u7684\u5904\u7406"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#3is-master-down-by-addr","text":"","title":"3\uff1a\u54e8\u5175\u6536\u5230\u5176\u4ed6\u54e8\u5175\u7684\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u56de\u590d\u4fe1\u606f\u540e\u7684\u5904\u7406"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#4","text":"","title":"4\uff1a\u5224\u65ad\u5b9e\u4f8b\u662f\u5426\u5ba2\u89c2\u4e0b\u7ebf"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#_2","text":"","title":"\u4e5d\uff1a\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#1","text":"","title":"1\uff1a\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#2","text":"","title":"2\uff1a\u9009\u4e3e\u9886\u5bfc\u8282\u70b9\u539f\u7406"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#3","text":"","title":"3\uff1a\u5224\u65ad\u662f\u5426\u5f00\u59cb\u6545\u969c\u8f6c\u79fb"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#4_1","text":"","title":"4\uff1a\u5f00\u59cb\u65b0\u4e00\u8f6e\u7684\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#5is-master-down-by-addr","text":"","title":"5\uff1a\u53d1\u9001\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u8fdb\u884c\u62c9\u7968"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#6is-master-down-by-addr","text":"","title":"6\uff1a\u5176\u4ed6\u54e8\u5175\u6536\u5230\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u540e\u8fdb\u884c\u6295\u7968"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#7is-master-down-by-addr","text":"","title":"7\uff1a\u54e8\u5175\u6536\u5230\u5176\u4ed6\u54e8\u5175\u7684\u201dis-master-down-by-addr\u201d\u547d\u4ee4\u56de\u590d\u4fe1\u606f\u540e\u7684\u5904\u7406"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/22-sentinel%E4%B8%89%E5%AE%A2%E8%A7%82%E4%B8%8B%E7%BA%BF%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E4%B9%8B%E9%80%89%E4%B8%BE%E9%A2%86%E5%AF%BC%E8%8A%82%E7%82%B9/#8","text":"","title":"8\uff1a\u7edf\u8ba1\u6295\u7968"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/23-sentinel%E5%9B%9B%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E6%B5%81%E7%A8%8B/","text":"cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b","title":"Introduction"},{"location":"Expert-cnblogs-gqtc-redis%E7%B3%BB%E5%88%97/Sentinel/23-sentinel%E5%9B%9B%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E6%B5%81%E7%A8%8B/#cnblogs#redis23sentinel","text":"","title":"cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b"},{"location":"Expert-cnblogs-%E5%B0%8F%E4%B8%8D%E7%82%B9%E5%95%8A/","text":"cnblogs \u5c0f\u4e0d\u70b9\u554a # redis Redis\u7cfb\u5217\u516b\uff1aredis\u4e3b\u4ece\u590d\u5236\u548c\u54e8\u5175 Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528 Redis\u7cfb\u5217\u5341\uff1a\u7f13\u5b58\u96ea\u5d29\u3001\u7f13\u5b58\u7a7f\u900f\u3001\u7f13\u5b58\u9884\u70ed\u3001\u7f13\u5b58\u66f4\u65b0\u3001\u7f13\u5b58\u964d\u7ea7","title":"Introduction"},{"location":"Expert-cnblogs-%E5%B0%8F%E4%B8%8D%E7%82%B9%E5%95%8A/#cnblogs#redis","text":"Redis\u7cfb\u5217\u516b\uff1aredis\u4e3b\u4ece\u590d\u5236\u548c\u54e8\u5175 Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528 Redis\u7cfb\u5217\u5341\uff1a\u7f13\u5b58\u96ea\u5d29\u3001\u7f13\u5b58\u7a7f\u900f\u3001\u7f13\u5b58\u9884\u70ed\u3001\u7f13\u5b58\u66f4\u65b0\u3001\u7f13\u5b58\u964d\u7ea7","title":"cnblogs \u5c0f\u4e0d\u70b9\u554a # redis"},{"location":"Expert-cnblogs-%E5%B0%8F%E4%B8%8D%E7%82%B9%E5%95%8A/Redis%E7%B3%BB%E5%88%97%E4%B9%9D-redis%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8/","text":"cnblogs Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528","title":"Introduction"},{"location":"Expert-cnblogs-%E5%B0%8F%E4%B8%8D%E7%82%B9%E5%95%8A/Redis%E7%B3%BB%E5%88%97%E4%B9%9D-redis%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8/#cnblogs#redisredis","text":"","title":"cnblogs Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528"},{"location":"HA/","text":"HA Redis\u7684HA\u9700\u8981\u7531\u5982\u4e0b\u4e09\u8005\u534f\u4f5c\u6765\u5b9e\u73b0: \u4e00\u3001API \u4e8c\u3001Sentinel cluster: \u529f\u80fd: 1\u3001\u4e0b\u7ebf\u68c0\u67e5\u3001automatic failover(\u540e\u9762\u4f1a\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd) 2\u3001configuration provider \u6ce8\u610f: sentinel\u672c\u8eab\u662f\u9700\u8981\u4fdd\u8bc1HA\u7684\uff0c\u5426\u5219\u5c31\u4f1a\u5bfc\u81f4\u6574\u4e2a\u7cfb\u7edf\u7684single point of failure\uff0c\u6240\u4ee5\u9700\u8981\u4f7f\u7528sentinel cluster\u3002 \u7ed9API\u63d0\u4f9b\u6700\u65b0\u7684master\u4fe1\u606f \u4e09\u3001Redis instance replication : master\u3001slave\u4e4b\u95f4\u9700\u8981\u8fdb\u884creplication \u6d41\u7a0b \u53c2\u8003\u6587\u7ae0 1\u3001cnblogs Redis\u4e2d\u7b97\u6cd5\u4e4b\u2014\u2014Raft\u7b97\u6cd5 2\u3001cnblogs gqtc # redis\u7cfb\u5217 Redis\u6e90\u7801\u89e3\u6790\uff1a21sentinel(\u4e8c)\u5b9a\u671f\u53d1\u9001\u6d88\u606f\u3001\u68c0\u6d4b\u4e3b\u89c2\u4e0b\u7ebf Redis\u6e90\u7801\u89e3\u6790\uff1a22sentinel(\u4e09)\u5ba2\u89c2\u4e0b\u7ebf\u4ee5\u53ca\u6545\u969c\u8f6c\u79fb\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9 Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b \u603b\u7ed3 \u6d41\u7a0b\u4e3b\u8981\u7531Sentinel cluster\u6765\u5b8c\u6210: \u4e00\u3001\u4e0b\u7ebf\u68c0\u67e5 \u4e8c\u3001Sentinel cluster\u901a\u8fc7raft\u7b97\u6cd5\u9009\u51faSentinel leader \u4e09\u3001\u7531Sentinel leader\u4eceslave\u4e2d\uff0c\u9009\u62e9\u4e00\u4e2a\u6700\u4f18\u7684\u4f5c\u4e3a\u65b0\u7684master\uff0c\u7136\u540e\u6267\u884c\u4e00\u7cfb\u5217\u7684\u5176\u4ed6\u64cd\u4f5c cnblogs Redis\u4e2d\u7b97\u6cd5\u4e4b\u2014\u2014Raft\u7b97\u6cd5 NOTE: \u8fd9\u7bc7\u6587\u7ae0\u8bb2\u8ff0\u4e86Redis sentinel\u6267\u884cfailover\u7684 \u6d41\u7a0b Sentinel\u7cfb\u7edf\u9009\u4e3e\u9886\u5934\u7684\u65b9\u6cd5\u662f\u5bf9Raft\u7b97\u6cd5\u7684\u9886\u5934\u9009\u4e3e\u65b9\u6cd5\u7684\u5b9e\u73b0\u3002 NOTE: leader election \u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4e00\u81f4\u6027\u662f\u5f88\u91cd\u8981\u7684\u30021990\u5e74Leslie Lamport\u63d0\u51fa\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u7684\u4e00\u81f4\u6027\u7b97\u6cd5Paxos\u7b97\u6cd5\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5c31\u67d0\u4e2a\u503c\u6216\u51b3\u8bae\u8fbe\u6210\u4e00\u81f4\u7684\u95ee\u9898\u3002Paxos\u7b97\u6cd5\u6d41\u7a0b\u7e41\u6742\u5b9e\u73b0\u8d77\u6765\u4e5f\u6bd4\u8f83\u590d\u6742\u3002 2013\u5e74\u65af\u5766\u798f\u7684Diego Ongaro\u3001John Ousterhout\u4e24\u4e2a\u4eba\u4ee5\u6613\u61c2\u4e3a\u76ee\u6807\u8bbe\u8ba1\u4e00\u81f4\u6027\u7b97\u6cd5Raft\u3002Raft\u4e00\u81f4\u6027\u7b97\u6cd5\u4fdd\u969c\u5728\u4efb\u4f55\u65f6\u5019\u4e00\u65e6\u5904\u4e8eleader\u670d\u52a1\u5668\u5f53\u6389\uff0c\u53ef\u4ee5\u5728\u5269\u4f59\u6b63\u5e38\u5de5\u4f5c\u7684\u670d\u52a1\u5668\u4e2d\u9009\u4e3e\u51fa\u65b0\u7684Leader\u670d\u52a1\u5668\u66f4\u65b0\u65b0Leader\u670d\u52a1\u5668\uff0c\u4fee\u6539\u4ece\u670d\u52a1\u5668\u7684\u590d\u5236\u76ee\u6807\u3002 Sentinel\u662f\u4e00\u4e2a\u8fd0\u884c\u5728\u7279\u6b8a\u6a21\u5f0f\u4e0b\u7684Redis\u670d\u52a1\u5668\u3002\u5b83\u8d1f\u8d23\u76d1\u89c6\u4e3b\u670d\u52a1\u5668\u4ee5\u53ca\u4e3b\u670d\u52a1\u5668\u4e0b\u7684\u4ece\u670d\u52a1\u5668\u3002\u5f53**\u9886\u5934Sentinel**\u8ba4\u4e3a\u4e3b\u670d\u52a1\u5668\u5df2\u7ecf\u8fdb\u5165\u4e3b\u89c2\u4e0b\u7ebf\u72b6\u6001\uff0c\u5c06\u5bf9\u5df2\u4e0b\u7ebf\u7684**\u4e3b\u670d\u52a1\u5668**\u6267\u884c\u6545\u969c\u8f6c\u79fb\u64cd\u4f5c\uff0c\u8be5\u64cd\u4f5c\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a \uff081\uff09\u5728\u5df2\u4e0b\u7ebf\u4e3b\u670d\u52a1\u5668\u4e0b\u7684\u4ece\u670d\u52a1\u5668\u4e2d\u6311\u9009\u4e00\u4e2a\u4ece\u670d\u52a1\u5668\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u65b0\u7684\u4e3b\u670d\u52a1\u5668\u3002 \uff082\uff09\u8ba9\u5df2\u4e0b\u7ebf\u4e3b\u670d\u52a1\u5668\u5c5e\u4e0b\u7684\u6240\u6709\u4ece\u670d\u52a1\u5668\u6539\u4e3a\u590d\u5236\u65b0\u7684\u4e3b\u670d\u52a1\u5668\u3002 \uff083\uff09\u5c06\u5df2\u4e0b\u7ebf\u4e3b\u670d\u52a1\u5668\u6210\u4e3a\u65b0\u4e3b\u670d\u52a1\u5668\u7684\u4ece\u670d\u52a1\u5668\u3002 Raft\u8be6\u89e3\uff1a http://www.cnblogs.com/likehua/p/5845575.html \u5206\u5e03\u5f0fRaft\u7b97\u6cd5\uff1a http://www.jdon.com/artichect/raft.html \u5206\u5e03\u5f0f\u4e00\u81f4\u7b97\u6cd5\u2014\u2014Paxos\uff1a http://www.cnblogs.com/cchust/p/5617989.html","title":"Introduction"},{"location":"HA/#ha","text":"Redis\u7684HA\u9700\u8981\u7531\u5982\u4e0b\u4e09\u8005\u534f\u4f5c\u6765\u5b9e\u73b0: \u4e00\u3001API \u4e8c\u3001Sentinel cluster: \u529f\u80fd: 1\u3001\u4e0b\u7ebf\u68c0\u67e5\u3001automatic failover(\u540e\u9762\u4f1a\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd) 2\u3001configuration provider \u6ce8\u610f: sentinel\u672c\u8eab\u662f\u9700\u8981\u4fdd\u8bc1HA\u7684\uff0c\u5426\u5219\u5c31\u4f1a\u5bfc\u81f4\u6574\u4e2a\u7cfb\u7edf\u7684single point of failure\uff0c\u6240\u4ee5\u9700\u8981\u4f7f\u7528sentinel cluster\u3002 \u7ed9API\u63d0\u4f9b\u6700\u65b0\u7684master\u4fe1\u606f \u4e09\u3001Redis instance replication : master\u3001slave\u4e4b\u95f4\u9700\u8981\u8fdb\u884creplication","title":"HA"},{"location":"HA/#_1","text":"","title":"\u6d41\u7a0b"},{"location":"HA/#_2","text":"1\u3001cnblogs Redis\u4e2d\u7b97\u6cd5\u4e4b\u2014\u2014Raft\u7b97\u6cd5 2\u3001cnblogs gqtc # redis\u7cfb\u5217 Redis\u6e90\u7801\u89e3\u6790\uff1a21sentinel(\u4e8c)\u5b9a\u671f\u53d1\u9001\u6d88\u606f\u3001\u68c0\u6d4b\u4e3b\u89c2\u4e0b\u7ebf Redis\u6e90\u7801\u89e3\u6790\uff1a22sentinel(\u4e09)\u5ba2\u89c2\u4e0b\u7ebf\u4ee5\u53ca\u6545\u969c\u8f6c\u79fb\u4e4b\u9009\u4e3e\u9886\u5bfc\u8282\u70b9 Redis\u6e90\u7801\u89e3\u6790\uff1a23sentinel(\u56db)\u6545\u969c\u8f6c\u79fb\u6d41\u7a0b","title":"\u53c2\u8003\u6587\u7ae0"},{"location":"HA/#_3","text":"\u6d41\u7a0b\u4e3b\u8981\u7531Sentinel cluster\u6765\u5b8c\u6210: \u4e00\u3001\u4e0b\u7ebf\u68c0\u67e5 \u4e8c\u3001Sentinel cluster\u901a\u8fc7raft\u7b97\u6cd5\u9009\u51faSentinel leader \u4e09\u3001\u7531Sentinel leader\u4eceslave\u4e2d\uff0c\u9009\u62e9\u4e00\u4e2a\u6700\u4f18\u7684\u4f5c\u4e3a\u65b0\u7684master\uff0c\u7136\u540e\u6267\u884c\u4e00\u7cfb\u5217\u7684\u5176\u4ed6\u64cd\u4f5c","title":"\u603b\u7ed3"},{"location":"HA/#cnblogs#redisraft","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u8bb2\u8ff0\u4e86Redis sentinel\u6267\u884cfailover\u7684 \u6d41\u7a0b Sentinel\u7cfb\u7edf\u9009\u4e3e\u9886\u5934\u7684\u65b9\u6cd5\u662f\u5bf9Raft\u7b97\u6cd5\u7684\u9886\u5934\u9009\u4e3e\u65b9\u6cd5\u7684\u5b9e\u73b0\u3002 NOTE: leader election \u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4e00\u81f4\u6027\u662f\u5f88\u91cd\u8981\u7684\u30021990\u5e74Leslie Lamport\u63d0\u51fa\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u7684\u4e00\u81f4\u6027\u7b97\u6cd5Paxos\u7b97\u6cd5\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5c31\u67d0\u4e2a\u503c\u6216\u51b3\u8bae\u8fbe\u6210\u4e00\u81f4\u7684\u95ee\u9898\u3002Paxos\u7b97\u6cd5\u6d41\u7a0b\u7e41\u6742\u5b9e\u73b0\u8d77\u6765\u4e5f\u6bd4\u8f83\u590d\u6742\u3002 2013\u5e74\u65af\u5766\u798f\u7684Diego Ongaro\u3001John Ousterhout\u4e24\u4e2a\u4eba\u4ee5\u6613\u61c2\u4e3a\u76ee\u6807\u8bbe\u8ba1\u4e00\u81f4\u6027\u7b97\u6cd5Raft\u3002Raft\u4e00\u81f4\u6027\u7b97\u6cd5\u4fdd\u969c\u5728\u4efb\u4f55\u65f6\u5019\u4e00\u65e6\u5904\u4e8eleader\u670d\u52a1\u5668\u5f53\u6389\uff0c\u53ef\u4ee5\u5728\u5269\u4f59\u6b63\u5e38\u5de5\u4f5c\u7684\u670d\u52a1\u5668\u4e2d\u9009\u4e3e\u51fa\u65b0\u7684Leader\u670d\u52a1\u5668\u66f4\u65b0\u65b0Leader\u670d\u52a1\u5668\uff0c\u4fee\u6539\u4ece\u670d\u52a1\u5668\u7684\u590d\u5236\u76ee\u6807\u3002 Sentinel\u662f\u4e00\u4e2a\u8fd0\u884c\u5728\u7279\u6b8a\u6a21\u5f0f\u4e0b\u7684Redis\u670d\u52a1\u5668\u3002\u5b83\u8d1f\u8d23\u76d1\u89c6\u4e3b\u670d\u52a1\u5668\u4ee5\u53ca\u4e3b\u670d\u52a1\u5668\u4e0b\u7684\u4ece\u670d\u52a1\u5668\u3002\u5f53**\u9886\u5934Sentinel**\u8ba4\u4e3a\u4e3b\u670d\u52a1\u5668\u5df2\u7ecf\u8fdb\u5165\u4e3b\u89c2\u4e0b\u7ebf\u72b6\u6001\uff0c\u5c06\u5bf9\u5df2\u4e0b\u7ebf\u7684**\u4e3b\u670d\u52a1\u5668**\u6267\u884c\u6545\u969c\u8f6c\u79fb\u64cd\u4f5c\uff0c\u8be5\u64cd\u4f5c\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a \uff081\uff09\u5728\u5df2\u4e0b\u7ebf\u4e3b\u670d\u52a1\u5668\u4e0b\u7684\u4ece\u670d\u52a1\u5668\u4e2d\u6311\u9009\u4e00\u4e2a\u4ece\u670d\u52a1\u5668\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u65b0\u7684\u4e3b\u670d\u52a1\u5668\u3002 \uff082\uff09\u8ba9\u5df2\u4e0b\u7ebf\u4e3b\u670d\u52a1\u5668\u5c5e\u4e0b\u7684\u6240\u6709\u4ece\u670d\u52a1\u5668\u6539\u4e3a\u590d\u5236\u65b0\u7684\u4e3b\u670d\u52a1\u5668\u3002 \uff083\uff09\u5c06\u5df2\u4e0b\u7ebf\u4e3b\u670d\u52a1\u5668\u6210\u4e3a\u65b0\u4e3b\u670d\u52a1\u5668\u7684\u4ece\u670d\u52a1\u5668\u3002 Raft\u8be6\u89e3\uff1a http://www.cnblogs.com/likehua/p/5845575.html \u5206\u5e03\u5f0fRaft\u7b97\u6cd5\uff1a http://www.jdon.com/artichect/raft.html \u5206\u5e03\u5f0f\u4e00\u81f4\u7b97\u6cd5\u2014\u2014Paxos\uff1a http://www.cnblogs.com/cchust/p/5617989.html","title":"cnblogs Redis\u4e2d\u7b97\u6cd5\u4e4b\u2014\u2014Raft\u7b97\u6cd5"},{"location":"HA/Replication/","text":"Replication \u4e00\u3001\"replication\"\u5373\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u4e3b\u5907\u4e2d\u7684\"\u5907\"\uff0c\u6709\u5907\u65e0\u60a3\uff1b \u4e8c\u3001Redis\u7684HA\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8ereplication \u4e09\u3001Redis\u7684\u4e24\u79cd\u90e8\u7f72\u65b9\u5f0f\uff0c\u90fd\u652f\u6301replication Replication\u5728cluster mode disabled\u548ccluster mode enabled\u7684\u60c5\u51b5\u4e0b\u5bf9\u6bd4\u5206\u6790 \u5b9e\u73b0\u5206\u6790 \u5728 server.h:struct redisServer \u4e2d\u6709\u6210\u5458\u53d8\u91cf list *slaves \uff0c\u663e\u7136\u5bf9\u4e8emaster\u800c\u8a00\uff0c\u8fd9\u4e2a\u6210\u5458\u53d8\u91cf\u662f\u7528\u4e8e\u4fdd\u5b58\u5b83\u7684\u6240\u6709\u7684slaves\u7684\uff1b \u5728 cluster.h:struct clusterNode \u4e2d\u6709\u6210\u5458\u53d8\u91cf struct clusterNode **slaves \uff0c\u663e\u7136\u8fd9\u662f\u7528\u4e8e\u4fdd\u5b58\u8fd9\u4e2anode\u7684\u6240\u6709\u7684slaves\u7684\uff1b \u770b\u5230\u4e86\u8fd9\u4e9b\uff0c\u6211\u60f3\u5230\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff1a\u96be\u9053\u4e24\u79cd\u6a21\u5f0f\u4e0b\uff0creplication\u7684\u5b9e\u73b0\u662f\u4e0d\u540c\u7684\uff1f \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e24\u8005\u7684\u7c7b\u578b\u662f\u4e0d\u4e00\u6837\u7684\uff0c server.h:struct redisServer \u7684\u6210\u5458\u53d8\u91cf list *slaves \u4e2d\u4fdd\u5b58\u7684\u5143\u7d20\u7684\u7c7b\u578b\u662f struct client \uff0c\u800c cluster.h:struct clusterNode \u7684\u6210\u5458\u53d8\u91cf struct clusterNode **slaves \u4e2d\u4fdd\u5b58\u7684\u5143\u7d20\u7684\u7c7b\u578b\u662f struct clusterNode \u3002 \u5176\u5b9e\u5206\u6790\u5230\u8fd9\u91cc\uff0c\u6211\u731c\u6d4b\uff1acluster\u6a21\u578b\u4e0b\u7684replication\u5e94\u8be5\u662f\u57fa\u4e8e\u666e\u901a\u7684replication\u5b9e\u73b0\u7684\uff0c\u5728cluster\u7684\u5b9e\u73b0\u4e2d\uff0c\u5e94\u8be5\u4f1a\u5bf9\u666e\u901a\u7684replication\u8fdb\u884c\u4e00\u5b9a\u5730\u5c01\u88c5\uff0ccluster\u7684\u5b9e\u73b0\u4e2d\uff0c\u5e94\u8be5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5bf9cluster\u4e2dnode\u4e4b\u95f4\u7684\u5173\u7cfb\u7684\u7ef4\u62a4\u7b49\uff1b \u6784\u5efa\u5206\u6790 cluster\u4e2d\uff0c\u6307\u5b9a\u67d0\u4e2a\u8282\u70b9\u4e3a\u53e6\u5916\u4e00\u4e2a\u8282\u70b9\u7684replication\uff1a CLUSTER REPLICATE node-id \u975ecluster\u4e2d\uff0c\u6307\u5b9a\u4e00\u4e2ainstance\u4e3a\u53e6\u5916\u4e00\u4e2ainstance\u7684replication\uff1a SLAVEOF host port","title":"Introduction"},{"location":"HA/Replication/#replication","text":"\u4e00\u3001\"replication\"\u5373\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u4e3b\u5907\u4e2d\u7684\"\u5907\"\uff0c\u6709\u5907\u65e0\u60a3\uff1b \u4e8c\u3001Redis\u7684HA\u7684\u5b9e\u73b0\u4f9d\u8d56\u4e8ereplication \u4e09\u3001Redis\u7684\u4e24\u79cd\u90e8\u7f72\u65b9\u5f0f\uff0c\u90fd\u652f\u6301replication","title":"Replication"},{"location":"HA/Replication/#replicationcluster#mode#disabledcluster#mode#enabled","text":"","title":"Replication\u5728cluster mode disabled\u548ccluster mode enabled\u7684\u60c5\u51b5\u4e0b\u5bf9\u6bd4\u5206\u6790"},{"location":"HA/Replication/#_1","text":"\u5728 server.h:struct redisServer \u4e2d\u6709\u6210\u5458\u53d8\u91cf list *slaves \uff0c\u663e\u7136\u5bf9\u4e8emaster\u800c\u8a00\uff0c\u8fd9\u4e2a\u6210\u5458\u53d8\u91cf\u662f\u7528\u4e8e\u4fdd\u5b58\u5b83\u7684\u6240\u6709\u7684slaves\u7684\uff1b \u5728 cluster.h:struct clusterNode \u4e2d\u6709\u6210\u5458\u53d8\u91cf struct clusterNode **slaves \uff0c\u663e\u7136\u8fd9\u662f\u7528\u4e8e\u4fdd\u5b58\u8fd9\u4e2anode\u7684\u6240\u6709\u7684slaves\u7684\uff1b \u770b\u5230\u4e86\u8fd9\u4e9b\uff0c\u6211\u60f3\u5230\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff1a\u96be\u9053\u4e24\u79cd\u6a21\u5f0f\u4e0b\uff0creplication\u7684\u5b9e\u73b0\u662f\u4e0d\u540c\u7684\uff1f \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e24\u8005\u7684\u7c7b\u578b\u662f\u4e0d\u4e00\u6837\u7684\uff0c server.h:struct redisServer \u7684\u6210\u5458\u53d8\u91cf list *slaves \u4e2d\u4fdd\u5b58\u7684\u5143\u7d20\u7684\u7c7b\u578b\u662f struct client \uff0c\u800c cluster.h:struct clusterNode \u7684\u6210\u5458\u53d8\u91cf struct clusterNode **slaves \u4e2d\u4fdd\u5b58\u7684\u5143\u7d20\u7684\u7c7b\u578b\u662f struct clusterNode \u3002 \u5176\u5b9e\u5206\u6790\u5230\u8fd9\u91cc\uff0c\u6211\u731c\u6d4b\uff1acluster\u6a21\u578b\u4e0b\u7684replication\u5e94\u8be5\u662f\u57fa\u4e8e\u666e\u901a\u7684replication\u5b9e\u73b0\u7684\uff0c\u5728cluster\u7684\u5b9e\u73b0\u4e2d\uff0c\u5e94\u8be5\u4f1a\u5bf9\u666e\u901a\u7684replication\u8fdb\u884c\u4e00\u5b9a\u5730\u5c01\u88c5\uff0ccluster\u7684\u5b9e\u73b0\u4e2d\uff0c\u5e94\u8be5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5bf9cluster\u4e2dnode\u4e4b\u95f4\u7684\u5173\u7cfb\u7684\u7ef4\u62a4\u7b49\uff1b","title":"\u5b9e\u73b0\u5206\u6790"},{"location":"HA/Replication/#_2","text":"cluster\u4e2d\uff0c\u6307\u5b9a\u67d0\u4e2a\u8282\u70b9\u4e3a\u53e6\u5916\u4e00\u4e2a\u8282\u70b9\u7684replication\uff1a CLUSTER REPLICATE node-id \u975ecluster\u4e2d\uff0c\u6307\u5b9a\u4e00\u4e2ainstance\u4e3a\u53e6\u5916\u4e00\u4e2ainstance\u7684replication\uff1a SLAVEOF host port","title":"\u6784\u5efa\u5206\u6790"},{"location":"HA/Replication/redis-doc-Replication/","text":"redis Replication At the base of Redis replication (excluding the high availability features provided as an additional layer by Redis Cluster or Redis Sentinel ) there is a very simple to use and configure leader follower (master-slave) replication: it allows slave Redis instances to be exact copies of master instances . The slave will automatically reconnect to the master every time the link breaks, and will attempt to be an exact copy of it regardless of what happens to the master . This system works using three main mechanisms: When a master and a slave instances are well-connected, the master keeps the slave updated by sending a stream of commands to the slave, in order to replicate the effects on the dataset happening in the master side due to: client writes, keys expired or evicted, any other action changing the master dataset. SUMMARY : \u5728 replication.c:replicationFeedSlaves \u4e2d\u5b9e\u73b0\u7684\u8fd9\u4e2a\u8fc7\u7a0b When the link between the master and the slave breaks, for network issues or because a timeout is sensed in the master or the slave, the slave reconnects and attempts to proceed with a partial resynchronization : it means that it will try to just obtain the part of the stream of commands it missed during the disconnection. When a partial resynchronization is not possible, the slave will ask for a full resynchronization . This will involve a more complex process in which the master needs to create a snapshot of all its data, send it to the slave, and then continue sending the stream of commands as the dataset changes. Redis uses by default asynchronous replication , which being low latency and high performance, is the natural replication mode for the vast majority of Redis use cases. However Redis slaves asynchronously acknowledge the amount of data they received periodically with the master. So the master does not wait every time for a command to be processed by the slaves, however it knows, if needed, what slave already processed what command. This allows to have optional syncrhonous replication . Synchronous replication of certain data can be requested by the clients using the WAIT command. However WAIT is only able to ensure that there are the specified number of acknowledged copies in the other Redis instances, it does not turn a set of Redis instances into a CP system \uff08CP system means Consistency and Partition tolerance \uff09with strong consistency : acknowledged writes can still be lost during a failover, depending on the exact configuration of the Redis persistence . However with WAIT the probability of losing a write after a failure event is greatly reduced to certain hard to trigger failure modes. You could check the Sentinel or Redis Cluster documentation for more information about high availability and failover\uff08\u6545\u969c\u6062\u590d\uff09. The rest of this document mainly describe the basic characteristics of Redis basic replication. The following are some very important facts about Redis replication: Redis uses asynchronous replication , with asynchronous slave-to-master acknowledges of the amount of data processed. SUMMARY :\u5982\u4f55\u5b9e\u73b0\u7684asynchronous replication\uff1f\u662f\u901a\u8fc7new\u4e00\u4e2athread\u6765\u5b9e\u73b0\uff1f A master can have multiple slaves. Slaves are able to accept connections from other slaves. Aside from connecting a number of slaves to the same master, slaves can also be connected to other slaves in a cascading-like structure. Since Redis 4.0, all the sub-slaves will receive exactly the same replication stream from the master . SUMMARY : \u662f\u8c01\u5411sub-slaves \u53d1\u9001replication stream\uff1f\u5e94\u8be5\u8fd8\u662fmaster\uff0c\u8fd9\u6837\u4f1a\u6bd4\u8f83\u7b80\u5355\u4e00\u4e9b\u3002\u662f\u8fd9\u6837\u7684\uff0c\u6700\u540e\u4e00\u53e5\u8bdd\u5c31\u8bc1\u5b9e\u4e86\uff1b Redis replication is non-blocking on the master side . This means that the master will continue to handle queries when one or more slaves perform the initial synchronization or a partial resynchronization . Replication is also largely non-blocking on the slave side . While the slave is performing the initial synchronization, it can handle queries using the old version of the dataset, assuming you configured Redis to do so in redis.conf . Otherwise, you can configure Redis slaves to return an error to clients if the replication stream is down. However, after the initial sync, the old dataset must be deleted and the new one must be loaded. The slave will block incoming connections during this brief window (that can be as long as many seconds for very large datasets). Since Redis 4.0 it is possible to configure Redis so that the deletion of the old data set happens in a different thread, however loading the new initial dataset will still happen in the main thread and block the slave. SUMMARY :\u5982\u4f55\u5b9e\u73b0\u7684\uff1f Replication can be used both for scalability , in order to have multiple slaves for read-only queries (for example, slow O(N) operations can be offloaded to slaves), or simply for improving data safety and high availability . It is possible to use replication to avoid the cost of having the master writing the full dataset to disk: a typical technique involves configuring your master redis.conf to avoid persisting to disk at all, then connect a slave configured to save from time to time, or with AOF enabled. However this setup must be handled with care, since a restarting master will start with an empty dataset: if the slave tries to synchronized with it, the slave will be emptied as well. SUMMARY : \u4e0b\u9762\u8fd9\u4e00\u6bb5\u63cf\u8ff0\u4e86\u8fd9\u79cd\u573a\u666f Safety of replication when master has persistence turned off In setups where Redis replication is used, it is strongly advised to have persistence turned on in the master and in the slaves. When this is not possible, for example because of latency concerns due to very slow disks, instances should be configured to avoid restarting automatically after a reboot. To better understand why masters with persistence turned off configured to auto restart are dangerous, check the following failure mode where data is wiped from the master and all its slaves: We have a setup with node A acting as master, with persistence turned down, and nodes B and C replicating from node A. Node A crashes, however it has some auto-restart system, that restarts the process. However since persistence is turned off, the node restarts with an empty data set. Nodes B and C will replicate from node A, which is empty, so they'll effectively destroy their copy of the data. When Redis Sentinel is used for high availability, also turning off persistence on the master, together with auto restart of the process, is dangerous. For example the master can restart fast enough for Sentinel to don't detect a failure, so that the failure mode described above happens. Every time data safety is important, and replication is used with master configured without persistence, auto restart of instances should be disabled. How Redis replication works Every Redis master has a replication ID : it is a large pseudo random string that marks a given story of the dataset. Each master also takes an offset that increments for every byte of replication stream that it is produced to be sent to slaves, in order to update the state of the slaves with the new changes modifying the dataset. The replication offset is incremented even if no slave is actually connected, so basically every given pair of: Replication ID, offset Identifies an exact version of the dataset of a master. SUMMARY : struct redisServer \u7684\u5b57\u6bb5 replid \u5c31\u8868\u793areplication ID\uff0c master_repl_offset \u5c31\u8868\u793acurrent replication offset When slaves connects to masters, they use the PSYNC command in order to send their old master replication ID and the offsets they processed so far. This way the master can send just the incremental part needed. However if there is not enough backlog in the master buffers, or if the slave is referring to an history (replication ID) which is no longer known, than a full resynchronization happens: in this case the slave will get a full copy of the dataset, from scratch. SUMMARY : struct redisServer \u7684\u5b57\u6bb5 repl_backlog \u5c31\u8868\u793aReplication backlog for partial syncs\uff0c\u5b83\u662f\u4e00\u4e2acircular array\u3002 The master starts a background saving process in order to produce an RDB file. At the same time it starts to buffer all new write commands received from the clients. When the background saving is complete, the master transfers the database file to the slave , which saves it on disk, and then loads it into memory. The master will then send all buffered commands to the slave. This is done as a stream of commands and is in the same format of the Redis protocol itself. You can try it yourself via telnet. Connect to the Redis port while the server is doing some work and issue the SYNC command. You'll see a bulk transfer and then every command received by the master will be re-issued in the telnet session. Actually SYNC is an old protocol no longer used by newer Redis instances, but is still there for backward compatibility: it does not allow partial resynchronizations, so now PSYNC is used instead. As already said, slaves are able to automatically reconnect when the master-slave link goes down for some reason. If the master receives multiple concurrent slave synchronization requests, it performs a single background save in order to serve all of them. SUMMARY : \u4e24\u4e2aprocess\u4e4b\u95f4\u662f\u5982\u4f55\u8fdb\u884c\u901a\u4fe1\u7684\uff1f Replication ID explained In the previous section we said that if two instances have the same replication ID and replication offset , they have exactly the same data. However it is useful to understand what exctly is the replication ID , and why instances have actually two replication IDs the main ID and the secondary ID . A replication ID basically marks a given history of the data set. Every time an instance restarts from scratch as a master, or a slave is promoted to master, a new replication ID is generated for this instance. The slaves connected to a master will inherit\uff08\u7ee7\u627f\uff09 its replication ID after the handshake . So two instances with the same ID are related by the fact that they hold the same data, but potentially at a different time. It is the offset that works as a logical time to understand, for a given history (replication ID) who holds the most updated data set. For instance if two instances A and B have the same replication ID, but one with offset 1000 and one with offset 1023, it means that the first lacks certain commands applied to the data set. It also means that A, by applying just a few commands, may reach exactly the same state of B. The reason why Redis instances have two replication IDs is because of slaves that are promoted to masters. After a failover, the promoted slave requires to still remember what was its past replication ID , because such replication ID was the one of the former master . In this way, when other slaves will synchronize with the new master, they will try to perform a partial resynchronization using the old master replication ID . This will work as expected, because when the slave is promoted to master it sets its secondary ID to its main ID , remembering what was the offset when this ID switch happend. Later it will select a new random replication ID , because a new history begins. When handling the new slaves connecting, the master will match their IDs and offsets both with the current ID and the secondary ID (up to a given offset, for safety). In short this means that after a failover, slaves connecting to the new promoted master don't have to perform a full sync. In case you wonder why a slave promoted to master needs to change its replication ID after a failover: it is possible that the old master is still working as a master because of some network partition: retaining the same replication ID would violate the fact that the same ID and same offset of any two random instances mean they have the same data set. Diskless replication Normally a full resynchronization requires to create an RDB file on disk, then reload the same RDB from disk in order to feed the slaves with the data. With slow disks this can be a very stressing operation for the master. Redis version 2.8.18 is the first version to have support for diskless replication. In this setup the child process directly sends the RDB over the wire to slaves, without using the disk as intermediate storage. Configuration To configure basic Redis replication is trivial: just add the following line to the slave configuration file: slaveof 192.168.1.1 6379 Of course you need to replace 192.168.1.1 6379 with your master IP address (or hostname) and port. Alternatively, you can call the SLAVEOF command and the master host will start a sync with the slave. There are also a few parameters for tuning the replication backlog taken in memory by the master to perform the partial resynchronization. See the example redis.conf shipped with the Redis distribution for more information. Diskless replication can be enabled using the repl-diskless-sync configuration parameter. The delay to start the transfer in order to wait more slaves to arrive after the first one, is controlled by the repl-diskless-sync-delay parameter. Please refer to the example redis.conf file in the Redis distribution for more details. Read-only slave Since Redis 2.6, slaves support a read-only mode that is enabled by default. This behavior is controlled by the slave-read-only option in the redis.conf file, and can be enabled and disabled at runtime using CONFIG SET . Read-only slaves will reject all write commands, so that it is not possible to write to a slave because of a mistake. This does not mean that the feature is intended to expose a slave instance to the internet or more generally to a network where untrusted clients exist, because administrative commands like DEBUG or CONFIG are still enabled. However, security of read-only instances can be improved by disabling commands in redis.conf using the rename-command directive. You may wonder why it is possible to revert the read-only setting and have slave instances that can be targeted by write operations. While those writes will be discarded if the slave and the master resynchronize or if the slave is restarted, there are a few legitimate use case for storing ephemeral data in writable slaves. For example computing slow Set or Sorted set operations and storing them into local keys is an use case for writable slaves that was observed multiple times. However note that writable slaves before version 4.0 were incapable of expiring keys with a time to live set . This means that if you use EXPIRE or other commands that set a maximum TTL for a key, the key will leak, and while you may no longer see it while accessing it with read commands, you will see it in the count of keys and it will still use memory. So in general mixing writable slaves (previous version 4.0) and keys with TTL is going to create issues. Redis 4.0 RC3 and greater versions totally solve this problem and now writable slaves are able to evict keys with TTL as masters do, with the exceptions of keys written in DB numbers greater than 63 (but by default Redis instances only have 16 databases). Also note that since Redis 4.0 slave writes are only local, and are not propagated to sub-slaves attached to the instance. Sub slaves instead will always receive the replication stream identical to the one sent by the top-level master to the intermediate slaves. So for example in the following setup: A ---> B ---> C Even if B is writable, C will not see B writes and will instead have identical dataset as the master instance A . Setting a slave to authenticate to a master If your master has a password via requirepass , it's trivial to configure the slave to use that password in all sync operations. To do it on a running instance, use redis-cli and type: config set masterauth <password> To set it permanently, add this to your config file: masterauth <password> Allow writes only with N attached replicas Starting with Redis 2.8, it is possible to configure a Redis master to accept write queries only if at least N slaves are currently connected to the master. However, because Redis uses asynchronous replication it is not possible to ensure the slave actually received a given write, so there is always a window for data loss . This is how the feature works: Redis slaves ping the master every second, acknowledging the amount of replication stream processed. Redis masters will remember the last time it received a ping from every slave. The user can configure a minimum number of slaves that have a lag not greater than a maximum number of seconds. If there are at least N slaves, with a lag less than M seconds, then the write will be accepted. You may think of it as a best effort data safety mechanism, where consistency is not ensured for a given write, but at least the time window for data loss is restricted to a given number of seconds. In general bound data loss is better than unbound one. If the conditions are not met, the master will instead reply with an error and the write will not be accepted. There are two configuration parameters for this feature: min-slaves-to-write <number of slaves> min-slaves-max-lag <number of seconds> For more information, please check the example redis.conf file shipped with the Redis source distribution. The INFO and ROLE command There are two Redis commands that provide a lot of information on the current replication parameters of master and slave instances. One is INFO . If the command is called with the replication argument as INFO replication only information relevant to the replication are displayed. Another more computer-friendly command is ROLE , that provides the replication status of masters and slaves together with their replication offsets, list of connected slaves and so forth. Partial resynchronizations after restarts and failovers Since Redis 4.0, when an instance is promoted to master after a failover, it will be still able to perform a partial resynchronization with the slaves of the old master. To do so, the slave remembers the old replication ID and offset of its former master, so can provide part of the backlog to the connecting slaves even if they ask for the old replication ID. However the new replication ID of the promoted slave will be different, since it constitutes a different history of the data set. For example, the master can return available and can continue accepting writes for some time, so using the same replication ID in the promoted slave would violate the rule that a of replication ID and offset pair identifies only a single data set. Moreover slaves when powered off gently and restarted, are able to store in the RDB file the information needed in order to resynchronize with their master. This is useful in case of upgrades. When this is needed, it is better to use the SHUTDOWN command in order to perform a save & quit operation on the slave. It is not possilbe to partially resynchronize a slave that restarted via the AOF file. However the instance may be turned to RDB persistence before shutting down it, than can be restarted, and finally AOF can be enabled again.","title":"Introduction"},{"location":"HA/Replication/redis-doc-Replication/#redis#replication","text":"At the base of Redis replication (excluding the high availability features provided as an additional layer by Redis Cluster or Redis Sentinel ) there is a very simple to use and configure leader follower (master-slave) replication: it allows slave Redis instances to be exact copies of master instances . The slave will automatically reconnect to the master every time the link breaks, and will attempt to be an exact copy of it regardless of what happens to the master . This system works using three main mechanisms: When a master and a slave instances are well-connected, the master keeps the slave updated by sending a stream of commands to the slave, in order to replicate the effects on the dataset happening in the master side due to: client writes, keys expired or evicted, any other action changing the master dataset. SUMMARY : \u5728 replication.c:replicationFeedSlaves \u4e2d\u5b9e\u73b0\u7684\u8fd9\u4e2a\u8fc7\u7a0b When the link between the master and the slave breaks, for network issues or because a timeout is sensed in the master or the slave, the slave reconnects and attempts to proceed with a partial resynchronization : it means that it will try to just obtain the part of the stream of commands it missed during the disconnection. When a partial resynchronization is not possible, the slave will ask for a full resynchronization . This will involve a more complex process in which the master needs to create a snapshot of all its data, send it to the slave, and then continue sending the stream of commands as the dataset changes. Redis uses by default asynchronous replication , which being low latency and high performance, is the natural replication mode for the vast majority of Redis use cases. However Redis slaves asynchronously acknowledge the amount of data they received periodically with the master. So the master does not wait every time for a command to be processed by the slaves, however it knows, if needed, what slave already processed what command. This allows to have optional syncrhonous replication . Synchronous replication of certain data can be requested by the clients using the WAIT command. However WAIT is only able to ensure that there are the specified number of acknowledged copies in the other Redis instances, it does not turn a set of Redis instances into a CP system \uff08CP system means Consistency and Partition tolerance \uff09with strong consistency : acknowledged writes can still be lost during a failover, depending on the exact configuration of the Redis persistence . However with WAIT the probability of losing a write after a failure event is greatly reduced to certain hard to trigger failure modes. You could check the Sentinel or Redis Cluster documentation for more information about high availability and failover\uff08\u6545\u969c\u6062\u590d\uff09. The rest of this document mainly describe the basic characteristics of Redis basic replication. The following are some very important facts about Redis replication: Redis uses asynchronous replication , with asynchronous slave-to-master acknowledges of the amount of data processed. SUMMARY :\u5982\u4f55\u5b9e\u73b0\u7684asynchronous replication\uff1f\u662f\u901a\u8fc7new\u4e00\u4e2athread\u6765\u5b9e\u73b0\uff1f A master can have multiple slaves. Slaves are able to accept connections from other slaves. Aside from connecting a number of slaves to the same master, slaves can also be connected to other slaves in a cascading-like structure. Since Redis 4.0, all the sub-slaves will receive exactly the same replication stream from the master . SUMMARY : \u662f\u8c01\u5411sub-slaves \u53d1\u9001replication stream\uff1f\u5e94\u8be5\u8fd8\u662fmaster\uff0c\u8fd9\u6837\u4f1a\u6bd4\u8f83\u7b80\u5355\u4e00\u4e9b\u3002\u662f\u8fd9\u6837\u7684\uff0c\u6700\u540e\u4e00\u53e5\u8bdd\u5c31\u8bc1\u5b9e\u4e86\uff1b Redis replication is non-blocking on the master side . This means that the master will continue to handle queries when one or more slaves perform the initial synchronization or a partial resynchronization . Replication is also largely non-blocking on the slave side . While the slave is performing the initial synchronization, it can handle queries using the old version of the dataset, assuming you configured Redis to do so in redis.conf . Otherwise, you can configure Redis slaves to return an error to clients if the replication stream is down. However, after the initial sync, the old dataset must be deleted and the new one must be loaded. The slave will block incoming connections during this brief window (that can be as long as many seconds for very large datasets). Since Redis 4.0 it is possible to configure Redis so that the deletion of the old data set happens in a different thread, however loading the new initial dataset will still happen in the main thread and block the slave. SUMMARY :\u5982\u4f55\u5b9e\u73b0\u7684\uff1f Replication can be used both for scalability , in order to have multiple slaves for read-only queries (for example, slow O(N) operations can be offloaded to slaves), or simply for improving data safety and high availability . It is possible to use replication to avoid the cost of having the master writing the full dataset to disk: a typical technique involves configuring your master redis.conf to avoid persisting to disk at all, then connect a slave configured to save from time to time, or with AOF enabled. However this setup must be handled with care, since a restarting master will start with an empty dataset: if the slave tries to synchronized with it, the slave will be emptied as well. SUMMARY : \u4e0b\u9762\u8fd9\u4e00\u6bb5\u63cf\u8ff0\u4e86\u8fd9\u79cd\u573a\u666f","title":"redis Replication"},{"location":"HA/Replication/redis-doc-Replication/#safety#of#replication#when#master#has#persistence#turned#off","text":"In setups where Redis replication is used, it is strongly advised to have persistence turned on in the master and in the slaves. When this is not possible, for example because of latency concerns due to very slow disks, instances should be configured to avoid restarting automatically after a reboot. To better understand why masters with persistence turned off configured to auto restart are dangerous, check the following failure mode where data is wiped from the master and all its slaves: We have a setup with node A acting as master, with persistence turned down, and nodes B and C replicating from node A. Node A crashes, however it has some auto-restart system, that restarts the process. However since persistence is turned off, the node restarts with an empty data set. Nodes B and C will replicate from node A, which is empty, so they'll effectively destroy their copy of the data. When Redis Sentinel is used for high availability, also turning off persistence on the master, together with auto restart of the process, is dangerous. For example the master can restart fast enough for Sentinel to don't detect a failure, so that the failure mode described above happens. Every time data safety is important, and replication is used with master configured without persistence, auto restart of instances should be disabled.","title":"Safety of replication when master has persistence turned off"},{"location":"HA/Replication/redis-doc-Replication/#how#redis#replication#works","text":"Every Redis master has a replication ID : it is a large pseudo random string that marks a given story of the dataset. Each master also takes an offset that increments for every byte of replication stream that it is produced to be sent to slaves, in order to update the state of the slaves with the new changes modifying the dataset. The replication offset is incremented even if no slave is actually connected, so basically every given pair of: Replication ID, offset Identifies an exact version of the dataset of a master. SUMMARY : struct redisServer \u7684\u5b57\u6bb5 replid \u5c31\u8868\u793areplication ID\uff0c master_repl_offset \u5c31\u8868\u793acurrent replication offset When slaves connects to masters, they use the PSYNC command in order to send their old master replication ID and the offsets they processed so far. This way the master can send just the incremental part needed. However if there is not enough backlog in the master buffers, or if the slave is referring to an history (replication ID) which is no longer known, than a full resynchronization happens: in this case the slave will get a full copy of the dataset, from scratch. SUMMARY : struct redisServer \u7684\u5b57\u6bb5 repl_backlog \u5c31\u8868\u793aReplication backlog for partial syncs\uff0c\u5b83\u662f\u4e00\u4e2acircular array\u3002 The master starts a background saving process in order to produce an RDB file. At the same time it starts to buffer all new write commands received from the clients. When the background saving is complete, the master transfers the database file to the slave , which saves it on disk, and then loads it into memory. The master will then send all buffered commands to the slave. This is done as a stream of commands and is in the same format of the Redis protocol itself. You can try it yourself via telnet. Connect to the Redis port while the server is doing some work and issue the SYNC command. You'll see a bulk transfer and then every command received by the master will be re-issued in the telnet session. Actually SYNC is an old protocol no longer used by newer Redis instances, but is still there for backward compatibility: it does not allow partial resynchronizations, so now PSYNC is used instead. As already said, slaves are able to automatically reconnect when the master-slave link goes down for some reason. If the master receives multiple concurrent slave synchronization requests, it performs a single background save in order to serve all of them. SUMMARY : \u4e24\u4e2aprocess\u4e4b\u95f4\u662f\u5982\u4f55\u8fdb\u884c\u901a\u4fe1\u7684\uff1f","title":"How Redis replication works"},{"location":"HA/Replication/redis-doc-Replication/#replication#id#explained","text":"In the previous section we said that if two instances have the same replication ID and replication offset , they have exactly the same data. However it is useful to understand what exctly is the replication ID , and why instances have actually two replication IDs the main ID and the secondary ID . A replication ID basically marks a given history of the data set. Every time an instance restarts from scratch as a master, or a slave is promoted to master, a new replication ID is generated for this instance. The slaves connected to a master will inherit\uff08\u7ee7\u627f\uff09 its replication ID after the handshake . So two instances with the same ID are related by the fact that they hold the same data, but potentially at a different time. It is the offset that works as a logical time to understand, for a given history (replication ID) who holds the most updated data set. For instance if two instances A and B have the same replication ID, but one with offset 1000 and one with offset 1023, it means that the first lacks certain commands applied to the data set. It also means that A, by applying just a few commands, may reach exactly the same state of B. The reason why Redis instances have two replication IDs is because of slaves that are promoted to masters. After a failover, the promoted slave requires to still remember what was its past replication ID , because such replication ID was the one of the former master . In this way, when other slaves will synchronize with the new master, they will try to perform a partial resynchronization using the old master replication ID . This will work as expected, because when the slave is promoted to master it sets its secondary ID to its main ID , remembering what was the offset when this ID switch happend. Later it will select a new random replication ID , because a new history begins. When handling the new slaves connecting, the master will match their IDs and offsets both with the current ID and the secondary ID (up to a given offset, for safety). In short this means that after a failover, slaves connecting to the new promoted master don't have to perform a full sync. In case you wonder why a slave promoted to master needs to change its replication ID after a failover: it is possible that the old master is still working as a master because of some network partition: retaining the same replication ID would violate the fact that the same ID and same offset of any two random instances mean they have the same data set.","title":"Replication ID explained"},{"location":"HA/Replication/redis-doc-Replication/#diskless#replication","text":"Normally a full resynchronization requires to create an RDB file on disk, then reload the same RDB from disk in order to feed the slaves with the data. With slow disks this can be a very stressing operation for the master. Redis version 2.8.18 is the first version to have support for diskless replication. In this setup the child process directly sends the RDB over the wire to slaves, without using the disk as intermediate storage.","title":"Diskless replication"},{"location":"HA/Replication/redis-doc-Replication/#configuration","text":"To configure basic Redis replication is trivial: just add the following line to the slave configuration file: slaveof 192.168.1.1 6379 Of course you need to replace 192.168.1.1 6379 with your master IP address (or hostname) and port. Alternatively, you can call the SLAVEOF command and the master host will start a sync with the slave. There are also a few parameters for tuning the replication backlog taken in memory by the master to perform the partial resynchronization. See the example redis.conf shipped with the Redis distribution for more information. Diskless replication can be enabled using the repl-diskless-sync configuration parameter. The delay to start the transfer in order to wait more slaves to arrive after the first one, is controlled by the repl-diskless-sync-delay parameter. Please refer to the example redis.conf file in the Redis distribution for more details.","title":"Configuration"},{"location":"HA/Replication/redis-doc-Replication/#read-only#slave","text":"Since Redis 2.6, slaves support a read-only mode that is enabled by default. This behavior is controlled by the slave-read-only option in the redis.conf file, and can be enabled and disabled at runtime using CONFIG SET . Read-only slaves will reject all write commands, so that it is not possible to write to a slave because of a mistake. This does not mean that the feature is intended to expose a slave instance to the internet or more generally to a network where untrusted clients exist, because administrative commands like DEBUG or CONFIG are still enabled. However, security of read-only instances can be improved by disabling commands in redis.conf using the rename-command directive. You may wonder why it is possible to revert the read-only setting and have slave instances that can be targeted by write operations. While those writes will be discarded if the slave and the master resynchronize or if the slave is restarted, there are a few legitimate use case for storing ephemeral data in writable slaves. For example computing slow Set or Sorted set operations and storing them into local keys is an use case for writable slaves that was observed multiple times. However note that writable slaves before version 4.0 were incapable of expiring keys with a time to live set . This means that if you use EXPIRE or other commands that set a maximum TTL for a key, the key will leak, and while you may no longer see it while accessing it with read commands, you will see it in the count of keys and it will still use memory. So in general mixing writable slaves (previous version 4.0) and keys with TTL is going to create issues. Redis 4.0 RC3 and greater versions totally solve this problem and now writable slaves are able to evict keys with TTL as masters do, with the exceptions of keys written in DB numbers greater than 63 (but by default Redis instances only have 16 databases). Also note that since Redis 4.0 slave writes are only local, and are not propagated to sub-slaves attached to the instance. Sub slaves instead will always receive the replication stream identical to the one sent by the top-level master to the intermediate slaves. So for example in the following setup: A ---> B ---> C Even if B is writable, C will not see B writes and will instead have identical dataset as the master instance A .","title":"Read-only slave"},{"location":"HA/Replication/redis-doc-Replication/#setting#a#slave#to#authenticate#to#a#master","text":"If your master has a password via requirepass , it's trivial to configure the slave to use that password in all sync operations. To do it on a running instance, use redis-cli and type: config set masterauth <password> To set it permanently, add this to your config file: masterauth <password>","title":"Setting a slave to authenticate to a master"},{"location":"HA/Replication/redis-doc-Replication/#allow#writes#only#with#n#attached#replicas","text":"Starting with Redis 2.8, it is possible to configure a Redis master to accept write queries only if at least N slaves are currently connected to the master. However, because Redis uses asynchronous replication it is not possible to ensure the slave actually received a given write, so there is always a window for data loss . This is how the feature works: Redis slaves ping the master every second, acknowledging the amount of replication stream processed. Redis masters will remember the last time it received a ping from every slave. The user can configure a minimum number of slaves that have a lag not greater than a maximum number of seconds. If there are at least N slaves, with a lag less than M seconds, then the write will be accepted. You may think of it as a best effort data safety mechanism, where consistency is not ensured for a given write, but at least the time window for data loss is restricted to a given number of seconds. In general bound data loss is better than unbound one. If the conditions are not met, the master will instead reply with an error and the write will not be accepted. There are two configuration parameters for this feature: min-slaves-to-write <number of slaves> min-slaves-max-lag <number of seconds> For more information, please check the example redis.conf file shipped with the Redis source distribution.","title":"Allow writes only with N attached replicas"},{"location":"HA/Replication/redis-doc-Replication/#the#info#and#role#command","text":"There are two Redis commands that provide a lot of information on the current replication parameters of master and slave instances. One is INFO . If the command is called with the replication argument as INFO replication only information relevant to the replication are displayed. Another more computer-friendly command is ROLE , that provides the replication status of masters and slaves together with their replication offsets, list of connected slaves and so forth.","title":"The INFO and ROLE command"},{"location":"HA/Replication/redis-doc-Replication/#partial#resynchronizations#after#restarts#and#failovers","text":"Since Redis 4.0, when an instance is promoted to master after a failover, it will be still able to perform a partial resynchronization with the slaves of the old master. To do so, the slave remembers the old replication ID and offset of its former master, so can provide part of the backlog to the connecting slaves even if they ask for the old replication ID. However the new replication ID of the promoted slave will be different, since it constitutes a different history of the data set. For example, the master can return available and can continue accepting writes for some time, so using the same replication ID in the promoted slave would violate the rule that a of replication ID and offset pair identifies only a single data set. Moreover slaves when powered off gently and restarted, are able to store in the RDB file the information needed in order to resynchronize with their master. This is useful in case of upgrades. When this is needed, it is better to use the SHUTDOWN command in order to perform a save & quit operation on the slave. It is not possilbe to partially resynchronize a slave that restarted via the AOF file. However the instance may be turned to RDB persistence before shutting down it, than can be restarted, and finally AOF can be enabled again.","title":"Partial resynchronizations after restarts and failovers"},{"location":"HA/Sentinel/","text":"Redis sentinel","title":"Introduction"},{"location":"HA/Sentinel/#redis#sentinel","text":"","title":"Redis sentinel"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/","text":"redis Redis Sentinel Documentation NOTE: \u4e00\u3001\u9996\u5148\u9700\u8981\u4e86\u89e3\u62d3\u6251\u7ed3\u6784 jianshu Redis\u54e8\u5175\uff08Sentinel\uff09\u6a21\u5f0f sentinel\u4e4b\u95f4\u76f8\u4e92\u8fde\u63a5 Redis Sentinel provides high availability for Redis. In practical terms this means that using Sentinel you can create a Redis deployment that resists without human intervention certain kinds of failures. NOTE: \"human intervention\"\u5373\"\u4eba\u5de5\u5e72\u9884\" Redis Sentinel also provides other collateral\uff08\u9644\u5c5e\u7684\uff09 tasks such as monitoring , notifications and acts as a configuration provider for clients. This is the full list of Sentinel capabilities at a macroscopical\uff08\u5b8f\u89c2\u7684\uff09 level (i.e. the big picture ): 1\u3001 Monitoring . Sentinel constantly checks if your master and slave instances are working as expected. 2\u3001 Notification . Sentinel can notify the system administrator , another computer programs, via an API, that something is wrong with one of the monitored Redis instances. 3\u3001 Automatic failover . If a master is not working as expected, Sentinel can start a failover process where a slave is promoted to master, the other additional slaves are reconfigured to use the new master, and the applications using the Redis server informed about the new address to use when connecting. NOTE: \u4e00\u3001\u81ea\u52a8\u6545\u969c\u8f6c\u79fb\u3002 \u5982\u679c\u4e3b\u670d\u52a1\u5668\u672a\u6309\u9884\u671f\u5de5\u4f5c\uff0cSentinel\u53ef\u4ee5\u542f\u52a8\u6545\u969c\u8f6c\u79fb\u8fc7\u7a0b\uff0c\u5176\u4e2d\u4ece\u670d\u52a1\u5668\u5347\u7ea7\u4e3a\u4e3b\u670d\u52a1\u5668\uff0c\u5176\u4ed6\u5176\u4ed6\u670d\u52a1\u5668\u91cd\u65b0\u914d\u7f6e\u4e3a\u4f7f\u7528\u65b0\u4e3b\u670d\u52a1\u5668\uff0c\u5e76\u4e14\u4f7f\u7528Redis\u670d\u52a1\u5668\u7684\u5e94\u7528\u7a0b\u5e8f\u901a\u77e5\u6709\u5173\u65b0\u670d\u52a1\u5668\u7684\u5730\u5740\u3002\u8fde\u63a5\u3002 \u4e8c\u3001\u8fd9\u662f\u4e00\u79cd\u4e2d\u5fc3\u5316\u7684\u67b6\u6784 4\u3001 Configuration provider . Sentinel acts as a source of authority for clients service discovery : clients connect to Sentinels in order to ask for the address of the current Redis master responsible for a given service. If a failover occurs, Sentinels will report the new address. NOTE: \u663e\u7136\uff0cRedis\u7684HA\u9700\u8981\u7531API\u3001sentinel\u3001master\u3001slave\u6765\u534f\u4f5c\u5b8c\u6210 Distributed nature of Sentinel NOTE: \u4e00\u3001Redis sentinel\u8ba9\u6211\u60f3\u8d77\u6765ZK\uff0c\u5b83\u4eec\u4e24\u8005\u6709\u7740\u8bf8\u591a\u7684\u76f8\u4f3c\u4e4b\u5904\uff0c\u663e\u7136\uff0cZK\u53ef\u4ee5\u4e3a\u7cfb\u7edf\u63d0\u4f9bsentinel\u529f\u80fd\uff0c\u6bd4\u5982kafka\u5c31\u662f\u8fd9\u6837\u505a\u7684 Redis Sentinel is a distributed system: Sentinel itself is designed to run in a configuration where there are multiple Sentinel processes cooperating together. The advantage of having multiple Sentinel processes cooperating are the following: 1\u3001Failure detection is performed when multiple Sentinels agree about the fact a given master is no longer available. This lowers the probability of false positives . 2\u3001 Sentinel works even if not all the Sentinel processes are working, making the system robust against failures. There is no fun in having a fail over system which is itself a single point of failure, after all. NOTE: \u907f\u514d\"single point of failure\"\u662fHA\u7684\u5173\u952e The sum of Sentinels, Redis instances (masters and slaves) and clients connecting to Sentinel and Redis, are also a larger distributed system with specific properties. In this document concepts will be introduced gradually starting from basic information needed in order to understand the basic properties of Sentinel, to more complex information (that are optional) in order to understand how exactly Sentinel works. Quick Start Obtaining Sentinel The current version of Sentinel is called Sentinel 2 . It is a rewrite of the initial Sentinel implementation using stronger and simpler to predict algorithms (that are explained in this documentation). Running Sentinel If you are using the redis-sentinel executable (or if you have a symbolic link with that name to the redis-server executable) you can run Sentinel with the following command line: redis-sentinel /path/to/sentinel.conf Otherwise you can use directly the redis-server executable starting it in Sentinel mode : redis-server /path/to/sentinel.conf --sentinel Both ways work the same. However it is mandatory to use a configuration file when running Sentinel , as this file will be used by the system in order to save the current state that will be reloaded in case of restarts. Sentinel will simply refuse to start if no configuration file is given or if the configuration file path is not writable. Sentinels by default run listening for connections to TCP port 26379 , so for Sentinels to work, port 26379 of your servers must be open to receive connections from the IP addresses of the other Sentinel instances . Otherwise Sentinels can't talk and can't agree about what to do, so failover will never be performed. NOTE: consensus algorithm Fundamental things to know about Sentinel before deploying 1\u3001You need at least three Sentinel instances for a robust deployment. NOTE: \u4e3a\u4e86\"Quorum\"\uff0c\u9700\u8981\u80fd\u591f\u88c1\u51b3\u51fa\u4e00\u4e2a\u7ed3\u679c 2\u3001The three Sentinel instances should be placed into computers or virtual machines that are believed to fail in an independent way. So for example different physical servers or Virtual Machines executed on different availability zones. NOTE: \u8de8\u673a\u623f\u707e\u5907 3\u3001Sentinel + Redis distributed system does not guarantee that acknowledged writes are retained(\u4fdd\u6301) during failures, since Redis uses asynchronous replication . However there are ways to deploy Sentinel that make the window to lose writes limited to certain moments, while there are other less secure ways to deploy it. NOTE: \u4e00\u3001\u5982\u4f55\u5b9e\u73b0\uff1f \u4e8c\u3001\u8fd9\u6bb5\u8bdd\u8ba9\u6211\u60f3\u8d77\u6765CAP\uff0c\u6211\u76ee\u524d\u7684\u60f3\u6cd5\u662f: Redis favor availability over consistency \u4e09\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\" acknowledged writes \"\u8981\u5982\u4f55\u7406\u89e3\uff1f 4\u3001You need Sentinel support in your clients. Popular client libraries have Sentinel support, but not all. 5\u3001There is no HA setup which is safe if you don't test from time to time in development environments, or even better if you can, in production environments, if they work. You may have a misconfiguration that will become apparent only when it's too late (at 3am when your master stops working). 6\u3001 Sentinel, Docker, or other forms of Network Address Translation or Port Mapping should be mixed with care : Docker performs port remapping, breaking Sentinel auto discovery of other Sentinel processes and the list of slaves for a master. Check the section about Sentinel and Docker later in this document for more information. Configuring Sentinel The Redis source distribution contains a file called sentinel.conf that is a self-documented example configuration file you can use to configure Sentinel, however a typical minimal configuration file looks like the following: sentinel monitor mymaster 127 .0.0.1 6379 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 sentinel monitor resque 192 .168.1.3 6380 4 sentinel down-after-milliseconds resque 10000 sentinel failover-timeout resque 180000 sentinel parallel-syncs resque 5 You only need to specify the masters to monitor, giving to each separated master (that may have any number of slaves) a different name . There is no need to specify slaves , which are auto-discovered . NOTE: \u4e00\u3001\u5982\u4f55**auto-discovered** slave\uff1f Sentinel will update the configuration automatically with additional information about slaves (in order to retain the information in case of restart). The configuration is also rewritten every time a slave is promoted to master during a failover and every time a new Sentinel is discovered. The example configuration above, basically monitor two sets of Redis instances , each composed of a master and an undefined number of slaves. One set of instances is called mymaster , and the other resque . The meaning of the arguments of sentinel monitor statements is the following: sentinel monitor <master-group-name> <ip> <port> <quorum> For the sake of clarity, let's check line by line what the configuration options mean: The first line is used to tell Redis to monitor a master called mymaster , that is at address 127.0.0.1 and port 6379, with a quorum(\u591a\u6570\u6d3e) of 2. Everything is pretty obvious but the quorum argument: 1\u3001The quorum is the number of Sentinels that need to agree about the fact the master is not reachable, in order for really mark the slave as failing, and eventually start a fail over procedure if possible. 2\u3001However the quorum is only used to detect the failure . In order to actually perform a failover, one of the Sentinels need to be elected leader for the failover and be authorized to proceed. This only happens with the vote of the majority of the Sentinel processes . So for example if you have 5 Sentinel processes, and the quorum for a given master set to the value of 2, this is what happens: 1\u3001If two Sentinels agree at the same time about the master being unreachable, one of the two will try to start a failover. 2\u3001If there are at least a total of three Sentinels reachable , the failover will be authorized and will actually start. In practical terms this means during failures Sentinel never starts a failover if the majority of Sentinel processes are unable to talk (aka no failover in the minority partition). NOTE: \u5b9e\u9645\u4e0a\uff0c\u8fd9\u610f\u5473\u7740\u5728\u6545\u969c\u671f\u95f4\uff0c\u5982\u679c\u5927\u591a\u6570Sentinel\u8fdb\u7a0b\u65e0\u6cd5\u901a\u8bdd\uff08\u5728\u5c11\u6570\u5206\u533a\u4e2d\u4e5f\u6ca1\u6709\u6545\u969c\u8f6c\u79fb\uff09\uff0cSentinel\u6c38\u8fdc\u4e0d\u4f1a\u542f\u52a8\u6545\u969c\u8f6c\u79fb\u3002 Other Sentinel options The other options are almost always in the form: sentinel <option_name> <master_name> <option_value> And are used for the following purposes: 1\u3001 down-after-milliseconds is the time in milliseconds an instance should not be reachable (either does not reply to our PINGs or it is replying with an error) for a Sentinel starting to think it is down. 2\u3001v parallel-syncs sets the number of slaves that can be reconfigured to use the new master after a failover at the same time. The lower the number, the more time it will take for the failover process to complete, however if the slaves are configured to serve old data, you may not want all the slaves to re-synchronize with the master at the same time. While the replication process is mostly non blocking for a slave, there is a moment when it stops to load the bulk data from the master. You may want to make sure only one slave at a time is not reachable by setting this option to the value of 1. parallel-syncs\u8bbe\u7f6e\u53ef\u5728\u540c\u4e00\u6545\u969c\u8f6c\u79fb\u540e\u91cd\u65b0\u914d\u7f6e\u4e3a\u4f7f\u7528\u65b0\u4e3b\u670d\u52a1\u5668\u7684\u4ece\u670d\u52a1\u5668\u6570\u3002 \u6570\u5b57\u8d8a\u5c0f\uff0c\u6545\u969c\u8f6c\u79fb\u8fc7\u7a0b\u5b8c\u6210\u6240\u9700\u7684\u65f6\u95f4\u5c31\u8d8a\u591a\uff0c\u4f46\u662f\u5982\u679c\u4ece\u5c5e\u670d\u52a1\u5668\u914d\u7f6e\u4e3a\u63d0\u4f9b\u65e7\u6570\u636e\uff0c\u5219\u53ef\u80fd\u4e0d\u5e0c\u671b\u6240\u6709\u4ece\u5c5e\u670d\u52a1\u5668\u540c\u65f6\u4e0e\u4e3b\u670d\u52a1\u5668\u91cd\u65b0\u540c\u6b65\u3002 \u867d\u7136\u590d\u5236\u8fc7\u7a0b\u5bf9\u4e8e\u4ece\u5c5e\u8bbe\u5907\u5927\u90e8\u5206\u662f\u975e\u963b\u585e\u7684\uff0c\u4f46\u662f\u6709\u4e00\u6bb5\u65f6\u95f4\u5b83\u505c\u6b62\u4ece\u4e3b\u8bbe\u5907\u52a0\u8f7d\u6279\u91cf\u6570\u636e\u3002 \u60a8\u53ef\u80fd\u5e0c\u671b\u901a\u8fc7\u5c06\u6b64\u9009\u9879\u8bbe\u7f6e\u4e3a\u503c1\u6765\u786e\u4fdd\u4e00\u6b21\u53ea\u80fd\u8bbf\u95ee\u4e00\u4e2a\u4ece\u7ad9\u3002 Additional options are described in the rest of this document and documented in the example sentinel.conf file shipped with the Redis distribution. All the configuration parameters can be modified at runtime using the SENTINEL SET command. See the Reconfiguring Sentinel at runtime section for more information. Example Sentinel deployments Now that you know the basic information about Sentinel, you may wonder where you should place your Sentinel processes , how much Sentinel processes you need and so forth. This section shows a few example deployments. We use ASCII art in order to show you configuration examples in a graphical format, this is what the different symbols means: +--------------------+ | This is a computer | | or VM that fails | | independently. We | | call it a \"box\" | +--------------------+ We write inside the boxes what they are running: +-------------------+ | Redis master M1 | | Redis Sentinel S1 | +-------------------+ Different boxes are connected by lines, to show that they are able to talk: +-------------+ +-------------+ | Sentinel S1 |---------------| Sentinel S2 | +-------------+ +-------------+ Network partitions are shown as interrupted lines using slashes: +-------------+ +-------------+ | Sentinel S1 |------ // ------| Sentinel S2 | +-------------+ +-------------+ Also note that: Masters are called M1, M2, M3, ..., Mn. Slaves are called R1, R2, R3, ..., Rn (R stands for replica ). Sentinels are called S1, S2, S3, ..., Sn. Clients are called C1, C2, C3, ..., Cn. When an instance changes role because of Sentinel actions, we put it inside square brackets, so [M1] means an instance that is now a master because of Sentinel intervention. Example 1: just two Sentinels, DON'T DO THIS +----+ +----+ | M1 |---------| R1 | | S1 | | S2 | +----+ +----+ Configuration: quorum = 1 In this setup, if the master M1 fails, R1 will be promoted since the two Sentinels can reach agreement about the failure (obviously with quorum set to 1) and can also authorize\uff08\u6279\u51c6\uff09 a failover because the majority is two. So apparently it could superficially work, however check the next points to see why this setup is broken. If the box where M1 is running stops working, also S1 stops working. The Sentinel running in the other box S2 will not be able to authorize a failover, so the system will become not available. Note that a majority is needed in order to order different failovers, and later propagate the latest configuration to all the Sentinels . Also note that the ability to failover in a single side of the above setup, without any agreement, would be very dangerous: +----+ +------+ | M1 |----//-----| [M1] | | S1 | | S2 | +----+ +------+ In the above configuration we created two masters (assuming S2 could failover without authorization) in a perfectly symmetrical way. Clients may write indefinitely to both sides, and there is no way to understand when the partition heals what configuration is the right one, in order to prevent a permanent split brain condition . \u5728\u4e0a\u9762\u7684\u914d\u7f6e\u4e2d\uff0c\u6211\u4eec\u4ee5\u5b8c\u5168\u5bf9\u79f0\u7684\u65b9\u5f0f\u521b\u5efa\u4e86\u4e24\u4e2a\u4e3b\u670d\u52a1\u5668\uff08\u5047\u8bbeS2\u53ef\u4ee5\u5728\u672a\u7ecf\u6388\u6743\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6545\u969c\u8f6c\u79fb\uff09\u3002 \u5ba2\u6237\u7aef\u53ef\u4ee5\u65e0\u9650\u671f\u5730\u5411\u53cc\u65b9\u5199\u5165\uff0c\u5e76\u4e14\u65e0\u6cd5\u7406\u89e3\u5206\u533a\u4f55\u65f6\u6062\u590d\u6b63\u786e\u7684\u914d\u7f6e\uff0c\u4ee5\u9632\u6b62\u6c38\u4e45\u6027\u7684\u88c2\u8111\u60c5\u51b5\u3002 So please deploy at least three Sentinels in three different boxes always. Example 2: basic setup with three boxes This is a very simple setup, that has the advantage to be simple to tune for additional safety. It is based on three boxes, each box running both a Redis process and a Sentinel process. +----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ Configuration: quorum = 2 If the master M1 fails, S2 and S3 will agree about the failure and will be able to authorize a failover, making clients able to continue. In every Sentinel setup, being Redis asynchronously replicated, there is always the risk of losing some write because a given acknowledged write may not be able to reach the slave which is promoted to master. However in the above setup there is an higher risk due to clients partitioned away with an old master, like in the following picture: +----+ | M1 | | S1 | <- C1 (writes will be lost) +----+ | / / +------+ | +----+ | [M2] |----+----| R3 | | S2 | | S3 | +------+ +----+ A quick tutorial Asking Sentinel about the state of a master Obtaining the address of the current master Testing the failover Sentinel API Sentinel commands Reconfiguring Sentinel at Runtime Adding or removing Sentinels Removing the old master or unreachable slaves Pub/Sub Messages Handling of -BUSY state Slaves priority Sentinel and Redis authentication Configuring Sentinel instances with authentication More advanced concepts In the following sections we'll cover a few details about how Sentinel work, without to resorting to implementation details and algorithms that will be covered in the final part of this document. SDOWN and ODOWN failure state Sentinels and Slaves auto discovery Sentinel reconfiguration of instances outside the failover procedure Slave selection and priority Algorithms and internals In the following sections we will explore the details of Sentinel behavior. It is not strictly needed for users to be aware of all the details, but a deep understanding of Sentinel may help to deploy and operate Sentinel in a more effective way. Quorum","title":"Introduction"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#redis#redis#sentinel#documentation","text":"NOTE: \u4e00\u3001\u9996\u5148\u9700\u8981\u4e86\u89e3\u62d3\u6251\u7ed3\u6784 jianshu Redis\u54e8\u5175\uff08Sentinel\uff09\u6a21\u5f0f sentinel\u4e4b\u95f4\u76f8\u4e92\u8fde\u63a5 Redis Sentinel provides high availability for Redis. In practical terms this means that using Sentinel you can create a Redis deployment that resists without human intervention certain kinds of failures. NOTE: \"human intervention\"\u5373\"\u4eba\u5de5\u5e72\u9884\" Redis Sentinel also provides other collateral\uff08\u9644\u5c5e\u7684\uff09 tasks such as monitoring , notifications and acts as a configuration provider for clients. This is the full list of Sentinel capabilities at a macroscopical\uff08\u5b8f\u89c2\u7684\uff09 level (i.e. the big picture ): 1\u3001 Monitoring . Sentinel constantly checks if your master and slave instances are working as expected. 2\u3001 Notification . Sentinel can notify the system administrator , another computer programs, via an API, that something is wrong with one of the monitored Redis instances. 3\u3001 Automatic failover . If a master is not working as expected, Sentinel can start a failover process where a slave is promoted to master, the other additional slaves are reconfigured to use the new master, and the applications using the Redis server informed about the new address to use when connecting. NOTE: \u4e00\u3001\u81ea\u52a8\u6545\u969c\u8f6c\u79fb\u3002 \u5982\u679c\u4e3b\u670d\u52a1\u5668\u672a\u6309\u9884\u671f\u5de5\u4f5c\uff0cSentinel\u53ef\u4ee5\u542f\u52a8\u6545\u969c\u8f6c\u79fb\u8fc7\u7a0b\uff0c\u5176\u4e2d\u4ece\u670d\u52a1\u5668\u5347\u7ea7\u4e3a\u4e3b\u670d\u52a1\u5668\uff0c\u5176\u4ed6\u5176\u4ed6\u670d\u52a1\u5668\u91cd\u65b0\u914d\u7f6e\u4e3a\u4f7f\u7528\u65b0\u4e3b\u670d\u52a1\u5668\uff0c\u5e76\u4e14\u4f7f\u7528Redis\u670d\u52a1\u5668\u7684\u5e94\u7528\u7a0b\u5e8f\u901a\u77e5\u6709\u5173\u65b0\u670d\u52a1\u5668\u7684\u5730\u5740\u3002\u8fde\u63a5\u3002 \u4e8c\u3001\u8fd9\u662f\u4e00\u79cd\u4e2d\u5fc3\u5316\u7684\u67b6\u6784 4\u3001 Configuration provider . Sentinel acts as a source of authority for clients service discovery : clients connect to Sentinels in order to ask for the address of the current Redis master responsible for a given service. If a failover occurs, Sentinels will report the new address. NOTE: \u663e\u7136\uff0cRedis\u7684HA\u9700\u8981\u7531API\u3001sentinel\u3001master\u3001slave\u6765\u534f\u4f5c\u5b8c\u6210","title":"redis Redis Sentinel Documentation"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#distributed#nature#of#sentinel","text":"NOTE: \u4e00\u3001Redis sentinel\u8ba9\u6211\u60f3\u8d77\u6765ZK\uff0c\u5b83\u4eec\u4e24\u8005\u6709\u7740\u8bf8\u591a\u7684\u76f8\u4f3c\u4e4b\u5904\uff0c\u663e\u7136\uff0cZK\u53ef\u4ee5\u4e3a\u7cfb\u7edf\u63d0\u4f9bsentinel\u529f\u80fd\uff0c\u6bd4\u5982kafka\u5c31\u662f\u8fd9\u6837\u505a\u7684 Redis Sentinel is a distributed system: Sentinel itself is designed to run in a configuration where there are multiple Sentinel processes cooperating together. The advantage of having multiple Sentinel processes cooperating are the following: 1\u3001Failure detection is performed when multiple Sentinels agree about the fact a given master is no longer available. This lowers the probability of false positives . 2\u3001 Sentinel works even if not all the Sentinel processes are working, making the system robust against failures. There is no fun in having a fail over system which is itself a single point of failure, after all. NOTE: \u907f\u514d\"single point of failure\"\u662fHA\u7684\u5173\u952e The sum of Sentinels, Redis instances (masters and slaves) and clients connecting to Sentinel and Redis, are also a larger distributed system with specific properties. In this document concepts will be introduced gradually starting from basic information needed in order to understand the basic properties of Sentinel, to more complex information (that are optional) in order to understand how exactly Sentinel works.","title":"Distributed nature of Sentinel"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#quick#start","text":"","title":"Quick Start"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#obtaining#sentinel","text":"The current version of Sentinel is called Sentinel 2 . It is a rewrite of the initial Sentinel implementation using stronger and simpler to predict algorithms (that are explained in this documentation).","title":"Obtaining Sentinel"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#running#sentinel","text":"If you are using the redis-sentinel executable (or if you have a symbolic link with that name to the redis-server executable) you can run Sentinel with the following command line: redis-sentinel /path/to/sentinel.conf Otherwise you can use directly the redis-server executable starting it in Sentinel mode : redis-server /path/to/sentinel.conf --sentinel Both ways work the same. However it is mandatory to use a configuration file when running Sentinel , as this file will be used by the system in order to save the current state that will be reloaded in case of restarts. Sentinel will simply refuse to start if no configuration file is given or if the configuration file path is not writable. Sentinels by default run listening for connections to TCP port 26379 , so for Sentinels to work, port 26379 of your servers must be open to receive connections from the IP addresses of the other Sentinel instances . Otherwise Sentinels can't talk and can't agree about what to do, so failover will never be performed. NOTE: consensus algorithm","title":"Running Sentinel"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#fundamental#things#to#know#about#sentinel#before#deploying","text":"1\u3001You need at least three Sentinel instances for a robust deployment. NOTE: \u4e3a\u4e86\"Quorum\"\uff0c\u9700\u8981\u80fd\u591f\u88c1\u51b3\u51fa\u4e00\u4e2a\u7ed3\u679c 2\u3001The three Sentinel instances should be placed into computers or virtual machines that are believed to fail in an independent way. So for example different physical servers or Virtual Machines executed on different availability zones. NOTE: \u8de8\u673a\u623f\u707e\u5907 3\u3001Sentinel + Redis distributed system does not guarantee that acknowledged writes are retained(\u4fdd\u6301) during failures, since Redis uses asynchronous replication . However there are ways to deploy Sentinel that make the window to lose writes limited to certain moments, while there are other less secure ways to deploy it. NOTE: \u4e00\u3001\u5982\u4f55\u5b9e\u73b0\uff1f \u4e8c\u3001\u8fd9\u6bb5\u8bdd\u8ba9\u6211\u60f3\u8d77\u6765CAP\uff0c\u6211\u76ee\u524d\u7684\u60f3\u6cd5\u662f: Redis favor availability over consistency \u4e09\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\" acknowledged writes \"\u8981\u5982\u4f55\u7406\u89e3\uff1f 4\u3001You need Sentinel support in your clients. Popular client libraries have Sentinel support, but not all. 5\u3001There is no HA setup which is safe if you don't test from time to time in development environments, or even better if you can, in production environments, if they work. You may have a misconfiguration that will become apparent only when it's too late (at 3am when your master stops working). 6\u3001 Sentinel, Docker, or other forms of Network Address Translation or Port Mapping should be mixed with care : Docker performs port remapping, breaking Sentinel auto discovery of other Sentinel processes and the list of slaves for a master. Check the section about Sentinel and Docker later in this document for more information.","title":"Fundamental things to know about Sentinel before deploying"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#configuring#sentinel","text":"The Redis source distribution contains a file called sentinel.conf that is a self-documented example configuration file you can use to configure Sentinel, however a typical minimal configuration file looks like the following: sentinel monitor mymaster 127 .0.0.1 6379 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 sentinel monitor resque 192 .168.1.3 6380 4 sentinel down-after-milliseconds resque 10000 sentinel failover-timeout resque 180000 sentinel parallel-syncs resque 5 You only need to specify the masters to monitor, giving to each separated master (that may have any number of slaves) a different name . There is no need to specify slaves , which are auto-discovered . NOTE: \u4e00\u3001\u5982\u4f55**auto-discovered** slave\uff1f Sentinel will update the configuration automatically with additional information about slaves (in order to retain the information in case of restart). The configuration is also rewritten every time a slave is promoted to master during a failover and every time a new Sentinel is discovered. The example configuration above, basically monitor two sets of Redis instances , each composed of a master and an undefined number of slaves. One set of instances is called mymaster , and the other resque . The meaning of the arguments of sentinel monitor statements is the following: sentinel monitor <master-group-name> <ip> <port> <quorum> For the sake of clarity, let's check line by line what the configuration options mean: The first line is used to tell Redis to monitor a master called mymaster , that is at address 127.0.0.1 and port 6379, with a quorum(\u591a\u6570\u6d3e) of 2. Everything is pretty obvious but the quorum argument: 1\u3001The quorum is the number of Sentinels that need to agree about the fact the master is not reachable, in order for really mark the slave as failing, and eventually start a fail over procedure if possible. 2\u3001However the quorum is only used to detect the failure . In order to actually perform a failover, one of the Sentinels need to be elected leader for the failover and be authorized to proceed. This only happens with the vote of the majority of the Sentinel processes . So for example if you have 5 Sentinel processes, and the quorum for a given master set to the value of 2, this is what happens: 1\u3001If two Sentinels agree at the same time about the master being unreachable, one of the two will try to start a failover. 2\u3001If there are at least a total of three Sentinels reachable , the failover will be authorized and will actually start. In practical terms this means during failures Sentinel never starts a failover if the majority of Sentinel processes are unable to talk (aka no failover in the minority partition). NOTE: \u5b9e\u9645\u4e0a\uff0c\u8fd9\u610f\u5473\u7740\u5728\u6545\u969c\u671f\u95f4\uff0c\u5982\u679c\u5927\u591a\u6570Sentinel\u8fdb\u7a0b\u65e0\u6cd5\u901a\u8bdd\uff08\u5728\u5c11\u6570\u5206\u533a\u4e2d\u4e5f\u6ca1\u6709\u6545\u969c\u8f6c\u79fb\uff09\uff0cSentinel\u6c38\u8fdc\u4e0d\u4f1a\u542f\u52a8\u6545\u969c\u8f6c\u79fb\u3002","title":"Configuring Sentinel"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#other#sentinel#options","text":"The other options are almost always in the form: sentinel <option_name> <master_name> <option_value> And are used for the following purposes: 1\u3001 down-after-milliseconds is the time in milliseconds an instance should not be reachable (either does not reply to our PINGs or it is replying with an error) for a Sentinel starting to think it is down. 2\u3001v parallel-syncs sets the number of slaves that can be reconfigured to use the new master after a failover at the same time. The lower the number, the more time it will take for the failover process to complete, however if the slaves are configured to serve old data, you may not want all the slaves to re-synchronize with the master at the same time. While the replication process is mostly non blocking for a slave, there is a moment when it stops to load the bulk data from the master. You may want to make sure only one slave at a time is not reachable by setting this option to the value of 1. parallel-syncs\u8bbe\u7f6e\u53ef\u5728\u540c\u4e00\u6545\u969c\u8f6c\u79fb\u540e\u91cd\u65b0\u914d\u7f6e\u4e3a\u4f7f\u7528\u65b0\u4e3b\u670d\u52a1\u5668\u7684\u4ece\u670d\u52a1\u5668\u6570\u3002 \u6570\u5b57\u8d8a\u5c0f\uff0c\u6545\u969c\u8f6c\u79fb\u8fc7\u7a0b\u5b8c\u6210\u6240\u9700\u7684\u65f6\u95f4\u5c31\u8d8a\u591a\uff0c\u4f46\u662f\u5982\u679c\u4ece\u5c5e\u670d\u52a1\u5668\u914d\u7f6e\u4e3a\u63d0\u4f9b\u65e7\u6570\u636e\uff0c\u5219\u53ef\u80fd\u4e0d\u5e0c\u671b\u6240\u6709\u4ece\u5c5e\u670d\u52a1\u5668\u540c\u65f6\u4e0e\u4e3b\u670d\u52a1\u5668\u91cd\u65b0\u540c\u6b65\u3002 \u867d\u7136\u590d\u5236\u8fc7\u7a0b\u5bf9\u4e8e\u4ece\u5c5e\u8bbe\u5907\u5927\u90e8\u5206\u662f\u975e\u963b\u585e\u7684\uff0c\u4f46\u662f\u6709\u4e00\u6bb5\u65f6\u95f4\u5b83\u505c\u6b62\u4ece\u4e3b\u8bbe\u5907\u52a0\u8f7d\u6279\u91cf\u6570\u636e\u3002 \u60a8\u53ef\u80fd\u5e0c\u671b\u901a\u8fc7\u5c06\u6b64\u9009\u9879\u8bbe\u7f6e\u4e3a\u503c1\u6765\u786e\u4fdd\u4e00\u6b21\u53ea\u80fd\u8bbf\u95ee\u4e00\u4e2a\u4ece\u7ad9\u3002 Additional options are described in the rest of this document and documented in the example sentinel.conf file shipped with the Redis distribution. All the configuration parameters can be modified at runtime using the SENTINEL SET command. See the Reconfiguring Sentinel at runtime section for more information.","title":"Other Sentinel options"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#example#sentinel#deployments","text":"Now that you know the basic information about Sentinel, you may wonder where you should place your Sentinel processes , how much Sentinel processes you need and so forth. This section shows a few example deployments. We use ASCII art in order to show you configuration examples in a graphical format, this is what the different symbols means: +--------------------+ | This is a computer | | or VM that fails | | independently. We | | call it a \"box\" | +--------------------+ We write inside the boxes what they are running: +-------------------+ | Redis master M1 | | Redis Sentinel S1 | +-------------------+ Different boxes are connected by lines, to show that they are able to talk: +-------------+ +-------------+ | Sentinel S1 |---------------| Sentinel S2 | +-------------+ +-------------+ Network partitions are shown as interrupted lines using slashes: +-------------+ +-------------+ | Sentinel S1 |------ // ------| Sentinel S2 | +-------------+ +-------------+ Also note that: Masters are called M1, M2, M3, ..., Mn. Slaves are called R1, R2, R3, ..., Rn (R stands for replica ). Sentinels are called S1, S2, S3, ..., Sn. Clients are called C1, C2, C3, ..., Cn. When an instance changes role because of Sentinel actions, we put it inside square brackets, so [M1] means an instance that is now a master because of Sentinel intervention.","title":"Example Sentinel deployments"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#example#1#just#two#sentinels#dont#do#this","text":"+----+ +----+ | M1 |---------| R1 | | S1 | | S2 | +----+ +----+ Configuration: quorum = 1 In this setup, if the master M1 fails, R1 will be promoted since the two Sentinels can reach agreement about the failure (obviously with quorum set to 1) and can also authorize\uff08\u6279\u51c6\uff09 a failover because the majority is two. So apparently it could superficially work, however check the next points to see why this setup is broken. If the box where M1 is running stops working, also S1 stops working. The Sentinel running in the other box S2 will not be able to authorize a failover, so the system will become not available. Note that a majority is needed in order to order different failovers, and later propagate the latest configuration to all the Sentinels . Also note that the ability to failover in a single side of the above setup, without any agreement, would be very dangerous: +----+ +------+ | M1 |----//-----| [M1] | | S1 | | S2 | +----+ +------+ In the above configuration we created two masters (assuming S2 could failover without authorization) in a perfectly symmetrical way. Clients may write indefinitely to both sides, and there is no way to understand when the partition heals what configuration is the right one, in order to prevent a permanent split brain condition . \u5728\u4e0a\u9762\u7684\u914d\u7f6e\u4e2d\uff0c\u6211\u4eec\u4ee5\u5b8c\u5168\u5bf9\u79f0\u7684\u65b9\u5f0f\u521b\u5efa\u4e86\u4e24\u4e2a\u4e3b\u670d\u52a1\u5668\uff08\u5047\u8bbeS2\u53ef\u4ee5\u5728\u672a\u7ecf\u6388\u6743\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6545\u969c\u8f6c\u79fb\uff09\u3002 \u5ba2\u6237\u7aef\u53ef\u4ee5\u65e0\u9650\u671f\u5730\u5411\u53cc\u65b9\u5199\u5165\uff0c\u5e76\u4e14\u65e0\u6cd5\u7406\u89e3\u5206\u533a\u4f55\u65f6\u6062\u590d\u6b63\u786e\u7684\u914d\u7f6e\uff0c\u4ee5\u9632\u6b62\u6c38\u4e45\u6027\u7684\u88c2\u8111\u60c5\u51b5\u3002 So please deploy at least three Sentinels in three different boxes always.","title":"Example 1: just two Sentinels, DON'T DO THIS"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#example#2#basic#setup#with#three#boxes","text":"This is a very simple setup, that has the advantage to be simple to tune for additional safety. It is based on three boxes, each box running both a Redis process and a Sentinel process. +----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ Configuration: quorum = 2 If the master M1 fails, S2 and S3 will agree about the failure and will be able to authorize a failover, making clients able to continue. In every Sentinel setup, being Redis asynchronously replicated, there is always the risk of losing some write because a given acknowledged write may not be able to reach the slave which is promoted to master. However in the above setup there is an higher risk due to clients partitioned away with an old master, like in the following picture: +----+ | M1 | | S1 | <- C1 (writes will be lost) +----+ | / / +------+ | +----+ | [M2] |----+----| R3 | | S2 | | S3 | +------+ +----+","title":"Example 2: basic setup with three boxes"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#a#quick#tutorial","text":"","title":"A quick tutorial"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#asking#sentinel#about#the#state#of#a#master","text":"","title":"Asking Sentinel about the state of a master"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#obtaining#the#address#of#the#current#master","text":"","title":"Obtaining the address of the current master"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#testing#the#failover","text":"","title":"Testing the failover"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#sentinel#api","text":"","title":"Sentinel API"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#sentinel#commands","text":"","title":"Sentinel commands"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#reconfiguring#sentinel#at#runtime","text":"","title":"Reconfiguring Sentinel at Runtime"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#adding#or#removing#sentinels","text":"","title":"Adding or removing Sentinels"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#removing#the#old#master#or#unreachable#slaves","text":"","title":"Removing the old master or unreachable slaves"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#pubsub#messages","text":"","title":"Pub/Sub Messages"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#handling#of#-busy#state","text":"","title":"Handling of -BUSY state"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#slaves#priority","text":"","title":"Slaves priority"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#sentinel#and#redis#authentication","text":"","title":"Sentinel and Redis authentication"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#configuring#sentinel#instances#with#authentication","text":"","title":"Configuring Sentinel instances with authentication"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#more#advanced#concepts","text":"In the following sections we'll cover a few details about how Sentinel work, without to resorting to implementation details and algorithms that will be covered in the final part of this document.","title":"More advanced concepts"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#sdown#and#odown#failure#state","text":"","title":"SDOWN and ODOWN failure state"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#sentinels#and#slaves#auto#discovery","text":"","title":"Sentinels and Slaves auto discovery"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#sentinel#reconfiguration#of#instances#outside#the#failover#procedure","text":"","title":"Sentinel reconfiguration of instances outside the failover procedure"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#slave#selection#and#priority","text":"","title":"Slave selection and priority"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#algorithms#and#internals","text":"In the following sections we will explore the details of Sentinel behavior. It is not strictly needed for users to be aware of all the details, but a deep understanding of Sentinel may help to deploy and operate Sentinel in a more effective way.","title":"Algorithms and internals"},{"location":"HA/Sentinel/Redis-Sentinel-Documentation/#quorum","text":"","title":"Quorum"},{"location":"cluster/","text":"Redis cluster \u5b9e\u73b0\u6982\u8ff0 redis cluster\u7684\u7ed3\u6784\u662fdecentralized \uff0c\u6240\u4ee5\u6bcf\u4e2anode\u4e3a\u4e86\u77e5\u9053\u5176\u4ed6node\u7684\u4fe1\u606f\u5fc5\u987b\u8981\u5c06\u5b83\u4eec\u4fdd\u5b58\u4e0b\u6765\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u6bcf\u4e2anode\u90fd\u9700\u8981\u6709\u4e00\u4e2a clusterState \uff0c\u5e76\u4e14redis cluster\u662f High-availability cluster \uff0c\u5373\u5b83\u9700\u8981\u652f\u6301failover\uff0c\u6240\u4ee5cluster\u9700\u8981\u6709\u80fd\u591f\u4fa6\u63a2cluster\u662f\u5426\u5f02\u5e38\u7684\u80fd\u529b\uff0c\u6240\u4ee5\u5b83\u9700\u8981\u4e0d\u65ad\u5730\u8fdb\u884cheartbeat\uff0c\u7136\u540e\u5c06\u5f97\u5230\u7684cluster\u4e2d\u7684\u5176\u4ed6node\u7684\u72b6\u6001\u4fdd\u5b58\u8d77\u6765\uff1b\u800craft\u7b97\u6cd5\u7684cluster\u7ed3\u6784\u662f\u6709centralized\u7684\uff0c\u5b83\u662f\u57fa\u4e8eleader-follower\u7684\uff0c\u4e0d\u662f\u4e00\u4e2adecentralized\u7684\u7ed3\u6784\uff1b redis\u4e2d\uff0c\u5f53cluster\u89e3\u51b3master election\u91c7\u7528\u7684\u662fraft\u7b97\u6cd5\u7684\u5b9e\u73b0\u601d\u8def\uff1b","title":"Introduction"},{"location":"cluster/#redis#cluster","text":"","title":"Redis cluster"},{"location":"cluster/#_1","text":"redis cluster\u7684\u7ed3\u6784\u662fdecentralized \uff0c\u6240\u4ee5\u6bcf\u4e2anode\u4e3a\u4e86\u77e5\u9053\u5176\u4ed6node\u7684\u4fe1\u606f\u5fc5\u987b\u8981\u5c06\u5b83\u4eec\u4fdd\u5b58\u4e0b\u6765\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u6bcf\u4e2anode\u90fd\u9700\u8981\u6709\u4e00\u4e2a clusterState \uff0c\u5e76\u4e14redis cluster\u662f High-availability cluster \uff0c\u5373\u5b83\u9700\u8981\u652f\u6301failover\uff0c\u6240\u4ee5cluster\u9700\u8981\u6709\u80fd\u591f\u4fa6\u63a2cluster\u662f\u5426\u5f02\u5e38\u7684\u80fd\u529b\uff0c\u6240\u4ee5\u5b83\u9700\u8981\u4e0d\u65ad\u5730\u8fdb\u884cheartbeat\uff0c\u7136\u540e\u5c06\u5f97\u5230\u7684cluster\u4e2d\u7684\u5176\u4ed6node\u7684\u72b6\u6001\u4fdd\u5b58\u8d77\u6765\uff1b\u800craft\u7b97\u6cd5\u7684cluster\u7ed3\u6784\u662f\u6709centralized\u7684\uff0c\u5b83\u662f\u57fa\u4e8eleader-follower\u7684\uff0c\u4e0d\u662f\u4e00\u4e2adecentralized\u7684\u7ed3\u6784\uff1b redis\u4e2d\uff0c\u5f53cluster\u89e3\u51b3master election\u91c7\u7528\u7684\u662fraft\u7b97\u6cd5\u7684\u5b9e\u73b0\u601d\u8def\uff1b","title":"\u5b9e\u73b0\u6982\u8ff0"},{"location":"cluster/Gossip/","text":"Redis cluster gossip bangbangde redis cluster \u7684Gossip\u534f\u8bae\u4ecb\u7ecd aliyun \u6df1\u5165\u7406\u89e3redis cluster\u7684failover\u673a\u5236 alibabacloud In-depth Analysis of Redis Cluster Gossip Protocol","title":"Introduction"},{"location":"cluster/Gossip/#redis#cluster#gossip","text":"bangbangde redis cluster \u7684Gossip\u534f\u8bae\u4ecb\u7ecd aliyun \u6df1\u5165\u7406\u89e3redis cluster\u7684failover\u673a\u5236 alibabacloud In-depth Analysis of Redis Cluster Gossip Protocol","title":"Redis cluster gossip"},{"location":"cluster/Gossip/redis-cluster-gossip-aliyun/","text":"aliyun \u6df1\u5165\u89e3\u6790redis cluster gossip\u673a\u5236 \u793e\u533a\u7248redis cluster\u662f\u4e00\u4e2aP2P\u65e0\u4e2d\u5fc3\u8282\u70b9\u7684\u96c6\u7fa4\u67b6\u6784\uff0c\u4f9d\u9760gossip\u534f\u8bae\u4f20\u64ad\u534f\u540c\u81ea\u52a8\u5316\u4fee\u590d\u96c6\u7fa4\u7684\u72b6\u6001\u3002\u672c\u6587\u5c06\u6df1\u5165redis cluster gossip\u534f\u8bae\u7684\u7ec6\u8282\uff0c\u5256\u6790redis cluster gossip\u534f\u8bae\u673a\u5236\u5982\u4f55\u8fd0\u8f6c\u3002 \u534f\u8bae\u89e3\u6790 cluster gossip\u534f\u8bae\u5b9a\u4e49\u5728\u5728ClusterMsg\u8fd9\u4e2a\u7ed3\u6784\u4e2d\uff0c\u6e90\u7801\u5982\u4e0b\uff1a typedef struct { char sig [ 4 ]; /* Signature \"RCmb\" (Redis Cluster message bus). */ uint32_t totlen ; /* Total length of this message */ uint16_t ver ; /* Protocol version, currently set to 1. */ uint16_t port ; /* TCP base port number. */ uint16_t type ; /* Message type */ uint16_t count ; /* Only used for some kind of messages. */ uint64_t currentEpoch ; /* The epoch accordingly to the sending node. */ uint64_t configEpoch ; /* The config epoch if it's a master, or the last epoch advertised by its master if it is a slave. */ uint64_t offset ; /* Master replication offset if node is a master or processed replication offset if node is a slave. */ char sender [ CLUSTER_NAMELEN ]; /* Name of the sender node */ unsigned char myslots [ CLUSTER_SLOTS / 8 ]; char slaveof [ CLUSTER_NAMELEN ]; char myip [ NET_IP_STR_LEN ]; /* Sender IP, if not all zeroed. */ char notused1 [ 34 ]; /* 34 bytes reserved for future usage. */ uint16_t cport ; /* Sender TCP cluster bus port */ uint16_t flags ; /* Sender node flags */ unsigned char state ; /* Cluster state from the POV of the sender */ unsigned char mflags [ 3 ]; /* Message flags: CLUSTERMSG_FLAG[012]_... */ union clusterMsgData data ; } clusterMsg ; \u53ef\u4ee5\u5bf9\u6b64\u7ed3\u6784\u5c06\u6d88\u606f\u5206\u4e3a\u4e09\u90e8\u5206\uff1a 1\u3001sender\u7684\u57fa\u672c\u4fe1\u606f\uff1a sender\uff1a node name configEpoch\uff1a\u6bcf\u4e2amaster\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684configEpoch\u505a\u6807\u5fd7\uff0c\u5982\u679c\u548c\u5176\u4ed6master\u8282\u70b9\u51b2\u7a81\uff0c\u4f1a\u5f3a\u5236\u81ea\u589e\u4f7f\u672c\u8282\u70b9\u5728\u96c6\u7fa4\u4e2d\u552f\u4e00 slaveof\uff1amaster\u4fe1\u606f\uff0c\u5047\u5982\u672c\u8282\u70b9\u662fslave\u8282\u70b9\u7684\u8bdd\uff0c\u534f\u8bae\u5e26\u6709master\u4fe1\u606f offset\uff1a\u4e3b\u4ece\u590d\u5236\u7684\u504f\u79fb flags\uff1a\u672c\u8282\u70b9\u5f53\u524d\u7684\u72b6\u6001\uff0c\u6bd4\u5982 CLUSTER_NODE_HANDSHAKE\u3001CLUSTER_NODE_MEET mflags\uff1a\u672c\u6761\u6d88\u606f\u7684\u7c7b\u578b\uff0c\u76ee\u524d\u53ea\u6709\u4e24\u7c7b\uff1aCLUSTERMSG_FLAG0_PAUSED\u3001CLUSTERMSG_FLAG0_FORCEACK myslots\uff1a\u672c\u8282\u70b9\u8d1f\u8d23\u7684slots\u4fe1\u606f port: cport: ip: 2\u3001\u96c6\u7fa4\u89c6\u56fe\u7684\u57fa\u672c\u4fe1\u606f: currentEpoch\uff1a\u8868\u793a\u672c\u8282\u70b9\u5f53\u524d\u8bb0\u5f55\u7684\u6574\u4e2a\u96c6\u7fa4\u7684\u7edf\u4e00\u7684epoch\uff0c\u7528\u6765\u51b3\u7b56\u9009\u4e3e\u6295\u7968\u7b49\uff0c\u4e0econfigEpoch\u4e0d\u540c\u7684\u662f\uff1aconfigEpoch\u8868\u793a\u7684\u662fmaster\u8282\u70b9\u7684\u552f\u4e00\u6807\u5fd7\uff0ccurrentEpoch\u662f\u96c6\u7fa4\u7684\u552f\u4e00\u6807\u5fd7\u3002 3\u3001\u5177\u4f53\u7684\u6d88\u606f\uff0c\u5bf9\u5e94clsuterMsgData\u7ed3\u6784\u4e2d\u7684\u6570\u636e\uff1a **ping\u3001pong\u3001meet\uff1aclusterMsgDataGossip\uff0c**\u8fd9\u4e2a\u534f\u8bae\u5c06sender\u8282\u70b9\u4e2d\u4fdd\u5b58\u7684\u96c6\u7fa4\u6240\u6709\u8282\u70b9\u7684\u4fe1\u606f\u90fd\u53d1\u9001\u7ed9\u5bf9\u7aef\uff0c\u8282\u70b9\u4e2a\u6570\u5728clusterMsg\u7684\u5b57\u6bb5count\u4e2d\u5b9a\u4e49\uff0c\u8fd9\u4e2a\u534f\u8bae\u5305\u542b\u5176\u4ed6\u8282\u70b9\u7684\u4fe1\u606f\u7684\u5b57\u6bb5\u6709\uff1a nodename\uff1a ping_sent\uff1a\u6700\u8fd1\u4e00\u6b21sender\u8282\u70b9\u7ed9\u8be5\u8282\u70b9\u53d1\u9001ping\u7684\u65f6\u95f4\u70b9\u3002\u6536\u5230pong\u56de\u590d\u540eping_sent\u4f1a\u88ab\u8d4b\u503c\u4e3a0 \u8fd9\u91cc\u4f5c\u8005\u7528\u4e86\u4e00\u4e2a\u6280\u5de7\u53bb\u51cf\u5c11gossip\u901a\u4fe1\u5e26\u5bbd\u3002 \u5982\u679creceiver\u8282\u70b9\u4e0a\u5173\u4e8e\u8be5\u8282\u70b9\u7684ping_sent=0 \u5e76\u4e14\u6ca1\u6709\u4efb\u4f55\u8282\u70b9\u6b63\u5728failover&\u8be5\u8282\u70b9\u6ca1\u6709fail&receiver\u8282\u70b9\u4e0a\u5173\u4e8e\u8be5\u8282\u70b9\u7684pong_received<sender\u4e0a\u7684pong_received\u5e76\u4e14sender\u7684pong_received\u5927\u4e8ereceiver\u8282\u70b9\u5185\u6838\u65f6\u95f4\u7684500ms\u5185\uff0c\u5219\u5c06receiver\u8282\u70b9\u5173\u4e8e\u8be5\u8282\u70b9\u7684pong_received\u65f6\u95f4\u8bbe\u7f6e\u4e3a\u548csender\u8282\u70b9\u4e00\u81f4\uff0c\u590d\u7528sender\u8282\u70b9\u7684pong_received\u3002\u90a3\u4e48received\u8282\u70b9\u5219\u4f1a\u51cf\u5c11\u5bf9\u8be5\u8282\u70b9\u53d1\u9001ping\u3002\u53c2\u8003issue: https://github.com/antirez/redis/issues/3929 pong_received\uff1a\u6700\u8fd1\u4e00\u6b21sender\u8282\u70b9\u6536\u5230\u8be5\u8282\u70b9\u53d1\u9001pong\u7684\u65f6\u95f4\u70b9 ip\uff1a port\uff1a cport\uff1a flags\uff1a\u5bf9\u5e94clusterMsg\u7684flags\uff0c\u53ea\u4e0d\u8fc7\u5b58\u50a8\u7684\u5176\u4ed6\u8282\u70b9\u7684 **fail\uff1aclusterMsgDataFail\uff0c**\u53ea\u6709\u4e00\u4e2a\u8868\u793afail\u8282\u70b9\u7684nodename\u5b57\u6bb5, \u7edf\u8ba1\u8d85\u8fc7\u4e00\u534a\u4ee5\u4e0a\u8282\u70b9\u4efb\u52a1node pfail\u540e\u53d1\u9001fail msg publish\uff1aclusterMsgDataPublish \uff0c\u96c6\u7fa4\u95f4\u540c\u6b65publish\u4fe1\u606f\uff0c\u4ee5\u652f\u6301\u5ba2\u6237\u7aef\u5728\u4efb\u4e00\u8282\u70b9\u53d1\u9001pub/sub update\uff1aclusterMsgDataUpdate \uff0c\u5f53receiver\u8282\u70b9\u53d1\u73b0sender\u8282\u70b9\u7684configepoch\u4f4e\u4e8e\u672c\u8282\u70b9\u7684\u65f6\u5019\uff0c\u4f1a\u7ed9sender\u8282\u70b9\u53d1\u9001\u4e00\u4e2aupdate\u6d88\u606f\u901a\u77e5sender\u8282\u70b9\u66f4\u65b0\u72b6\u6001\uff0c\u5305\u542b\uff1a configEpoch\uff1areceiver\u8282\u70b9\u4e2d\u4fdd\u5b58\u7684sender\u8282\u70b9\u7684configepoch nodename\uff1areceiver\u8282\u70b9\u4e2d\u4fdd\u5b58\u7684sender\u8282\u70b9\u7684nodename slots\uff1areceiver\u8282\u70b9\u4e2d\u4fdd\u5b58\u7684sender\u8282\u70b9\u7684slots\u5217\u8868 \u8fd0\u8f6c\u673a\u5236 \u901a\u8fc7gossip\u534f\u8bae\uff0ccluster\u53ef\u4ee5\u63d0\u4f9b\u96c6\u7fa4\u95f4\u72b6\u6001\u540c\u6b65\u66f4\u65b0\u3001\u9009\u4e3e\u81ea\u52a9failover\u7b49\u91cd\u8981\u7684\u96c6\u7fa4\u529f\u80fd\u3002 \u63e1\u624b\u8054\u7ed3 \u5ba2\u6237\u7aef\u7ed9\u8282\u70b9 X \u53d1\u9001 cluster meet \u8282\u70b9Y \u7684\u8bf7\u6c42\u540e\uff0c\u8282\u70b9 X \u4e4b\u540e\u5c31\u4f1a\u5c1d\u8bd5\u4e3b\u4ece\u548c\u8282\u70b9 Y \u5efa\u7acb\u8fde\u63a5\u3002\u6b64\u65f6\u5728\u8282\u70b9 X \u4e2d\u4fdd\u5b58\u8282\u70b9 Y \u7684\u72b6\u6001\u662f\uff1a CLUSTER_NODE_HANDSHAKE \uff1a\u8868\u793a\u8282\u70b9Y\u6b63\u5904\u4e8e\u63e1\u624b\u72b6\u6001\uff0c\u53ea\u6709\u6536\u5230\u6765\u81ea\u8282\u70b9Y\u7684ping\u3001pong\u3001meet\u5176\u4e2d\u4e00\u79cd\u6d88\u606f\u540e\u8be5\u72b6\u6001\u624d\u4f1a\u88ab\u6e05\u9664 CLUSTER_NODE_MEET \uff1a\u8868\u793a\u8fd8\u672a\u7ed9\u8282\u70b9 Y \u53d1\u9001meet\u6d88\u606f\uff0c\u4e00\u65e6\u53d1\u9001\u8be5\u72b6\u6001\u6e05\u9664\uff0c\u4e0d\u7ba1\u662f\u5426\u6210\u529f \u4ee5\u4e0b\u662fmeet\u8fc7\u7a0b\uff1a \uff080\uff09\u8282\u70b9 X \u901a\u8fc7 getRandomHexChars \u8fd9\u4e2a\u51fd\u6570\u7ed9\u8282\u70b9Y\u968f\u673a\u751f\u6210 nodename \uff081\uff09\u8282\u70b9 X \u5728 clusterCron \u8fd0\u8f6c\u65f6\u4f1a\u4ece cluster->nodes \u5217\u8868\u4e2d\u83b7\u53d6\u672a\u5efa\u7acbtcp\u8fde\u63a5\uff0c\u5982\u672a\u53d1\u9001\u8fc7meet\uff0c\u53d1\u9001 CLUSTERMSG_TYPE_MEET \uff0c\u8282\u70b9Y\u6536\u5230meet\u6d88\u606f\u540e\uff1a \uff082\uff09\u67e5\u770b\u8282\u70b9 X \u8fd8\u672a\u5efa\u7acb\u63e1\u624b\u6210\u529f\uff0c\u6bd4\u8f83sender\u53d1\u9001\u8fc7\u6765\u7684\u6d88\u606f\uff0c\u66f4\u65b0\u672c\u5730\u5173\u4e8e\u8282\u70b9 X \u7684\u4fe1\u606f \uff083\uff09\u67e5\u770b\u8282\u70b9 X \u5728 nodes \u4e0d\u5b58\u5728\uff0c\u6dfb\u52a0X\u8fdbnodes\uff0c\u968f\u673a\u7ed9X\u53d6nodename\u3002\u72b6\u6001\u8bbe\u7f6e\u4e3a CLUSTER_NODE_HANDSHAKE \uff084\uff09\u8fdb\u5165gossip\u5904\u7406\u8fd9\u4e2agossip\u6d88\u606f\u643a\u5e26\u7684\u96c6\u7fa4\u5176\u4ed6\u8282\u70b9\u7684\u4fe1\u606f\uff0c\u7ed9\u96c6\u7fa4\u5176\u4ed6\u8282\u70b9\u5efa\u7acb**\u63e1\u624b**\u3002 \uff085\uff09\u7ed9\u8282\u70b9X\u53d1\u9001 CLUSTERMSG_TYPE_PONG \uff0c\u8282\u70b9Y\u5904\u7406\u7ed3\u675f\uff08\u6ce8\u610f\u6b64\u65f6\u8282\u70b9Y\u7684clusterReadHandler\u51fd\u6570link->node\u4e3aNULL\uff09\u3002 \uff086\uff09\u8282\u70b9X\u6536\u5230pong\u540e\uff0c\u53d1\u73b0\u548c\u8282\u70b9Y\u6b63\u5904\u5728\u63e1\u624b\u9636\u6bb5\uff0c\u66f4\u65b0\u8282\u70b9Y\u7684\u5730\u5740\u548cnodename\uff0c\u6e05\u9664 CLUSTER_NODE_HANDSHAKE \u72b6\u6001\u3002 \uff087\uff09\u8282\u70b9X\u5728 cron() \u51fd\u6570\u4e2d\u5c06\u7ed9\u672a\u5efa\u7acb\u8fde\u63a5\u7684\u8282\u70b9Y\u53d1\u9001ping \uff088\uff09\u8282\u70b9Y\u6536\u5230ping\u540e\u7ed9\u8282\u70b9X\u53d1\u9001pong \uff089\uff09\u8282\u70b9X\u5c06\u4fdd\u5b58\u7684\u8282\u70b9Y\u7684\u72b6\u6001CLUSTER_NODE_HANDSHAKE\u6e05\u9664\uff0c\u66f4\u65b0\u4e00\u4e0bnodename\u548c\u5730\u5740\uff0c\u81f3\u6b64\u63e1\u624b\u5b8c\u6210\uff0c\u4e24\u4e2a\u8282\u70b9\u90fd\u4fdd\u5b58\u76f8\u540c\u7684nodename\u548c\u4fe1\u606f\u3002 \u770b\u5b8c\u6574\u4e2a\u63e1\u624b\u8fc7\u7a0b\u540e\uff0c\u6211\u4eec\u5c1d\u8bd5\u601d\u8003\u4e24\u4e2a\u95ee\u9898\uff1a 1\u3001\u5982\u679c\u53d1\u9001meet\u5931\u8d25\u540e\uff0c\u8282\u70b9X\u7684\u72b6\u6001 CLUSTER_NODE_MEET \u72b6\u6001\u53c8\u88ab\u6e05\u9664\u4e86\uff0ccluster\u4f1a\u5982\u4f55\u5904\u7406\u5462\uff1f \u8fd9\u65f6\u5019\u8282\u70b9Y\u5728\u4e0b\u4e00\u4e2a clusterCron() \u51fd\u6570\u4e2d\u4f1a\u76f4\u63a5\u7ed9\u8282\u70b9Y\u53d1\u9001ping\uff0c\u4f46\u662f\u4e0d\u4f1a\u5c06\u8282\u70b9X\u5b58\u5165cluster->nodes\uff0c\u5bfc\u81f4\u8282\u70b9X\u8ba4\u4e3a\u5df2\u7ecf\u5efa\u7acb\u8fde\u63a5\uff0c\u7136\u800c\u8282\u70b9Y\u5e76\u6ca1\u6709\u627f\u8ba4\u3002\u5728\u540e\u9762\u8282\u70b9\u4f20\u64ad\u4e2d\uff0c\u5982\u679c\u6709\u5176\u4ed6\u8282\u70b9\u6301\u6709\u8282\u70b9X\u7684\u4fe1\u606f\u5e76\u7ed9\u8282\u70b9Y\u53d1\u9001ping\uff0c\u4e5f\u4f1a\u89e6\u53d1\u8282\u70b9Y\u4e3b\u52a8\u518d\u53bb\u7ed9\u8282\u70b9X\u53d1\u9001meet\u5efa\u7acb\u8fde\u63a5\u3002 2\u3001\u5982\u679c\u8282\u70b9Y\u5df2\u7ecf\u6709\u5b58\u50a8\u8282\u70b9X\uff0c\u4f46\u8fd8\u662f\u6536\u5230\u4e86\u8282\u70b9X\u7684meet\u8bf7\u6c42\uff0c\u5982\u4f55\u5904\u7406\uff1f nodename\u76f8\u540c\uff1a \uff081\uff09\u8282\u70b9Y\u53d1\u9001pong\u7ed9\u8282\u70b9X \uff082\uff09\u5982\u679c\u6b63\u5904\u4e8e\u63e1\u624b\u8282\u70b9\uff0c\u4f1a\u76f4\u63a5\u5220\u9664\u8282\u70b9\uff0c\u8fd9\u91cc\u4f1a\u5bfc\u81f4\u8282\u70b9Y\u4e22\u5931\u4e86\u8282\u70b9X\u7684\u6d88\u606f\u3002\u76f8\u5f53\u4e8e\u95ee\u98981\u3002 \uff083\uff09\u975e\u63e1\u624b\u9636\u6bb5\u5f80\u4e0b\u8d70\u6b63\u5e38\u7684ping\u6d41\u7a0b nodename\u4e0d\u540c\uff1a \uff081\uff09\u8282\u70b9Y\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a\u968f\u673anodename\u653e\u5165nodes\u4e2d\u5e76\u8bbe\u7f6e\u4e3a\u63e1\u624b\u9636\u6bb5\uff0c\u6b64\u65f6\u6709\u4e24\u4e2anodename\u5b58\u5728\u3002 \uff082\uff09\u8282\u70b9Y\u53d1\u9001pong\u7ed9\u8282\u70b9X \uff083\uff09\u8282\u70b9Y\u5982\u679c\u5df2\u7ecf\u521b\u5efa\u8fc7\u548c\u8282\u70b9X\u7684\u8fde\u63a5\uff0c\u8282\u70b9Y\u4f1a\u5728\u672c\u5730\u66f4\u65b0\u8282\u70b9X\u7684nodename\uff0c\u5220\u9664\u7b2c\u4e00\u4e2anodename\u5b58\u50a8\u7684node\uff0c\u66f4\u65b0\u63e1\u624b\u72b6\u6001\uff0c\u6b64\u65f6\u53ea\u5269\u4e0b\u7b2c\u4e8c\u4e2a\u6b63\u786e\u7684nodename\u3002 \uff084\uff09\u8282\u70b9Y\u5982\u679c\u6ca1\u521b\u5efa\u8fc7\u548c\u8282\u70b9X\u7684\u94fe\u63a5\uff0c\u4f1a\u5728clustercron(\uff09\u4e2d\u518d\u6b21\u7ed9\u8282\u70b9X\u53d1\u9001ping\u8bf7\u6c42\uff0c\u4e24\u4e2anodename\u4f1a\u5148\u540e\u5404\u53d1\u9001\u4e00\u6b21\u3002 \uff085\uff09\u7b2c\u4e00\u4e2anodename\u53d1\u9001ping\u540e\uff0c\u5728\u6536\u5230\u8282\u70b9X\u56de\u590d\u7684pong\u4e2d\uff0c\u66f4\u65b0\u8282\u70b9X\u7684nodename \uff086\uff09\u7b2c\u4e8c\u4e2anodename\u53d1\u9001ping\u540e\uff0c\u5728\u6536\u5230\u8282\u70b9X\u56de\u590d\u7684pong\u4e2d\uff0c\u53d1\u9001\u8282\u70b9X\u7684nodename\u5df2\u7ecf\u5b58\u5728\uff0c\u7b2c\u4e8c\u4e2anodename\u5904\u4e8e\u63e1\u624b\u72b6\u6001\uff0c\u8fd9\u65f6\u5019\u76f4\u63a5\u5220\u9664\u4e86\u7b2c\u4e8c\u4e2anodename\u3002 \u7ed3\u8bba\uff1a\u53ea\u6709nodename\u76f8\u540c\u5e76\u4e14\u4e24\u4e2a\u8282\u70b9\u90fd\u5728\u63e1\u624b\u9636\u6bb5\uff0c\u4f1a\u5bfc\u81f4\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u4e22\u6389\u53e6\u5916\u4e00\u4e2a\u8282\u70b9\u3002 \u5065\u5eb7\u68c0\u6d4b\u53cafailover \u8be6\u60c5\u89c1\u6587\u7ae0\uff1a https://yq.aliyun.com/articles/638627?utm_content=m_1000016044 \u72b6\u6001\u66f4\u65b0\u53ca\u51b2\u7a81\u89e3\u51b3 \u5047\u5982\u51fa\u73b0\u4e24\u4e2amaster\u7684\u65f6\u5019gossip\u534f\u8bae\u662f\u5982\u4f55\u5904\u7406\u51b2\u7a81\u7684\u5462\uff1f \u9996\u5148\u8981\u7406\u89e3\u4e24\u4e2a\u91cd\u8981\u7684\u53d8\u91cf\uff1a configEpoch\uff1a \u6bcf\u4e2a\u5206\u7247\u6709\u552f\u4e00\u7684epoch\u503c\uff0c\u4e3b\u5907epoch\u5e94\u8be5\u4e00\u81f4 currentEpoch\uff1a\u96c6\u7fa4\u5f53\u524d\u7684epoch\uff0c=\u96c6\u7fa4\u4e2d\u6700\u5927\u5206\u7247\u7684epoch \u5728ping\u5305\u4e2d\u4f1a\u81ea\u5e26sender\u8282\u70b9\u7684slots\u4fe1\u606f\u548ccurrentEpoch, configEpoch\u3002 master\u8282\u70b9\u6536\u5230\u6765\u81easlave\u8282\u70b9\u540e\u7684\u5904\u7406\u6d41\u7a0b\uff1a \uff081\uff09receiver\u6bd4\u8f83sender\u7684\u89d2\u8272\uff0c \u5982\u679csender\u8ba4\u4e3a\u81ea\u5df1\u662fmaster\uff0c\u4f46\u662f\u5728receiver\u88ab\u6807\u8bb0\u4e3aslave\uff0c\u5219receiver\u8282\u70b9\u5728\u96c6\u7fa4\u89c6\u56fe\u4e2d\u5c06sender\u6807\u8bb0\u4e3amaster\u3002 \u5982\u679csender\u8ba4\u4e3a\u81ea\u5df1\u662fslave\uff0c\u4f46\u662f\u5728receiver\u88ab\u6807\u8bb0\u4e3amaster, \u5219\u5728receiver\u7684\u96c6\u7fa4\u89c6\u56fe\u4e2d\u5c06sender\u6807\u8bb0\u4e3aslave, \u52a0\u5165\u5230sender\u6807\u8bb0\u7684master\u4e2d\uff0c\u5e76\u4e14\u5220\u9664sender\u5728reciver\u96c6\u7fa4\u89c6\u56fe\u4e2d\u7684slots\u4fe1\u606f\u3002 \uff082\uff09\u6bd4\u8f83sender\u81ea\u5e26\u7684slot\u4fe1\u606f\u548creceiver\u96c6\u7fa4\u89c6\u56fe\u4e2d\u7684slots\u662f\u5426\u51b2\u7a81\uff0c\u6709\u51b2\u7a81\u5219\u8fdb\u884c\u4e0b\u4e00\u6b65\u6bd4\u8f83 \uff083\uff09\u6bd4\u8f83sender\u7684configEpoch \u662f\u5426 > receiver\u96c6\u7fa4\u89c6\u56fe\u4e2d\u7684slots\u62e5\u6709\u8005\u7684configepoch\uff0c\u5982\u662f\u5728clusterUpdateSlotsConfigWith\u51fd\u6570\u4e2d\u91cd\u65b0\u8bbe\u7f6eslots\u62e5\u6709\u8005\u4e3asender\uff0c\u5e76\u4e14\u5c06\u65e7slots\u62e5\u6709\u8005\u8bbe\u7f6e\u4e3asender\u7684slave\uff0c\u518d\u6bd4\u8f83\u672c\u8282\u70b9\u662f\u6709\u810fslot, \u6709\u5219\u6e05\u9664\u6389\u3002 \uff084\uff09\u6bd4\u8f83sender\u81ea\u8eab\u7684slots\u4fe1\u606f < receiver\u96c6\u7fa4\u89c6\u56fe\u4e2d\u7684slots\u62e5\u6709\u8005\u7684configepoch\uff0c\u53d1\u9001update\u4fe1\u606f\uff0c\u901a\u77e5sender\u66f4\u65b0\uff0csender\u8282\u70b9\u4e5f\u4f1a\u6267\u884cclusterUpdateSlotsConfigWith\u51fd\u6570\u3002 \u5982\u679c\u4e24\u4e2a\u8282\u70b9\u7684configEpoch, currentEpoch\uff0c\u89d2\u8272\u90fd\u662fmaster\uff0c \u8fd9\u65f6\u5019\u5982\u4f55\u5904\u7406\u5462\uff1f receiver\u7684currentEpoch\u81ea\u589e\u5e76\u4e14\u8d4b\u503c\u7ed9configEpoch\uff0c\u4e5f\u5c31\u662f\u5f3a\u5236\u81ea\u589e\u6765\u89e3\u51b3\u51b2\u7a81\u3002\u8fd9\u65f6\u5019\u56e0\u4e3aconfigEpoch\u5927\uff0c\u53c8\u53ef\u4ee5\u8d70\u56de\u4e0a\u6587\u7684\u6d41\u7a0b\u3002 \u6240\u4ee5\u53ef\u80fd\u5b58\u5728\u53ccmaster\u540c\u65f6\u5b58\u5728\u7684\u60c5\u51b5\uff0c\u4f46\u662f\u6700\u7ec8\u4f1a\u6311\u9009\u51fa\u65b0\u7684master\u3002 \u7ed3\u675f\u8bed \u4e91\u6570\u636e\u5e93Redis\u7248\uff08ApsaraDB for Redis\uff09\u662f\u4e00\u79cd\u7a33\u5b9a\u53ef\u9760\u3001\u6027\u80fd\u5353\u8d8a\u3001\u53ef\u5f39\u6027\u4f38\u7f29\u7684\u6570\u636e\u5e93\u670d\u52a1\u3002\u57fa\u4e8e\u98de\u5929\u5206\u5e03\u5f0f\u7cfb\u7edf\u548c\u5168SSD\u76d8\u9ad8\u6027\u80fd\u5b58\u50a8\uff0c\u652f\u6301\u4e3b\u5907\u7248\u548c\u96c6\u7fa4\u7248\u4e24\u5957\u9ad8\u53ef\u7528\u67b6\u6784\u3002\u63d0\u4f9b\u4e86\u5168\u5957\u7684\u5bb9\u707e\u5207\u6362\u3001\u6545\u969c\u8fc1\u79fb\u3001\u5728\u7ebf\u6269\u5bb9\u3001\u6027\u80fd\u4f18\u5316\u7684\u6570\u636e\u5e93\u89e3\u51b3\u65b9\u6848\u3002\u6b22\u8fce\u5404\u4f4d\u8d2d\u4e70\u4f7f\u7528: \u4e91\u6570\u636e\u5e93 Redis \u7248","title":"Introduction"},{"location":"cluster/Gossip/redis-cluster-gossip-aliyun/#aliyun#redis#cluster#gossip","text":"\u793e\u533a\u7248redis cluster\u662f\u4e00\u4e2aP2P\u65e0\u4e2d\u5fc3\u8282\u70b9\u7684\u96c6\u7fa4\u67b6\u6784\uff0c\u4f9d\u9760gossip\u534f\u8bae\u4f20\u64ad\u534f\u540c\u81ea\u52a8\u5316\u4fee\u590d\u96c6\u7fa4\u7684\u72b6\u6001\u3002\u672c\u6587\u5c06\u6df1\u5165redis cluster gossip\u534f\u8bae\u7684\u7ec6\u8282\uff0c\u5256\u6790redis cluster gossip\u534f\u8bae\u673a\u5236\u5982\u4f55\u8fd0\u8f6c\u3002","title":"aliyun \u6df1\u5165\u89e3\u6790redis cluster gossip\u673a\u5236"},{"location":"cluster/Gossip/redis-cluster-gossip-aliyun/#_1","text":"cluster gossip\u534f\u8bae\u5b9a\u4e49\u5728\u5728ClusterMsg\u8fd9\u4e2a\u7ed3\u6784\u4e2d\uff0c\u6e90\u7801\u5982\u4e0b\uff1a typedef struct { char sig [ 4 ]; /* Signature \"RCmb\" (Redis Cluster message bus). */ uint32_t totlen ; /* Total length of this message */ uint16_t ver ; /* Protocol version, currently set to 1. */ uint16_t port ; /* TCP base port number. */ uint16_t type ; /* Message type */ uint16_t count ; /* Only used for some kind of messages. */ uint64_t currentEpoch ; /* The epoch accordingly to the sending node. */ uint64_t configEpoch ; /* The config epoch if it's a master, or the last epoch advertised by its master if it is a slave. */ uint64_t offset ; /* Master replication offset if node is a master or processed replication offset if node is a slave. */ char sender [ CLUSTER_NAMELEN ]; /* Name of the sender node */ unsigned char myslots [ CLUSTER_SLOTS / 8 ]; char slaveof [ CLUSTER_NAMELEN ]; char myip [ NET_IP_STR_LEN ]; /* Sender IP, if not all zeroed. */ char notused1 [ 34 ]; /* 34 bytes reserved for future usage. */ uint16_t cport ; /* Sender TCP cluster bus port */ uint16_t flags ; /* Sender node flags */ unsigned char state ; /* Cluster state from the POV of the sender */ unsigned char mflags [ 3 ]; /* Message flags: CLUSTERMSG_FLAG[012]_... */ union clusterMsgData data ; } clusterMsg ; \u53ef\u4ee5\u5bf9\u6b64\u7ed3\u6784\u5c06\u6d88\u606f\u5206\u4e3a\u4e09\u90e8\u5206\uff1a 1\u3001sender\u7684\u57fa\u672c\u4fe1\u606f\uff1a sender\uff1a node name configEpoch\uff1a\u6bcf\u4e2amaster\u8282\u70b9\u90fd\u6709\u4e00\u4e2a\u552f\u4e00\u7684configEpoch\u505a\u6807\u5fd7\uff0c\u5982\u679c\u548c\u5176\u4ed6master\u8282\u70b9\u51b2\u7a81\uff0c\u4f1a\u5f3a\u5236\u81ea\u589e\u4f7f\u672c\u8282\u70b9\u5728\u96c6\u7fa4\u4e2d\u552f\u4e00 slaveof\uff1amaster\u4fe1\u606f\uff0c\u5047\u5982\u672c\u8282\u70b9\u662fslave\u8282\u70b9\u7684\u8bdd\uff0c\u534f\u8bae\u5e26\u6709master\u4fe1\u606f offset\uff1a\u4e3b\u4ece\u590d\u5236\u7684\u504f\u79fb flags\uff1a\u672c\u8282\u70b9\u5f53\u524d\u7684\u72b6\u6001\uff0c\u6bd4\u5982 CLUSTER_NODE_HANDSHAKE\u3001CLUSTER_NODE_MEET mflags\uff1a\u672c\u6761\u6d88\u606f\u7684\u7c7b\u578b\uff0c\u76ee\u524d\u53ea\u6709\u4e24\u7c7b\uff1aCLUSTERMSG_FLAG0_PAUSED\u3001CLUSTERMSG_FLAG0_FORCEACK myslots\uff1a\u672c\u8282\u70b9\u8d1f\u8d23\u7684slots\u4fe1\u606f port: cport: ip: 2\u3001\u96c6\u7fa4\u89c6\u56fe\u7684\u57fa\u672c\u4fe1\u606f: currentEpoch\uff1a\u8868\u793a\u672c\u8282\u70b9\u5f53\u524d\u8bb0\u5f55\u7684\u6574\u4e2a\u96c6\u7fa4\u7684\u7edf\u4e00\u7684epoch\uff0c\u7528\u6765\u51b3\u7b56\u9009\u4e3e\u6295\u7968\u7b49\uff0c\u4e0econfigEpoch\u4e0d\u540c\u7684\u662f\uff1aconfigEpoch\u8868\u793a\u7684\u662fmaster\u8282\u70b9\u7684\u552f\u4e00\u6807\u5fd7\uff0ccurrentEpoch\u662f\u96c6\u7fa4\u7684\u552f\u4e00\u6807\u5fd7\u3002 3\u3001\u5177\u4f53\u7684\u6d88\u606f\uff0c\u5bf9\u5e94clsuterMsgData\u7ed3\u6784\u4e2d\u7684\u6570\u636e\uff1a **ping\u3001pong\u3001meet\uff1aclusterMsgDataGossip\uff0c**\u8fd9\u4e2a\u534f\u8bae\u5c06sender\u8282\u70b9\u4e2d\u4fdd\u5b58\u7684\u96c6\u7fa4\u6240\u6709\u8282\u70b9\u7684\u4fe1\u606f\u90fd\u53d1\u9001\u7ed9\u5bf9\u7aef\uff0c\u8282\u70b9\u4e2a\u6570\u5728clusterMsg\u7684\u5b57\u6bb5count\u4e2d\u5b9a\u4e49\uff0c\u8fd9\u4e2a\u534f\u8bae\u5305\u542b\u5176\u4ed6\u8282\u70b9\u7684\u4fe1\u606f\u7684\u5b57\u6bb5\u6709\uff1a nodename\uff1a ping_sent\uff1a\u6700\u8fd1\u4e00\u6b21sender\u8282\u70b9\u7ed9\u8be5\u8282\u70b9\u53d1\u9001ping\u7684\u65f6\u95f4\u70b9\u3002\u6536\u5230pong\u56de\u590d\u540eping_sent\u4f1a\u88ab\u8d4b\u503c\u4e3a0 \u8fd9\u91cc\u4f5c\u8005\u7528\u4e86\u4e00\u4e2a\u6280\u5de7\u53bb\u51cf\u5c11gossip\u901a\u4fe1\u5e26\u5bbd\u3002 \u5982\u679creceiver\u8282\u70b9\u4e0a\u5173\u4e8e\u8be5\u8282\u70b9\u7684ping_sent=0 \u5e76\u4e14\u6ca1\u6709\u4efb\u4f55\u8282\u70b9\u6b63\u5728failover&\u8be5\u8282\u70b9\u6ca1\u6709fail&receiver\u8282\u70b9\u4e0a\u5173\u4e8e\u8be5\u8282\u70b9\u7684pong_received<sender\u4e0a\u7684pong_received\u5e76\u4e14sender\u7684pong_received\u5927\u4e8ereceiver\u8282\u70b9\u5185\u6838\u65f6\u95f4\u7684500ms\u5185\uff0c\u5219\u5c06receiver\u8282\u70b9\u5173\u4e8e\u8be5\u8282\u70b9\u7684pong_received\u65f6\u95f4\u8bbe\u7f6e\u4e3a\u548csender\u8282\u70b9\u4e00\u81f4\uff0c\u590d\u7528sender\u8282\u70b9\u7684pong_received\u3002\u90a3\u4e48received\u8282\u70b9\u5219\u4f1a\u51cf\u5c11\u5bf9\u8be5\u8282\u70b9\u53d1\u9001ping\u3002\u53c2\u8003issue: https://github.com/antirez/redis/issues/3929 pong_received\uff1a\u6700\u8fd1\u4e00\u6b21sender\u8282\u70b9\u6536\u5230\u8be5\u8282\u70b9\u53d1\u9001pong\u7684\u65f6\u95f4\u70b9 ip\uff1a port\uff1a cport\uff1a flags\uff1a\u5bf9\u5e94clusterMsg\u7684flags\uff0c\u53ea\u4e0d\u8fc7\u5b58\u50a8\u7684\u5176\u4ed6\u8282\u70b9\u7684 **fail\uff1aclusterMsgDataFail\uff0c**\u53ea\u6709\u4e00\u4e2a\u8868\u793afail\u8282\u70b9\u7684nodename\u5b57\u6bb5, \u7edf\u8ba1\u8d85\u8fc7\u4e00\u534a\u4ee5\u4e0a\u8282\u70b9\u4efb\u52a1node pfail\u540e\u53d1\u9001fail msg publish\uff1aclusterMsgDataPublish \uff0c\u96c6\u7fa4\u95f4\u540c\u6b65publish\u4fe1\u606f\uff0c\u4ee5\u652f\u6301\u5ba2\u6237\u7aef\u5728\u4efb\u4e00\u8282\u70b9\u53d1\u9001pub/sub update\uff1aclusterMsgDataUpdate \uff0c\u5f53receiver\u8282\u70b9\u53d1\u73b0sender\u8282\u70b9\u7684configepoch\u4f4e\u4e8e\u672c\u8282\u70b9\u7684\u65f6\u5019\uff0c\u4f1a\u7ed9sender\u8282\u70b9\u53d1\u9001\u4e00\u4e2aupdate\u6d88\u606f\u901a\u77e5sender\u8282\u70b9\u66f4\u65b0\u72b6\u6001\uff0c\u5305\u542b\uff1a configEpoch\uff1areceiver\u8282\u70b9\u4e2d\u4fdd\u5b58\u7684sender\u8282\u70b9\u7684configepoch nodename\uff1areceiver\u8282\u70b9\u4e2d\u4fdd\u5b58\u7684sender\u8282\u70b9\u7684nodename slots\uff1areceiver\u8282\u70b9\u4e2d\u4fdd\u5b58\u7684sender\u8282\u70b9\u7684slots\u5217\u8868","title":"\u534f\u8bae\u89e3\u6790"},{"location":"cluster/Gossip/redis-cluster-gossip-aliyun/#_2","text":"\u901a\u8fc7gossip\u534f\u8bae\uff0ccluster\u53ef\u4ee5\u63d0\u4f9b\u96c6\u7fa4\u95f4\u72b6\u6001\u540c\u6b65\u66f4\u65b0\u3001\u9009\u4e3e\u81ea\u52a9failover\u7b49\u91cd\u8981\u7684\u96c6\u7fa4\u529f\u80fd\u3002","title":"\u8fd0\u8f6c\u673a\u5236"},{"location":"cluster/Gossip/redis-cluster-gossip-aliyun/#_3","text":"\u5ba2\u6237\u7aef\u7ed9\u8282\u70b9 X \u53d1\u9001 cluster meet \u8282\u70b9Y \u7684\u8bf7\u6c42\u540e\uff0c\u8282\u70b9 X \u4e4b\u540e\u5c31\u4f1a\u5c1d\u8bd5\u4e3b\u4ece\u548c\u8282\u70b9 Y \u5efa\u7acb\u8fde\u63a5\u3002\u6b64\u65f6\u5728\u8282\u70b9 X \u4e2d\u4fdd\u5b58\u8282\u70b9 Y \u7684\u72b6\u6001\u662f\uff1a CLUSTER_NODE_HANDSHAKE \uff1a\u8868\u793a\u8282\u70b9Y\u6b63\u5904\u4e8e\u63e1\u624b\u72b6\u6001\uff0c\u53ea\u6709\u6536\u5230\u6765\u81ea\u8282\u70b9Y\u7684ping\u3001pong\u3001meet\u5176\u4e2d\u4e00\u79cd\u6d88\u606f\u540e\u8be5\u72b6\u6001\u624d\u4f1a\u88ab\u6e05\u9664 CLUSTER_NODE_MEET \uff1a\u8868\u793a\u8fd8\u672a\u7ed9\u8282\u70b9 Y \u53d1\u9001meet\u6d88\u606f\uff0c\u4e00\u65e6\u53d1\u9001\u8be5\u72b6\u6001\u6e05\u9664\uff0c\u4e0d\u7ba1\u662f\u5426\u6210\u529f \u4ee5\u4e0b\u662fmeet\u8fc7\u7a0b\uff1a \uff080\uff09\u8282\u70b9 X \u901a\u8fc7 getRandomHexChars \u8fd9\u4e2a\u51fd\u6570\u7ed9\u8282\u70b9Y\u968f\u673a\u751f\u6210 nodename \uff081\uff09\u8282\u70b9 X \u5728 clusterCron \u8fd0\u8f6c\u65f6\u4f1a\u4ece cluster->nodes \u5217\u8868\u4e2d\u83b7\u53d6\u672a\u5efa\u7acbtcp\u8fde\u63a5\uff0c\u5982\u672a\u53d1\u9001\u8fc7meet\uff0c\u53d1\u9001 CLUSTERMSG_TYPE_MEET \uff0c\u8282\u70b9Y\u6536\u5230meet\u6d88\u606f\u540e\uff1a \uff082\uff09\u67e5\u770b\u8282\u70b9 X \u8fd8\u672a\u5efa\u7acb\u63e1\u624b\u6210\u529f\uff0c\u6bd4\u8f83sender\u53d1\u9001\u8fc7\u6765\u7684\u6d88\u606f\uff0c\u66f4\u65b0\u672c\u5730\u5173\u4e8e\u8282\u70b9 X \u7684\u4fe1\u606f \uff083\uff09\u67e5\u770b\u8282\u70b9 X \u5728 nodes \u4e0d\u5b58\u5728\uff0c\u6dfb\u52a0X\u8fdbnodes\uff0c\u968f\u673a\u7ed9X\u53d6nodename\u3002\u72b6\u6001\u8bbe\u7f6e\u4e3a CLUSTER_NODE_HANDSHAKE \uff084\uff09\u8fdb\u5165gossip\u5904\u7406\u8fd9\u4e2agossip\u6d88\u606f\u643a\u5e26\u7684\u96c6\u7fa4\u5176\u4ed6\u8282\u70b9\u7684\u4fe1\u606f\uff0c\u7ed9\u96c6\u7fa4\u5176\u4ed6\u8282\u70b9\u5efa\u7acb**\u63e1\u624b**\u3002 \uff085\uff09\u7ed9\u8282\u70b9X\u53d1\u9001 CLUSTERMSG_TYPE_PONG \uff0c\u8282\u70b9Y\u5904\u7406\u7ed3\u675f\uff08\u6ce8\u610f\u6b64\u65f6\u8282\u70b9Y\u7684clusterReadHandler\u51fd\u6570link->node\u4e3aNULL\uff09\u3002 \uff086\uff09\u8282\u70b9X\u6536\u5230pong\u540e\uff0c\u53d1\u73b0\u548c\u8282\u70b9Y\u6b63\u5904\u5728\u63e1\u624b\u9636\u6bb5\uff0c\u66f4\u65b0\u8282\u70b9Y\u7684\u5730\u5740\u548cnodename\uff0c\u6e05\u9664 CLUSTER_NODE_HANDSHAKE \u72b6\u6001\u3002 \uff087\uff09\u8282\u70b9X\u5728 cron() \u51fd\u6570\u4e2d\u5c06\u7ed9\u672a\u5efa\u7acb\u8fde\u63a5\u7684\u8282\u70b9Y\u53d1\u9001ping \uff088\uff09\u8282\u70b9Y\u6536\u5230ping\u540e\u7ed9\u8282\u70b9X\u53d1\u9001pong \uff089\uff09\u8282\u70b9X\u5c06\u4fdd\u5b58\u7684\u8282\u70b9Y\u7684\u72b6\u6001CLUSTER_NODE_HANDSHAKE\u6e05\u9664\uff0c\u66f4\u65b0\u4e00\u4e0bnodename\u548c\u5730\u5740\uff0c\u81f3\u6b64\u63e1\u624b\u5b8c\u6210\uff0c\u4e24\u4e2a\u8282\u70b9\u90fd\u4fdd\u5b58\u76f8\u540c\u7684nodename\u548c\u4fe1\u606f\u3002 \u770b\u5b8c\u6574\u4e2a\u63e1\u624b\u8fc7\u7a0b\u540e\uff0c\u6211\u4eec\u5c1d\u8bd5\u601d\u8003\u4e24\u4e2a\u95ee\u9898\uff1a 1\u3001\u5982\u679c\u53d1\u9001meet\u5931\u8d25\u540e\uff0c\u8282\u70b9X\u7684\u72b6\u6001 CLUSTER_NODE_MEET \u72b6\u6001\u53c8\u88ab\u6e05\u9664\u4e86\uff0ccluster\u4f1a\u5982\u4f55\u5904\u7406\u5462\uff1f \u8fd9\u65f6\u5019\u8282\u70b9Y\u5728\u4e0b\u4e00\u4e2a clusterCron() \u51fd\u6570\u4e2d\u4f1a\u76f4\u63a5\u7ed9\u8282\u70b9Y\u53d1\u9001ping\uff0c\u4f46\u662f\u4e0d\u4f1a\u5c06\u8282\u70b9X\u5b58\u5165cluster->nodes\uff0c\u5bfc\u81f4\u8282\u70b9X\u8ba4\u4e3a\u5df2\u7ecf\u5efa\u7acb\u8fde\u63a5\uff0c\u7136\u800c\u8282\u70b9Y\u5e76\u6ca1\u6709\u627f\u8ba4\u3002\u5728\u540e\u9762\u8282\u70b9\u4f20\u64ad\u4e2d\uff0c\u5982\u679c\u6709\u5176\u4ed6\u8282\u70b9\u6301\u6709\u8282\u70b9X\u7684\u4fe1\u606f\u5e76\u7ed9\u8282\u70b9Y\u53d1\u9001ping\uff0c\u4e5f\u4f1a\u89e6\u53d1\u8282\u70b9Y\u4e3b\u52a8\u518d\u53bb\u7ed9\u8282\u70b9X\u53d1\u9001meet\u5efa\u7acb\u8fde\u63a5\u3002 2\u3001\u5982\u679c\u8282\u70b9Y\u5df2\u7ecf\u6709\u5b58\u50a8\u8282\u70b9X\uff0c\u4f46\u8fd8\u662f\u6536\u5230\u4e86\u8282\u70b9X\u7684meet\u8bf7\u6c42\uff0c\u5982\u4f55\u5904\u7406\uff1f nodename\u76f8\u540c\uff1a \uff081\uff09\u8282\u70b9Y\u53d1\u9001pong\u7ed9\u8282\u70b9X \uff082\uff09\u5982\u679c\u6b63\u5904\u4e8e\u63e1\u624b\u8282\u70b9\uff0c\u4f1a\u76f4\u63a5\u5220\u9664\u8282\u70b9\uff0c\u8fd9\u91cc\u4f1a\u5bfc\u81f4\u8282\u70b9Y\u4e22\u5931\u4e86\u8282\u70b9X\u7684\u6d88\u606f\u3002\u76f8\u5f53\u4e8e\u95ee\u98981\u3002 \uff083\uff09\u975e\u63e1\u624b\u9636\u6bb5\u5f80\u4e0b\u8d70\u6b63\u5e38\u7684ping\u6d41\u7a0b nodename\u4e0d\u540c\uff1a \uff081\uff09\u8282\u70b9Y\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a\u968f\u673anodename\u653e\u5165nodes\u4e2d\u5e76\u8bbe\u7f6e\u4e3a\u63e1\u624b\u9636\u6bb5\uff0c\u6b64\u65f6\u6709\u4e24\u4e2anodename\u5b58\u5728\u3002 \uff082\uff09\u8282\u70b9Y\u53d1\u9001pong\u7ed9\u8282\u70b9X \uff083\uff09\u8282\u70b9Y\u5982\u679c\u5df2\u7ecf\u521b\u5efa\u8fc7\u548c\u8282\u70b9X\u7684\u8fde\u63a5\uff0c\u8282\u70b9Y\u4f1a\u5728\u672c\u5730\u66f4\u65b0\u8282\u70b9X\u7684nodename\uff0c\u5220\u9664\u7b2c\u4e00\u4e2anodename\u5b58\u50a8\u7684node\uff0c\u66f4\u65b0\u63e1\u624b\u72b6\u6001\uff0c\u6b64\u65f6\u53ea\u5269\u4e0b\u7b2c\u4e8c\u4e2a\u6b63\u786e\u7684nodename\u3002 \uff084\uff09\u8282\u70b9Y\u5982\u679c\u6ca1\u521b\u5efa\u8fc7\u548c\u8282\u70b9X\u7684\u94fe\u63a5\uff0c\u4f1a\u5728clustercron(\uff09\u4e2d\u518d\u6b21\u7ed9\u8282\u70b9X\u53d1\u9001ping\u8bf7\u6c42\uff0c\u4e24\u4e2anodename\u4f1a\u5148\u540e\u5404\u53d1\u9001\u4e00\u6b21\u3002 \uff085\uff09\u7b2c\u4e00\u4e2anodename\u53d1\u9001ping\u540e\uff0c\u5728\u6536\u5230\u8282\u70b9X\u56de\u590d\u7684pong\u4e2d\uff0c\u66f4\u65b0\u8282\u70b9X\u7684nodename \uff086\uff09\u7b2c\u4e8c\u4e2anodename\u53d1\u9001ping\u540e\uff0c\u5728\u6536\u5230\u8282\u70b9X\u56de\u590d\u7684pong\u4e2d\uff0c\u53d1\u9001\u8282\u70b9X\u7684nodename\u5df2\u7ecf\u5b58\u5728\uff0c\u7b2c\u4e8c\u4e2anodename\u5904\u4e8e\u63e1\u624b\u72b6\u6001\uff0c\u8fd9\u65f6\u5019\u76f4\u63a5\u5220\u9664\u4e86\u7b2c\u4e8c\u4e2anodename\u3002 \u7ed3\u8bba\uff1a\u53ea\u6709nodename\u76f8\u540c\u5e76\u4e14\u4e24\u4e2a\u8282\u70b9\u90fd\u5728\u63e1\u624b\u9636\u6bb5\uff0c\u4f1a\u5bfc\u81f4\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u4e22\u6389\u53e6\u5916\u4e00\u4e2a\u8282\u70b9\u3002","title":"\u63e1\u624b\u8054\u7ed3"},{"location":"cluster/Gossip/redis-cluster-gossip-aliyun/#failover","text":"\u8be6\u60c5\u89c1\u6587\u7ae0\uff1a https://yq.aliyun.com/articles/638627?utm_content=m_1000016044","title":"\u5065\u5eb7\u68c0\u6d4b\u53cafailover"},{"location":"cluster/Gossip/redis-cluster-gossip-aliyun/#_4","text":"\u5047\u5982\u51fa\u73b0\u4e24\u4e2amaster\u7684\u65f6\u5019gossip\u534f\u8bae\u662f\u5982\u4f55\u5904\u7406\u51b2\u7a81\u7684\u5462\uff1f \u9996\u5148\u8981\u7406\u89e3\u4e24\u4e2a\u91cd\u8981\u7684\u53d8\u91cf\uff1a configEpoch\uff1a \u6bcf\u4e2a\u5206\u7247\u6709\u552f\u4e00\u7684epoch\u503c\uff0c\u4e3b\u5907epoch\u5e94\u8be5\u4e00\u81f4 currentEpoch\uff1a\u96c6\u7fa4\u5f53\u524d\u7684epoch\uff0c=\u96c6\u7fa4\u4e2d\u6700\u5927\u5206\u7247\u7684epoch \u5728ping\u5305\u4e2d\u4f1a\u81ea\u5e26sender\u8282\u70b9\u7684slots\u4fe1\u606f\u548ccurrentEpoch, configEpoch\u3002 master\u8282\u70b9\u6536\u5230\u6765\u81easlave\u8282\u70b9\u540e\u7684\u5904\u7406\u6d41\u7a0b\uff1a \uff081\uff09receiver\u6bd4\u8f83sender\u7684\u89d2\u8272\uff0c \u5982\u679csender\u8ba4\u4e3a\u81ea\u5df1\u662fmaster\uff0c\u4f46\u662f\u5728receiver\u88ab\u6807\u8bb0\u4e3aslave\uff0c\u5219receiver\u8282\u70b9\u5728\u96c6\u7fa4\u89c6\u56fe\u4e2d\u5c06sender\u6807\u8bb0\u4e3amaster\u3002 \u5982\u679csender\u8ba4\u4e3a\u81ea\u5df1\u662fslave\uff0c\u4f46\u662f\u5728receiver\u88ab\u6807\u8bb0\u4e3amaster, \u5219\u5728receiver\u7684\u96c6\u7fa4\u89c6\u56fe\u4e2d\u5c06sender\u6807\u8bb0\u4e3aslave, \u52a0\u5165\u5230sender\u6807\u8bb0\u7684master\u4e2d\uff0c\u5e76\u4e14\u5220\u9664sender\u5728reciver\u96c6\u7fa4\u89c6\u56fe\u4e2d\u7684slots\u4fe1\u606f\u3002 \uff082\uff09\u6bd4\u8f83sender\u81ea\u5e26\u7684slot\u4fe1\u606f\u548creceiver\u96c6\u7fa4\u89c6\u56fe\u4e2d\u7684slots\u662f\u5426\u51b2\u7a81\uff0c\u6709\u51b2\u7a81\u5219\u8fdb\u884c\u4e0b\u4e00\u6b65\u6bd4\u8f83 \uff083\uff09\u6bd4\u8f83sender\u7684configEpoch \u662f\u5426 > receiver\u96c6\u7fa4\u89c6\u56fe\u4e2d\u7684slots\u62e5\u6709\u8005\u7684configepoch\uff0c\u5982\u662f\u5728clusterUpdateSlotsConfigWith\u51fd\u6570\u4e2d\u91cd\u65b0\u8bbe\u7f6eslots\u62e5\u6709\u8005\u4e3asender\uff0c\u5e76\u4e14\u5c06\u65e7slots\u62e5\u6709\u8005\u8bbe\u7f6e\u4e3asender\u7684slave\uff0c\u518d\u6bd4\u8f83\u672c\u8282\u70b9\u662f\u6709\u810fslot, \u6709\u5219\u6e05\u9664\u6389\u3002 \uff084\uff09\u6bd4\u8f83sender\u81ea\u8eab\u7684slots\u4fe1\u606f < receiver\u96c6\u7fa4\u89c6\u56fe\u4e2d\u7684slots\u62e5\u6709\u8005\u7684configepoch\uff0c\u53d1\u9001update\u4fe1\u606f\uff0c\u901a\u77e5sender\u66f4\u65b0\uff0csender\u8282\u70b9\u4e5f\u4f1a\u6267\u884cclusterUpdateSlotsConfigWith\u51fd\u6570\u3002 \u5982\u679c\u4e24\u4e2a\u8282\u70b9\u7684configEpoch, currentEpoch\uff0c\u89d2\u8272\u90fd\u662fmaster\uff0c \u8fd9\u65f6\u5019\u5982\u4f55\u5904\u7406\u5462\uff1f receiver\u7684currentEpoch\u81ea\u589e\u5e76\u4e14\u8d4b\u503c\u7ed9configEpoch\uff0c\u4e5f\u5c31\u662f\u5f3a\u5236\u81ea\u589e\u6765\u89e3\u51b3\u51b2\u7a81\u3002\u8fd9\u65f6\u5019\u56e0\u4e3aconfigEpoch\u5927\uff0c\u53c8\u53ef\u4ee5\u8d70\u56de\u4e0a\u6587\u7684\u6d41\u7a0b\u3002 \u6240\u4ee5\u53ef\u80fd\u5b58\u5728\u53ccmaster\u540c\u65f6\u5b58\u5728\u7684\u60c5\u51b5\uff0c\u4f46\u662f\u6700\u7ec8\u4f1a\u6311\u9009\u51fa\u65b0\u7684master\u3002","title":"\u72b6\u6001\u66f4\u65b0\u53ca\u51b2\u7a81\u89e3\u51b3"},{"location":"cluster/Gossip/redis-cluster-gossip-aliyun/#_5","text":"\u4e91\u6570\u636e\u5e93Redis\u7248\uff08ApsaraDB for Redis\uff09\u662f\u4e00\u79cd\u7a33\u5b9a\u53ef\u9760\u3001\u6027\u80fd\u5353\u8d8a\u3001\u53ef\u5f39\u6027\u4f38\u7f29\u7684\u6570\u636e\u5e93\u670d\u52a1\u3002\u57fa\u4e8e\u98de\u5929\u5206\u5e03\u5f0f\u7cfb\u7edf\u548c\u5168SSD\u76d8\u9ad8\u6027\u80fd\u5b58\u50a8\uff0c\u652f\u6301\u4e3b\u5907\u7248\u548c\u96c6\u7fa4\u7248\u4e24\u5957\u9ad8\u53ef\u7528\u67b6\u6784\u3002\u63d0\u4f9b\u4e86\u5168\u5957\u7684\u5bb9\u707e\u5207\u6362\u3001\u6545\u969c\u8fc1\u79fb\u3001\u5728\u7ebf\u6269\u5bb9\u3001\u6027\u80fd\u4f18\u5316\u7684\u6570\u636e\u5e93\u89e3\u51b3\u65b9\u6848\u3002\u6b22\u8fce\u5404\u4f4d\u8d2d\u4e70\u4f7f\u7528: \u4e91\u6570\u636e\u5e93 Redis \u7248","title":"\u7ed3\u675f\u8bed"},{"location":"cluster/Read-the-code/","text":"\u5165\u53e3 \u4ece cluster_announce_port \u5165\u624b\uff1b\u53d1\u73b0\u4e86 cluster.c:clusterInit \u4e2d\u4f7f\u7528\u4e86\u5b83\uff1b cluster.c:clusterInit \u5728 server.c:initServer \u4e2d\u88ab\u8c03\u7528\u3002 \u5982\u4f55\u5f00\u542fredis cluster\uff1f \u5728 Redis cluster tutorial \u4e2d\u6709\u4ecb\u7ecd\uff1a Redis Cluster configuration parameters We are about to create an example cluster deployment. Before we continue, let's introduce the configuration parameters that Redis Cluster introduces in the redis.conf file. Some will be obvious, others will be more clear as you continue reading. cluster-enabled : If yes enables Redis Cluster support in a specific Redis instance. Otherwise the instance starts as a stand alone instance as usual. \u5728 config.c \u4e2d\u4e5f\u662f\u6839\u636e\u6b64\u914d\u7f6e\u9879\u6765\u521d\u59cb\u5316 struct redisServer.c:cluster_enabled \u6210\u5458\u53d8\u91cf\u7684\uff0c\u8be5\u6210\u5458\u53d8\u91cf\u8868\u793a\u662f\u5426\u542f\u52a8redis cluster\uff1b \u5728 server.c:initServer \u4e2d\u6709\u5982\u4e0bcode\uff1a if ( server . cluster_enabled ) clusterInit (); \u663e\u7136\uff0c\u53ea\u6709\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u5f00\u542f\u4e86cluster\u540e\uff0credis server\u624d\u4f1a\u5728\u542f\u52a8\u7684\u65f6\u5019\u6267\u884c clusterInit() \u3002 \u5173\u4e8e\u5982\u4f55\u6784\u5efaredis cluster\uff0c\u53c2\u89c1 Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528 \u3002 \u6570\u636e\u7ed3\u6784 server.c:struct clusterState \u96c6\u7fa4\u7684\u72b6\u6001 server.c:struct clusterState \u5c31\u662f replicated state machines \u3002 \u6210\u5458\u53d8\u91cf currentEpoch server.c:struct clusterNode \u96c6\u7fa4\u8282\u70b9 \u6210\u5458\u53d8\u91cf configEpoch \u5bf9\u4e8e\u6bcf\u4e2a\u96c6\u7fa4\u4e2d\u7684node\u800c\u8a00\uff0c\u5176\u914d\u7f6e\u662fcluster\u6700\u6700\u5173\u5fc3\u7684\uff0c\u6240\u4ee5\u7ed9\u4ed6\u53d6\u540d\u4e2d\u5e26\u6709 config \u3002 \u6210\u5458\u53d8\u91cf currentEpoch VS \u6210\u5458\u53d8\u91cf configEpoch \u5728 Redis Cluster Specification \u4e2d\u6709\u5982\u4e0b\u4ecb\u7ecd\uff1a The currentEpoch and configEpoch fields of the sending node that are used to mount the distributed algorithms used by Redis Cluster (this is explained in detail in the next sections). If the node is a slave the configEpoch is the last known configEpoch of its master. \u663e\u7136\u4e24\u8005\u7684\u76f8\u540c\u70b9\u5c31\u662f:mount the distributed algorithms used by Redis Cluster \u5728 Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb \u4e2d\u6709\u5bf9 currentEpoch \u548c configEpoch \u6709\u7740\u975e\u5e38\u8be6\u7ec6\u7684\u4ecb\u7ecd\uff0c\u5176\u5b9e\u4ece\u5b83\u4eec\u7684\u7528\u9014\u6765\u770b\uff0c\u5c31\u53ef\u4ee5\u660e\u767d\u4e3a\u4ec0\u4e48 currentEpoch \u7f6e\u4e8e server.c:struct clusterNode \u800c configEpoch \u7f6e\u4e8e server.c:struct clusterState \u4e2d\u3002","title":"Introduction"},{"location":"cluster/Read-the-code/#_1","text":"\u4ece cluster_announce_port \u5165\u624b\uff1b\u53d1\u73b0\u4e86 cluster.c:clusterInit \u4e2d\u4f7f\u7528\u4e86\u5b83\uff1b cluster.c:clusterInit \u5728 server.c:initServer \u4e2d\u88ab\u8c03\u7528\u3002","title":"\u5165\u53e3"},{"location":"cluster/Read-the-code/#redis#cluster","text":"\u5728 Redis cluster tutorial \u4e2d\u6709\u4ecb\u7ecd\uff1a","title":"\u5982\u4f55\u5f00\u542fredis cluster\uff1f"},{"location":"cluster/Read-the-code/#redis#cluster#configuration#parameters","text":"We are about to create an example cluster deployment. Before we continue, let's introduce the configuration parameters that Redis Cluster introduces in the redis.conf file. Some will be obvious, others will be more clear as you continue reading. cluster-enabled : If yes enables Redis Cluster support in a specific Redis instance. Otherwise the instance starts as a stand alone instance as usual. \u5728 config.c \u4e2d\u4e5f\u662f\u6839\u636e\u6b64\u914d\u7f6e\u9879\u6765\u521d\u59cb\u5316 struct redisServer.c:cluster_enabled \u6210\u5458\u53d8\u91cf\u7684\uff0c\u8be5\u6210\u5458\u53d8\u91cf\u8868\u793a\u662f\u5426\u542f\u52a8redis cluster\uff1b \u5728 server.c:initServer \u4e2d\u6709\u5982\u4e0bcode\uff1a if ( server . cluster_enabled ) clusterInit (); \u663e\u7136\uff0c\u53ea\u6709\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u5f00\u542f\u4e86cluster\u540e\uff0credis server\u624d\u4f1a\u5728\u542f\u52a8\u7684\u65f6\u5019\u6267\u884c clusterInit() \u3002 \u5173\u4e8e\u5982\u4f55\u6784\u5efaredis cluster\uff0c\u53c2\u89c1 Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528 \u3002","title":"Redis Cluster configuration parameters"},{"location":"cluster/Read-the-code/#_2","text":"","title":"\u6570\u636e\u7ed3\u6784"},{"location":"cluster/Read-the-code/#servercstruct#clusterstate","text":"server.c:struct clusterState \u5c31\u662f replicated state machines \u3002","title":"server.c:struct clusterState \u96c6\u7fa4\u7684\u72b6\u6001"},{"location":"cluster/Read-the-code/#currentepoch","text":"","title":"\u6210\u5458\u53d8\u91cfcurrentEpoch"},{"location":"cluster/Read-the-code/#servercstruct#clusternode","text":"","title":"server.c:struct clusterNode \u96c6\u7fa4\u8282\u70b9"},{"location":"cluster/Read-the-code/#configepoch","text":"\u5bf9\u4e8e\u6bcf\u4e2a\u96c6\u7fa4\u4e2d\u7684node\u800c\u8a00\uff0c\u5176\u914d\u7f6e\u662fcluster\u6700\u6700\u5173\u5fc3\u7684\uff0c\u6240\u4ee5\u7ed9\u4ed6\u53d6\u540d\u4e2d\u5e26\u6709 config \u3002","title":"\u6210\u5458\u53d8\u91cfconfigEpoch"},{"location":"cluster/Read-the-code/#currentepoch#vs#configepoch","text":"\u5728 Redis Cluster Specification \u4e2d\u6709\u5982\u4e0b\u4ecb\u7ecd\uff1a The currentEpoch and configEpoch fields of the sending node that are used to mount the distributed algorithms used by Redis Cluster (this is explained in detail in the next sections). If the node is a slave the configEpoch is the last known configEpoch of its master. \u663e\u7136\u4e24\u8005\u7684\u76f8\u540c\u70b9\u5c31\u662f:mount the distributed algorithms used by Redis Cluster \u5728 Redis\u6e90\u7801\u89e3\u6790\uff1a27\u96c6\u7fa4(\u4e09)\u4e3b\u4ece\u590d\u5236\u3001\u6545\u969c\u8f6c\u79fb \u4e2d\u6709\u5bf9 currentEpoch \u548c configEpoch \u6709\u7740\u975e\u5e38\u8be6\u7ec6\u7684\u4ecb\u7ecd\uff0c\u5176\u5b9e\u4ece\u5b83\u4eec\u7684\u7528\u9014\u6765\u770b\uff0c\u5c31\u53ef\u4ee5\u660e\u767d\u4e3a\u4ec0\u4e48 currentEpoch \u7f6e\u4e8e server.c:struct clusterNode \u800c configEpoch \u7f6e\u4e8e server.c:struct clusterState \u4e2d\u3002","title":"\u6210\u5458\u53d8\u91cfcurrentEpoch VS \u6210\u5458\u53d8\u91cfconfigEpoch"},{"location":"cluster/Sharding-strategy/","text":"Redis cluster data sharding strategy Redis cluster \u91c7\u7528\u7684\u662f \"hash slot\" \u7684data sharding strategy\uff0c\u8fd9\u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\u8fdb\u884c\u4e86\u8bf4\u660e: 1\u3001redis doc redis partition 2\u3001redis Redis Cluster Specification 3\u3001csdn Redis Cluster and Consistent Hashing \u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\u4e5f\u5bf9Redis\u6ca1\u6709\u91c7\u7528consistent hash\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 Redis hash slot VS consistent hash Redis\u91c7\u7528\u7684\u662fhash slot\uff0c\u800c\u4e0d\u662fconsistent hash\uff0c\u6211\u89c9\u5f97Redis\u7684hash slot\u65b9\u6848\u7684\u4f18\u52bf\u662f: \u5141\u8bb8\u7528\u6237\"keys-to-nodes map\"\u8fdb\u884c\u7075\u6d3b\u7684\u63a7\u5236(\u5176\u5b9e\u4ece\u53e6\u5916\u4e00\u4e2a\u89d2\u5ea6\u6765\u8bf4\uff0c\u5b83\u7684\u4f18\u52bf\u4e5f\u662f\u5b83\u7684\u52a3\u52bf) \u4e0b\u9762\u662f\u5bf9\u6b64\u7684\u8be6\u7ec6\u8bf4\u660e: \u4e00\u3001\u7528\u6237\u53ef\u4ee5\u624b\u52a8\u4ee5**hash slot**\u4e3a\u5355\u4f4d\u6765\u5728\u8282\u70b9\u4e4b\u95f4\u79fb\u52a8\u6570\u636e\uff0c\u4ece\u800c\u5b9e\u73b0 1\u3001 load balance \uff1b \u4e8c\u3001\u5728 \"redis doc redis partition # Data store or cache? \"\u4e2d\u4e5f\u8fdb\u884c\u4e86\u4ecb\u7ecd: If Redis is used as a store, a fixed keys-to-nodes map is used, so the number of nodes must be fixed and cannot vary . Otherwise, a system is needed that is able to rebalance keys between nodes when nodes are added or removed, and currently only Redis Cluster is able to do this - Redis Cluster is generally available and production-ready as of April 1 st , 2015 . consistent hash\u7684\u65b9\u6848\u5219\u5b8c\u5168\u662f\u81ea\u52a8\u7684\uff0c\u8fd0\u7ef4\u4eba\u5458\u65e0\u6cd5\u4ecb\u5165\u3001\u65e0\u6cd5\u5bf9\"keys-to-nodes map\"\u8fdb\u884c\u63a7\u5236\u3002 stackoverflow does redis cluster use consistent hashing I'm using redis cluster 3.0.1. I think redis cluster use consistent hashing. The hash slots are similar to virtual nodes in consistent hashing . Cassandra's data distribution is almost the same as redis cluster, and this article said it's consistent hashing. But the redis cluster turorial said redis cluster does not use consistent hash. NOTE: hash slot\u7684\u786e\u548cvirtual node\u7c7b\u4f3c\uff0c\u4f46\u662fRedis\u7684hash slot \u548c node\u662f\u53ef\u4ee5\u7531\u7528\u6237\u6765\u8fdb\u884c\u6307\u5b9a\u7684\uff0c\u800cconsistent hash\u5219\u662f\u65e0\u6cd5\u7684 A You are right, virtual nodes is quite simalar with hash slot. But virtual nodes is not an original concept of consistent hashing, but more like a trick used by Cassandra based on consistent hashing. So it's also ok for redis to say not using consistent hashing. NOTE: \u4ece\u8fd9\u6bb5\u8bdd\u53ef\u4ee5\u770b\u51fa\uff0cvirtual node\u662fCassandra\u4e2d\u4f7f\u7528\u7684\u4e00\u4e2atrick So, don't bother with phraseology(\u672f\u8bed). Redis doc Partitioning: how to split data among multiple Redis instances Partitioning is the process of splitting your data into multiple Redis instances, so that every instance will only contain a subset of your keys. The first part of this document will introduce you to the concept of partitioning, the second part will show you the alternatives for Redis partitioning. NOTE: partition\u5176\u5b9e\u5c31\u662fshard Why partitioning is useful NOTE: \u5176\u5b9e\u5c31\u662fdistributed computing\u7684\u4f18\u52bf Disadvantages of partitioning Partitioning basics NOTE: \u5982\u4f55\u5efa\u7acb\"key-node map\" Range partitioning One of the simplest ways to perform partitioning is with range partitioning , and is accomplished by mapping ranges of objects into specific Redis instances. For example, I could say users from ID 0 to ID 10000 will go into instance R0 , while users form ID 10001 to ID 20000 will go into instance R1 and so forth. Hash partitioning Different implementations of partitioning Partitioning can be the responsibility of different parts of a software stack. Client side partitioning Proxy assisted partitioning Query routing Redis Cluster implements an hybrid form of query routing, with the help of the client (the request is not directly forwarded from a Redis instance to another, but the client gets redirected to the right node). Data store or cache? NOTE: Redis\u7684\u5b9a\u4f4d\u662f\u5b83\u4e0d\u4ec5\u4ec5\u4f5c\u4e3a\u4e00\u4e2acache\uff0c\u5b83\u8fd8\u53ef\u4ee5\u4f5c\u4e3adata store\uff0c\u56e0\u6b64\u5b83\u7684\u5b9e\u73b0\u662f\u9700\u8981\u8003\u8651\u4e24\u8005\u7684\u9700\u6c42\u7684 Although partitioning in Redis is conceptually the same whether using Redis as a data store or as a cache , there is a significant limitation when using it as a data store. When Redis is used as a data store , a given key must always map to the same Redis instance. When Redis is used as a cache, if a given node is unavailable it is not a big problem if a different node is used, altering the key-instance map as we wish to improve the availability of the system (that is, the ability of the system to reply to our queries). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u5927\u610f\u662f: \u4ece\u7406\u8bba\u4e0a\u6765\u8bf4\uff0c\u65e0\u8bba\u662f\u5c06Redis\u4f5c\u4e3a data store \u6216 cache \uff0c\"partitioning\"\u7684\u672c\u8d28\u90fd\u662f\u76f8\u540c\u7684\uff0c\u4e0a\u6587\u7136\u540e\u4ee5\"\u5c06Redis\u4f5c\u4e3adata store\"\u3001\"\u5c06Redis\u4f5c\u4e3acache\"\u5206\u7c7b\u8ba8\u8bba: \u4e00\u3001\u5f53\u5c06Redis\u4f5c\u4e3adata store\u7684\u65f6\u5019\uff0c\u6709\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u9650\u5236: \"a given key must always map to the same Redis instance\"\u5373\"\u4e00\u4e2akey\uff0c\u5fc5\u987b\u603b\u662fmap\u5230\u76f8\u540c\u7684Redis instance\"\uff0c\u5373key-instance map\u662f\u4e0d\u80fd\u591f\u968f\u610f\u4fee\u6539\u7684\uff0c\u8fd9\u5c31\u662f\u540e\u9762\u6240\u8bf4\u7684\"a fixed keys-to-nodes map\" \u76f8\u53cd\uff0c\u5f53\u5c06Redis\u4f5c\u4e3acache\u7684\u65f6\u5019\uff0c\"if a given node is unavailable it is not a big problem if a different node is used, altering the key-instance map as we wish to improve the availability of the system (that is, the ability of the system to reply to our queries)\" \u62ec\u53f7\u4e2d\u7684\u5185\u5bb9\u662f\u5bf9 availability \u8fdb\u884c\u89e3\u91ca\u7684\uff1b \u4e8c\u3001\u5f53\u5c06Redis\u4f5c\u4e3acache\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u968f\u610f\u7684\u4fee\u6539key-instance map\uff0c\u53ea\u8981\u80fd\u591f\u4fdd\u8bc1 availability \u5373\u53ef\uff1b \u663e\u7136\uff0c\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\u53ef\u77e5: \u7531\u4e8eRedis\u65e2\u9700\u8981\u6ee1\u8db3data store\u7684\u9700\u6c42\u53c8\u9700\u8981\u6ee1\u8db3cache\u7684\u9700\u6c42\uff0c\u56e0\u6b64Redis cluster\u6ca1\u6709\u91c7\u7528consistent hash\u7684\u65b9\u6848\uff0c\u800c\u662f\u91c7\u7528\u7684hash slot\u7684\u65b9\u6848\uff0c\u8fd9\u79cd\u65b9\u6848\u5141\u8bb8\u7528\u6237\u5bf9\"keys-to-nodes map\"\u8fdb\u884c\u7075\u6d3b\u63a7\u5236 Consistent hashing implementations are often able to switch to other nodes if the preferred node for a given key is not available. Similarly if you add a new node, part of the new keys will start to be stored on the new node. The main concept here is the following: 1\u3001If Redis is used as a cache scaling up and down using consistent hashing is easy. 2\u3001If Redis is used as a store, a fixed keys-to-nodes map is used, so the number of nodes must be fixed and cannot vary . Otherwise, a system is needed that is able to rebalance keys between nodes when nodes are added or removed, and currently only Redis Cluster is able to do this - Redis Cluster is generally available and production-ready as of April 1 st , 2015 . Implementations of Redis partitioning So far we covered Redis partitioning in theory, but what about practice? What system should you use? Redis Cluster Redis Cluster is the preferred way to get automatic sharding and high availability. It is generally available and production-ready as of April 1 st , 2015 . You can get more information about Redis Cluster in the Cluster tutorial . Once Redis Cluster is available, and if a Redis Cluster compliant client is available for your language, Redis Cluster will be the de facto standard for Redis partitioning. Redis Cluster is a mix between query routing and client side partitioning . Twemproxy NOTE: \u5178\u578b\u7684client side partition Twemproxy is a proxy developed at Twitter for the Memcached ASCII and the Redis protocol. It is single threaded, it is written in C, and is extremely fast. It is open source software released under the terms of the Apache 2.0 license. Twemproxy supports automatic partitioning among multiple Redis instances, with optional node ejection(\u9a71\u79bb) if a node is not available (this will change the keys-instances map, so you should use this feature only if you are using Redis as a cache). It is not a single point of failure since you can start multiple proxies and instruct your clients to connect to the first that accepts the connection. NOTE: \u4e00\u3001\u6210\u7ec4\u6765\u907f\u514d**single point of failure** Basically Twemproxy is an intermediate layer between clients and Redis instances, that will reliably handle partitioning for us with minimal additional complexities. You can read more about Twemproxy in this antirez blog post .","title":"Introduction"},{"location":"cluster/Sharding-strategy/#redis#cluster#data#sharding#strategy","text":"Redis cluster \u91c7\u7528\u7684\u662f \"hash slot\" \u7684data sharding strategy\uff0c\u8fd9\u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\u8fdb\u884c\u4e86\u8bf4\u660e: 1\u3001redis doc redis partition 2\u3001redis Redis Cluster Specification 3\u3001csdn Redis Cluster and Consistent Hashing \u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\u4e5f\u5bf9Redis\u6ca1\u6709\u91c7\u7528consistent hash\u8fdb\u884c\u4e86\u8bf4\u660e\u3002","title":"Redis cluster data sharding strategy"},{"location":"cluster/Sharding-strategy/#redis#hash#slot#vs#consistent#hash","text":"Redis\u91c7\u7528\u7684\u662fhash slot\uff0c\u800c\u4e0d\u662fconsistent hash\uff0c\u6211\u89c9\u5f97Redis\u7684hash slot\u65b9\u6848\u7684\u4f18\u52bf\u662f: \u5141\u8bb8\u7528\u6237\"keys-to-nodes map\"\u8fdb\u884c\u7075\u6d3b\u7684\u63a7\u5236(\u5176\u5b9e\u4ece\u53e6\u5916\u4e00\u4e2a\u89d2\u5ea6\u6765\u8bf4\uff0c\u5b83\u7684\u4f18\u52bf\u4e5f\u662f\u5b83\u7684\u52a3\u52bf) \u4e0b\u9762\u662f\u5bf9\u6b64\u7684\u8be6\u7ec6\u8bf4\u660e: \u4e00\u3001\u7528\u6237\u53ef\u4ee5\u624b\u52a8\u4ee5**hash slot**\u4e3a\u5355\u4f4d\u6765\u5728\u8282\u70b9\u4e4b\u95f4\u79fb\u52a8\u6570\u636e\uff0c\u4ece\u800c\u5b9e\u73b0 1\u3001 load balance \uff1b \u4e8c\u3001\u5728 \"redis doc redis partition # Data store or cache? \"\u4e2d\u4e5f\u8fdb\u884c\u4e86\u4ecb\u7ecd: If Redis is used as a store, a fixed keys-to-nodes map is used, so the number of nodes must be fixed and cannot vary . Otherwise, a system is needed that is able to rebalance keys between nodes when nodes are added or removed, and currently only Redis Cluster is able to do this - Redis Cluster is generally available and production-ready as of April 1 st , 2015 . consistent hash\u7684\u65b9\u6848\u5219\u5b8c\u5168\u662f\u81ea\u52a8\u7684\uff0c\u8fd0\u7ef4\u4eba\u5458\u65e0\u6cd5\u4ecb\u5165\u3001\u65e0\u6cd5\u5bf9\"keys-to-nodes map\"\u8fdb\u884c\u63a7\u5236\u3002","title":"Redis hash slot VS consistent hash"},{"location":"cluster/Sharding-strategy/#stackoverflow#does#redis#cluster#use#consistent#hashing","text":"I'm using redis cluster 3.0.1. I think redis cluster use consistent hashing. The hash slots are similar to virtual nodes in consistent hashing . Cassandra's data distribution is almost the same as redis cluster, and this article said it's consistent hashing. But the redis cluster turorial said redis cluster does not use consistent hash. NOTE: hash slot\u7684\u786e\u548cvirtual node\u7c7b\u4f3c\uff0c\u4f46\u662fRedis\u7684hash slot \u548c node\u662f\u53ef\u4ee5\u7531\u7528\u6237\u6765\u8fdb\u884c\u6307\u5b9a\u7684\uff0c\u800cconsistent hash\u5219\u662f\u65e0\u6cd5\u7684","title":"stackoverflow does redis cluster use consistent hashing"},{"location":"cluster/Sharding-strategy/#a","text":"You are right, virtual nodes is quite simalar with hash slot. But virtual nodes is not an original concept of consistent hashing, but more like a trick used by Cassandra based on consistent hashing. So it's also ok for redis to say not using consistent hashing. NOTE: \u4ece\u8fd9\u6bb5\u8bdd\u53ef\u4ee5\u770b\u51fa\uff0cvirtual node\u662fCassandra\u4e2d\u4f7f\u7528\u7684\u4e00\u4e2atrick So, don't bother with phraseology(\u672f\u8bed).","title":"A"},{"location":"cluster/Sharding-strategy/#redis#doc#partitioning#how#to#split#data#among#multiple#redis#instances","text":"Partitioning is the process of splitting your data into multiple Redis instances, so that every instance will only contain a subset of your keys. The first part of this document will introduce you to the concept of partitioning, the second part will show you the alternatives for Redis partitioning. NOTE: partition\u5176\u5b9e\u5c31\u662fshard","title":"Redis doc Partitioning: how to split data among multiple Redis instances"},{"location":"cluster/Sharding-strategy/#why#partitioning#is#useful","text":"NOTE: \u5176\u5b9e\u5c31\u662fdistributed computing\u7684\u4f18\u52bf","title":"Why partitioning is useful"},{"location":"cluster/Sharding-strategy/#disadvantages#of#partitioning","text":"","title":"Disadvantages of partitioning"},{"location":"cluster/Sharding-strategy/#partitioning#basics","text":"NOTE: \u5982\u4f55\u5efa\u7acb\"key-node map\"","title":"Partitioning basics"},{"location":"cluster/Sharding-strategy/#range#partitioning","text":"One of the simplest ways to perform partitioning is with range partitioning , and is accomplished by mapping ranges of objects into specific Redis instances. For example, I could say users from ID 0 to ID 10000 will go into instance R0 , while users form ID 10001 to ID 20000 will go into instance R1 and so forth.","title":"Range partitioning"},{"location":"cluster/Sharding-strategy/#hash#partitioning","text":"","title":"Hash partitioning"},{"location":"cluster/Sharding-strategy/#different#implementations#of#partitioning","text":"Partitioning can be the responsibility of different parts of a software stack.","title":"Different implementations of partitioning"},{"location":"cluster/Sharding-strategy/#client#side#partitioning","text":"","title":"Client side partitioning"},{"location":"cluster/Sharding-strategy/#proxy#assisted#partitioning","text":"","title":"Proxy assisted partitioning"},{"location":"cluster/Sharding-strategy/#query#routing","text":"Redis Cluster implements an hybrid form of query routing, with the help of the client (the request is not directly forwarded from a Redis instance to another, but the client gets redirected to the right node).","title":"Query routing"},{"location":"cluster/Sharding-strategy/#data#store#or#cache","text":"NOTE: Redis\u7684\u5b9a\u4f4d\u662f\u5b83\u4e0d\u4ec5\u4ec5\u4f5c\u4e3a\u4e00\u4e2acache\uff0c\u5b83\u8fd8\u53ef\u4ee5\u4f5c\u4e3adata store\uff0c\u56e0\u6b64\u5b83\u7684\u5b9e\u73b0\u662f\u9700\u8981\u8003\u8651\u4e24\u8005\u7684\u9700\u6c42\u7684 Although partitioning in Redis is conceptually the same whether using Redis as a data store or as a cache , there is a significant limitation when using it as a data store. When Redis is used as a data store , a given key must always map to the same Redis instance. When Redis is used as a cache, if a given node is unavailable it is not a big problem if a different node is used, altering the key-instance map as we wish to improve the availability of the system (that is, the ability of the system to reply to our queries). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u5927\u610f\u662f: \u4ece\u7406\u8bba\u4e0a\u6765\u8bf4\uff0c\u65e0\u8bba\u662f\u5c06Redis\u4f5c\u4e3a data store \u6216 cache \uff0c\"partitioning\"\u7684\u672c\u8d28\u90fd\u662f\u76f8\u540c\u7684\uff0c\u4e0a\u6587\u7136\u540e\u4ee5\"\u5c06Redis\u4f5c\u4e3adata store\"\u3001\"\u5c06Redis\u4f5c\u4e3acache\"\u5206\u7c7b\u8ba8\u8bba: \u4e00\u3001\u5f53\u5c06Redis\u4f5c\u4e3adata store\u7684\u65f6\u5019\uff0c\u6709\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u9650\u5236: \"a given key must always map to the same Redis instance\"\u5373\"\u4e00\u4e2akey\uff0c\u5fc5\u987b\u603b\u662fmap\u5230\u76f8\u540c\u7684Redis instance\"\uff0c\u5373key-instance map\u662f\u4e0d\u80fd\u591f\u968f\u610f\u4fee\u6539\u7684\uff0c\u8fd9\u5c31\u662f\u540e\u9762\u6240\u8bf4\u7684\"a fixed keys-to-nodes map\" \u76f8\u53cd\uff0c\u5f53\u5c06Redis\u4f5c\u4e3acache\u7684\u65f6\u5019\uff0c\"if a given node is unavailable it is not a big problem if a different node is used, altering the key-instance map as we wish to improve the availability of the system (that is, the ability of the system to reply to our queries)\" \u62ec\u53f7\u4e2d\u7684\u5185\u5bb9\u662f\u5bf9 availability \u8fdb\u884c\u89e3\u91ca\u7684\uff1b \u4e8c\u3001\u5f53\u5c06Redis\u4f5c\u4e3acache\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u968f\u610f\u7684\u4fee\u6539key-instance map\uff0c\u53ea\u8981\u80fd\u591f\u4fdd\u8bc1 availability \u5373\u53ef\uff1b \u663e\u7136\uff0c\u901a\u8fc7\u4e0a\u8ff0\u5206\u6790\u53ef\u77e5: \u7531\u4e8eRedis\u65e2\u9700\u8981\u6ee1\u8db3data store\u7684\u9700\u6c42\u53c8\u9700\u8981\u6ee1\u8db3cache\u7684\u9700\u6c42\uff0c\u56e0\u6b64Redis cluster\u6ca1\u6709\u91c7\u7528consistent hash\u7684\u65b9\u6848\uff0c\u800c\u662f\u91c7\u7528\u7684hash slot\u7684\u65b9\u6848\uff0c\u8fd9\u79cd\u65b9\u6848\u5141\u8bb8\u7528\u6237\u5bf9\"keys-to-nodes map\"\u8fdb\u884c\u7075\u6d3b\u63a7\u5236 Consistent hashing implementations are often able to switch to other nodes if the preferred node for a given key is not available. Similarly if you add a new node, part of the new keys will start to be stored on the new node. The main concept here is the following: 1\u3001If Redis is used as a cache scaling up and down using consistent hashing is easy. 2\u3001If Redis is used as a store, a fixed keys-to-nodes map is used, so the number of nodes must be fixed and cannot vary . Otherwise, a system is needed that is able to rebalance keys between nodes when nodes are added or removed, and currently only Redis Cluster is able to do this - Redis Cluster is generally available and production-ready as of April 1 st , 2015 .","title":"Data store or cache?"},{"location":"cluster/Sharding-strategy/#implementations#of#redis#partitioning","text":"So far we covered Redis partitioning in theory, but what about practice? What system should you use?","title":"Implementations of Redis partitioning"},{"location":"cluster/Sharding-strategy/#redis#cluster","text":"Redis Cluster is the preferred way to get automatic sharding and high availability. It is generally available and production-ready as of April 1 st , 2015 . You can get more information about Redis Cluster in the Cluster tutorial . Once Redis Cluster is available, and if a Redis Cluster compliant client is available for your language, Redis Cluster will be the de facto standard for Redis partitioning. Redis Cluster is a mix between query routing and client side partitioning .","title":"Redis Cluster"},{"location":"cluster/Sharding-strategy/#twemproxy","text":"NOTE: \u5178\u578b\u7684client side partition Twemproxy is a proxy developed at Twitter for the Memcached ASCII and the Redis protocol. It is single threaded, it is written in C, and is extremely fast. It is open source software released under the terms of the Apache 2.0 license. Twemproxy supports automatic partitioning among multiple Redis instances, with optional node ejection(\u9a71\u79bb) if a node is not available (this will change the keys-instances map, so you should use this feature only if you are using Redis as a cache). It is not a single point of failure since you can start multiple proxies and instruct your clients to connect to the first that accepts the connection. NOTE: \u4e00\u3001\u6210\u7ec4\u6765\u907f\u514d**single point of failure** Basically Twemproxy is an intermediate layer between clients and Redis instances, that will reliably handle partitioning for us with minimal additional complexities. You can read more about Twemproxy in this antirez blog post .","title":"Twemproxy"},{"location":"cluster/ad-hoc-network/","text":"AdHoc\u7f51\u7edc baike AdHoc\u7f51\u7edc lifewire What Is an Ad Hoc Wireless Network? wikipedia Wireless ad hoc network","title":"AdHoc\u7f51\u7edc"},{"location":"cluster/ad-hoc-network/#adhoc","text":"","title":"AdHoc\u7f51\u7edc"},{"location":"cluster/ad-hoc-network/#baike#adhoc","text":"","title":"baike AdHoc\u7f51\u7edc"},{"location":"cluster/ad-hoc-network/#lifewire#what#is#an#ad#hoc#wireless#network","text":"","title":"lifewire What Is an Ad Hoc Wireless Network?"},{"location":"cluster/ad-hoc-network/#wikipedia#wireless#ad#hoc#network","text":"","title":"wikipedia Wireless ad hoc network"},{"location":"cluster/draft-cristian-Life-in-Redis-Cluster-Meet-and-Gossip-with-your-neighbors/","text":"cristian Life in a Redis Cluster: Meet and Gossip with your neighbors NOTE: \u8fd9\u7bc7\u6587\u7ae0\u63cf\u8ff0\u4e86Redis cluster\u7684\u8fd0\u4f5c\u539f\u7406 Redis Cluster is going to change your view of Redis. Since release 3.0.0 , Redis is able to operate in Cluster mode , providing automatic data sharding across multiple nodes with a high degree of fault tolerance . Well known techniques have been around for quite some time now to partition the data set and scale Redis to many instances , the most simple and effective one being client side partitioning implemented with consistent hashing . Redis Cluster goes a step forward in the direction of high scalability : it has been designed from the beginning with focus on high performance and with the aim to provide a set of consistency and availability guarantees that are suitable for different kinds of applications. There is an increasing amount of documentation around Redis Cluster , including official resources like the quick-start tutorial for end users and detailed tech specifications for developers. In this post we\u2019ll focus on the cluster networking internals, to give you an overview of the role and the importance of inter-node communications. Redis Cluster topology A set of isolated Redis instances, with proper configuration and guidance, can be joined to form a Redis Cluster. The minimal working cluster is composed of three master nodes, but Redis Cluster is designed and tested to scale efficiently up to 1000 nodes. In our examples, for the sake of simplicity, we\u2019ll consider only sets of masters without attached slaves. Nodes in a Redis Cluster talk to each other using the Cluster Bus . Every node is connected to every other node through a dedicated tcp connection, thus forming a fully connected network . This node-to-node communication channel is used exclusively for cluster operations (e.g. configuration update , failure detection , failover authorization ) and its links are kept separate from normal client connections. Nodes exchange data over the bus using a binary protocol, which is deliberately undocumented because it is not intended to be used by clients. Of course, code speaks and Redis source code is always worth studying. Cluster heartbeat: PING / PONG packets Under normal conditions, nodes of the cluster continuously exchange PING and PONG packets over the bus (remember we\u2019re talking about raw binary packets here, not to be confused with the PING client command). This packet flow constitutes the heartbeat mechanism of the cluster, a means of information propagation fundamental to some key features such as, for example, node auto-discovery and failure detection . In fact, each ping/pong packet carries important pieces of information about the state of the cluster from the point of view of the sender node, like hash slots distribution , configuration information and additional data about other trusted nodes. This animation 1 shows the heartbeat packet flow in a cluster of 5 master nodes under normal operation. While it is extremely slowed down, you can see that every PING packet triggers a PONG reply from the receiver node: In order to avoid exchanging too many packets on the network, usually a node will ping only a few (not all) randomly chosen nodes every second. However, the cluster-node-timeout configuration parameter controls (among other things) the volume of heartbeat packets exchanged between nodes: each node in the cluster will always try to ping every other node that didn\u2019t send a PING or received a PONG for longer than half this timeout value. SUMMARY : \u8fd9\u4e9b\u5e94\u8be5\u662f\u901a\u8fc7time event\u6765\u5b9e\u73b0\u7684\uff1b In other words, information about other nodes is refreshed after -at most- half cluster-node-timeout milliseconds, so this value significantly affects the hearthbeat traffic . Although packets are usually small in size, a lower timeout will cause a sensibly higher network traffic in a very large cluster. This setting is also involved in failure detection : if a node seems to be unreachable, before complete timeout expiration the sender will try to renew the underlying tcp connection and ping it again. SUMMARY : \u662f\u54ea\u4e2adata structure\u7528\u4e8e\u4fdd\u5b58other node\u7684information\u7684\uff1f cluster.h:struct clusterState \u7684 nodes \u6210\u5458\u53d8\u91cf\u3002 Cluster bootstrap: MEET packets Fundamental to the bootstrap process of a Redis Cluster and, in general, to the addition of new nodes, is the MEET packet. Its binary structure is similar to the PING / PONG packets, but a node receiving a MEET message will accept the sender as a new trusted node of the cluster. In fact, being part of a Redis Cluster is a matter of being trusted by your neighbors. While any node in a cluster will blindly reply to incoming PINGs on the cluster bus port, it won\u2019t process any other sensible packet nor will consider another Redis instance as part of the cluster until such untrusted source introduces itself with a MEET packet. To stimulate a new node to join an existing Redis Cluster we need to use the CLUSTER MEET command. The syntax CLUSTER MEET ip port will force Redis, when configured in cluster mode, to send a MEET packet to the specified node. Note that port here is the bus port, and any node of the cluster can be used as target, as we\u2019re going to explain. The creation of a Redis Cluster is usually handled with the redis-trib tool, a Ruby utility that simplifies cluster operations for sysadms. What this script actually does in order to bootstrap a cluster of N nodes, is a kind of brute force approach: the first Redis instance passed on the command line is selected as a trusted node, then a CLUSTER MEET command is sent to every other node to meet with this first one. This means that initially the trust relationship between cluster nodes is effectively forced by the sysadm. In this animation you can see the bootstrap phase of a Redis Cluster with 5 masters: Here node 9000 is the first trusted node, so every other node meets with it and receives a PONG reply. After some time that node 9000 knows about every other node in the cluster, you can see other nodes spontaneously getting in touch with each other through additional MEET messages, thus forming the fully connected network of an operational Redis Cluster. How did this information propagate across all nodes? Gossip: rumors worth listening to Redis Cluster uses a simple Gossip protocol in order to quickly spread information through any connected node. Heartbeat packets carry information on their own, but they also contain a special header for gossip data . This section contains information about a few random nodes among the set of nodes known to the sender. The number of entries included in the gossip header is proportional to the size of the cluster (usually 1/10 of the nodes, but may be empty as well in some cases). The trick is: almost any heartbeat packet contains data about some (a few) nodes of the cluster from the point of view of the sender. This is how information propagates quickly at large scale without requiring an exponential number of messages exchanged. Through gossip, all nodes eventually converge to a common shared view of the state of the cluster. This continuous information exchange is crucial in various point in life of the cluster like, for example, at bootstrap time and during node or network failures. In particular, when connecting a new node to an existing cluster, the redis-trib script just issues a single CLUSTER MEET command, then gossip makes the magic of auto-discover. This last animation shows in details what happens when a new node is connected to a cluster of 50 nodes: The video only shows MEET messages and PING / PONG packets that carry the information of the new node (so a lot of other heartbeat traffic is not represented). Nodes are initially in gray as they don\u2019t know about the newly connected Redis instance, nor it knows about them. What can happen here is either: the new node gets to know about a cluster node first (which is then highlighted in blue), or a cluster node is informed about the new node first (in this case is highlighted in red). This second event happens either by directly receiving a MEET or through a gossip info from a peer. In the example above, node 9050 meets with 9049 and receives gossip info by the PONG reply, then the information quickly flows and the nodes eventually form a fully connected graph again. It is worth noting that is much easier for the new node to discover existing nodes (i.e. case 1 is the majority), because every MEET/PONG exchange carries data about potentially unknown nodes. If you\u2019re wondering how fast it is, the 50 nodes above were configured with a cluster-node-timeout of 500 ms and the auto-discovery phase converged in about 400 ms. Animations in this post were created from real data using this packet-flight tool hacked by Carlos Bueno . You may want to have a look at his famous HTTP request visualization. \u21a9","title":"cristian [Life in a Redis Cluster: Meet and Gossip with your neighbors](https://cristian.regolo.cc/2015/09/05/life-in-a-redis-cluster.html)"},{"location":"cluster/draft-cristian-Life-in-Redis-Cluster-Meet-and-Gossip-with-your-neighbors/#cristian#life#in#a#redis#cluster#meet#and#gossip#with#your#neighbors","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u63cf\u8ff0\u4e86Redis cluster\u7684\u8fd0\u4f5c\u539f\u7406 Redis Cluster is going to change your view of Redis. Since release 3.0.0 , Redis is able to operate in Cluster mode , providing automatic data sharding across multiple nodes with a high degree of fault tolerance . Well known techniques have been around for quite some time now to partition the data set and scale Redis to many instances , the most simple and effective one being client side partitioning implemented with consistent hashing . Redis Cluster goes a step forward in the direction of high scalability : it has been designed from the beginning with focus on high performance and with the aim to provide a set of consistency and availability guarantees that are suitable for different kinds of applications. There is an increasing amount of documentation around Redis Cluster , including official resources like the quick-start tutorial for end users and detailed tech specifications for developers. In this post we\u2019ll focus on the cluster networking internals, to give you an overview of the role and the importance of inter-node communications.","title":"cristian Life in a Redis Cluster: Meet and Gossip with your neighbors"},{"location":"cluster/draft-cristian-Life-in-Redis-Cluster-Meet-and-Gossip-with-your-neighbors/#redis#cluster#topology","text":"A set of isolated Redis instances, with proper configuration and guidance, can be joined to form a Redis Cluster. The minimal working cluster is composed of three master nodes, but Redis Cluster is designed and tested to scale efficiently up to 1000 nodes. In our examples, for the sake of simplicity, we\u2019ll consider only sets of masters without attached slaves. Nodes in a Redis Cluster talk to each other using the Cluster Bus . Every node is connected to every other node through a dedicated tcp connection, thus forming a fully connected network . This node-to-node communication channel is used exclusively for cluster operations (e.g. configuration update , failure detection , failover authorization ) and its links are kept separate from normal client connections. Nodes exchange data over the bus using a binary protocol, which is deliberately undocumented because it is not intended to be used by clients. Of course, code speaks and Redis source code is always worth studying.","title":"Redis Cluster topology"},{"location":"cluster/draft-cristian-Life-in-Redis-Cluster-Meet-and-Gossip-with-your-neighbors/#cluster#heartbeat#ping#pong#packets","text":"Under normal conditions, nodes of the cluster continuously exchange PING and PONG packets over the bus (remember we\u2019re talking about raw binary packets here, not to be confused with the PING client command). This packet flow constitutes the heartbeat mechanism of the cluster, a means of information propagation fundamental to some key features such as, for example, node auto-discovery and failure detection . In fact, each ping/pong packet carries important pieces of information about the state of the cluster from the point of view of the sender node, like hash slots distribution , configuration information and additional data about other trusted nodes. This animation 1 shows the heartbeat packet flow in a cluster of 5 master nodes under normal operation. While it is extremely slowed down, you can see that every PING packet triggers a PONG reply from the receiver node: In order to avoid exchanging too many packets on the network, usually a node will ping only a few (not all) randomly chosen nodes every second. However, the cluster-node-timeout configuration parameter controls (among other things) the volume of heartbeat packets exchanged between nodes: each node in the cluster will always try to ping every other node that didn\u2019t send a PING or received a PONG for longer than half this timeout value. SUMMARY : \u8fd9\u4e9b\u5e94\u8be5\u662f\u901a\u8fc7time event\u6765\u5b9e\u73b0\u7684\uff1b In other words, information about other nodes is refreshed after -at most- half cluster-node-timeout milliseconds, so this value significantly affects the hearthbeat traffic . Although packets are usually small in size, a lower timeout will cause a sensibly higher network traffic in a very large cluster. This setting is also involved in failure detection : if a node seems to be unreachable, before complete timeout expiration the sender will try to renew the underlying tcp connection and ping it again. SUMMARY : \u662f\u54ea\u4e2adata structure\u7528\u4e8e\u4fdd\u5b58other node\u7684information\u7684\uff1f cluster.h:struct clusterState \u7684 nodes \u6210\u5458\u53d8\u91cf\u3002","title":"Cluster heartbeat: PING / PONG packets"},{"location":"cluster/draft-cristian-Life-in-Redis-Cluster-Meet-and-Gossip-with-your-neighbors/#cluster#bootstrap#meet#packets","text":"Fundamental to the bootstrap process of a Redis Cluster and, in general, to the addition of new nodes, is the MEET packet. Its binary structure is similar to the PING / PONG packets, but a node receiving a MEET message will accept the sender as a new trusted node of the cluster. In fact, being part of a Redis Cluster is a matter of being trusted by your neighbors. While any node in a cluster will blindly reply to incoming PINGs on the cluster bus port, it won\u2019t process any other sensible packet nor will consider another Redis instance as part of the cluster until such untrusted source introduces itself with a MEET packet. To stimulate a new node to join an existing Redis Cluster we need to use the CLUSTER MEET command. The syntax CLUSTER MEET ip port will force Redis, when configured in cluster mode, to send a MEET packet to the specified node. Note that port here is the bus port, and any node of the cluster can be used as target, as we\u2019re going to explain. The creation of a Redis Cluster is usually handled with the redis-trib tool, a Ruby utility that simplifies cluster operations for sysadms. What this script actually does in order to bootstrap a cluster of N nodes, is a kind of brute force approach: the first Redis instance passed on the command line is selected as a trusted node, then a CLUSTER MEET command is sent to every other node to meet with this first one. This means that initially the trust relationship between cluster nodes is effectively forced by the sysadm. In this animation you can see the bootstrap phase of a Redis Cluster with 5 masters: Here node 9000 is the first trusted node, so every other node meets with it and receives a PONG reply. After some time that node 9000 knows about every other node in the cluster, you can see other nodes spontaneously getting in touch with each other through additional MEET messages, thus forming the fully connected network of an operational Redis Cluster. How did this information propagate across all nodes?","title":"Cluster bootstrap: MEET packets"},{"location":"cluster/draft-cristian-Life-in-Redis-Cluster-Meet-and-Gossip-with-your-neighbors/#gossip#rumors#worth#listening#to","text":"Redis Cluster uses a simple Gossip protocol in order to quickly spread information through any connected node. Heartbeat packets carry information on their own, but they also contain a special header for gossip data . This section contains information about a few random nodes among the set of nodes known to the sender. The number of entries included in the gossip header is proportional to the size of the cluster (usually 1/10 of the nodes, but may be empty as well in some cases). The trick is: almost any heartbeat packet contains data about some (a few) nodes of the cluster from the point of view of the sender. This is how information propagates quickly at large scale without requiring an exponential number of messages exchanged. Through gossip, all nodes eventually converge to a common shared view of the state of the cluster. This continuous information exchange is crucial in various point in life of the cluster like, for example, at bootstrap time and during node or network failures. In particular, when connecting a new node to an existing cluster, the redis-trib script just issues a single CLUSTER MEET command, then gossip makes the magic of auto-discover. This last animation shows in details what happens when a new node is connected to a cluster of 50 nodes: The video only shows MEET messages and PING / PONG packets that carry the information of the new node (so a lot of other heartbeat traffic is not represented). Nodes are initially in gray as they don\u2019t know about the newly connected Redis instance, nor it knows about them. What can happen here is either: the new node gets to know about a cluster node first (which is then highlighted in blue), or a cluster node is informed about the new node first (in this case is highlighted in red). This second event happens either by directly receiving a MEET or through a gossip info from a peer. In the example above, node 9050 meets with 9049 and receives gossip info by the PONG reply, then the information quickly flows and the nodes eventually form a fully connected graph again. It is worth noting that is much easier for the new node to discover existing nodes (i.e. case 1 is the majority), because every MEET/PONG exchange carries data about potentially unknown nodes. If you\u2019re wondering how fast it is, the 50 nodes above were configured with a cluster-node-timeout of 500 ms and the auto-discovery phase converged in about 400 ms. Animations in this post were created from real data using this packet-flight tool hacked by Carlos Bueno . You may want to have a look at his famous HTTP request visualization. \u21a9","title":"Gossip: rumors worth listening to"},{"location":"cluster/draft-segmentfault-Redis-Cluster-availability%E5%88%86%E6%9E%90/","text":"segmentfault Redis Cluster availability \u5206\u6790","title":"segmentfault [Redis Cluster availability \u5206\u6790](https://segmentfault.com/a/1190000039234661)"},{"location":"cluster/draft-segmentfault-Redis-Cluster-availability%E5%88%86%E6%9E%90/#segmentfault#redis#cluster#availability","text":"","title":"segmentfault Redis Cluster availability \u5206\u6790"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/","text":"redis Redis Cluster Specification Welcome to the Redis Cluster Specification . Here you'll find information about algorithms and design rationales of Redis Cluster . This document is a work in progress as it is continuously synchronized with the actual implementation of Redis. Main properties and rationales of the design Redis Cluster goals Redis Cluster is a distributed implementation of Redis with the following goals, in order of importance in the design: 1\u3001High performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values. NOTE: \u4e00\u3001Redis\u90fd\u662f\u4f7f\u7528\u7684**asynchronous replication** \u4e8c\u3001\u5173\u4e8e \"no merge operations are performed on values\"\uff0c\u53c2\u89c1 \"Why merge operations are avoided\" \u7ae0\u8282 2\u3001Acceptable degree of write safety : the system tries (in a best-effort way) to retain all the writes originating from clients connected with the majority of the master nodes . Usually there are small windows where acknowledged writes can be lost. Windows to lose acknowledged writes are larger when clients are in a minority partition . NOTE: \u53ef\u63a5\u53d7\u7684\u5199\u5b89\u5168\u6027:\u7cfb\u7edf\u5c1d\u8bd5(\u4ee5\u6700\u4f73\u65b9\u5f0f)\u4fdd\u7559\u6765\u81ea\u8fde\u63a5\u5230\u5927\u591a\u6570\u4e3b\u8282\u70b9\u7684\u5ba2\u6237\u673a\u7684\u6240\u6709\u5199\u3002\u901a\u5e38\u6709\u4e00\u4e9b\u5c0f\u7a97\u53e3\uff0c\u5728\u90a3\u91cc\u53ef\u4ee5\u4e22\u5931\u5df2\u786e\u8ba4\u7684\u5199\u64cd\u4f5c\u3002\u5f53\u5ba2\u6237\u7aef\u4f4d\u4e8e\u5c11\u6570\u5206\u533a\u65f6\uff0c\u4e22\u5931\u5df2\u786e\u8ba4\u5199\u64cd\u4f5c\u7684\u7a97\u53e3\u4f1a\u66f4\u5927\u3002 3\u3001Availability: Redis Cluster is able to survive partitions where the majority of the master nodes are reachable and there is at least one reachable slave for every master node that is no longer reachable. Moreover using replicas migration , masters no longer replicated by any slave will receive one from a master which is covered by multiple slaves. NOTE: \u4e00\u3001\u53ef\u7528\u6027:Redis\u96c6\u7fa4\u80fd\u591f\u5728\u5927\u591a\u6570\u4e3b\u8282\u70b9\u90fd\u53ef\u8bbf\u95ee\u7684\u5206\u533a\u4e2d\u5b58\u6d3b\uff0c\u5e76\u4e14\u5bf9\u4e8e\u6bcf\u4e2a\u4e0d\u518d\u53ef\u8bbf\u95ee\u7684\u4e3b\u8282\u70b9\uff0c\u81f3\u5c11\u6709\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u4ece\u5c5e\u8282\u70b9\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u526f\u672c\u8fc1\u79fb\uff0c\u4e0d\u518d\u7531\u4efb\u4f55\u5974\u96b6\u590d\u5236\u7684\u4e3b\u4eba\u5c06\u4ece\u4e00\u4e2a\u7531\u591a\u4e2a\u5974\u96b6\u8986\u76d6\u7684\u4e3b\u4eba\u90a3\u91cc\u63a5\u6536\u4e00\u4e2a\u3002 \u4e8c\u3001\"replicas migration\"\u6307\u7684\u662fRedis cluster\u5c06\u4e00\u4e2a\u62e5\u6709\u591a\u4e2aslave\u7684master\u7684\u5176\u4e2d\u4e00\u4e2aslave\u6307\u5b9a\u4e3a\u53e6\u5916\u4e00\u4e2a\u6ca1\u6709slave\u7684master\u7684slave What is described in this document is implemented in Redis 3.0 or greater. Implemented subset Redis Cluster implements all the single key commands available in the non-distributed version of Redis. Commands performing complex multi-key operations like Set type unions or intersections are implemented as well as long as the keys all belong to the same node . Redis Cluster implements a concept called hash tags that can be used in order to force certain keys to be stored in the same node. However during manual reshardings, multi-key operations may become unavailable for some time while single key operations are always available. Redis Cluster does not support multiple databases like the stand alone version of Redis. There is just database 0 and the SELECT command is not allowed. NOTE: why? Clients and Servers roles in the Redis Cluster protocol In Redis Cluster nodes are responsible for holding the data, and taking the state of the cluster, including mapping keys to the right nodes. Cluster nodes are also able to auto-discover other nodes, detect non-working nodes, and promote slave nodes to master when needed in order to continue to operate when a failure occurs. NOTE: \u5b9e\u73b0\u4e0a\uff0c cluster.h:struct clusterNode \u7528\u4e8e\u63cf\u8ff0cluster node\uff0c cluster.h:struct clusterState \u7528\u4e8e\u63cf\u8ff0the state of the cluster\uff1b\u6b63\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63cf\u8ff0\u7684\uff0c\u6bcf\u4e2acluster node\u90fd\u9700\u8981\u4f7f\u7528 cluster.h:struct clusterState \u6765\u8bb0\u5f55the state of the cluster\u3002 To perform their tasks all the cluster nodes are connected using a TCP bus and a binary protocol , called the Redis Cluster Bus . Every node is connected to every other node in the cluster using the cluster bus . Nodes use a gossip protocol 1\u3001to propagate information about the cluster in order to discover new nodes, 2\u3001to send ping packets to make sure all the other nodes are working properly, and 3\u3001to send cluster messages needed to signal specific conditions. The cluster bus is also used in order to propagate Pub/Sub messages across the cluster and to orchestrate manual failovers when requested by users ( manual failovers are failovers which are not initiated by the Redis Cluster failure detector, but by the system administrator directly). NOTE: \u4e00\u3001cluster node\u4e4b\u95f4\u7684\u8fde\u63a5\u65b9\u5f0f\u662f\u600e\u6837\u7684\uff1f \"Every node is connected to every other node in the cluster using the cluster bus \" \u8282\u70b9\u4e4b\u95f4\u662f\u76f8\u4e92\u8fde\u63a5\u7684 \u4e0a\u9762\u8bf4node use a gossip protocol\uff0c\u90a3\u80fd\u5426\u636e\u6b64\u63a8\u65ad\u51fa\u662f\u4e24\u4e24\u4e92\u8054\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\u53c2\u89c1\u4e0b\u9762\u7684topology\u3002 \u4e8c\u3001\u603b\u7684\u6765\u8bf4\uff0ccluster node\u4e4b\u95f4\u7684\u901a\u4fe1: 1\u3001 Redis Cluster Bus 2\u3001 gossip protocol Since cluster nodes are not able to proxy requests, clients may be redirected to other nodes using redirection errors -MOVED and -ASK . The client is in theory free to send requests to all the nodes in the cluster, getting redirected if needed, so the client is not required to hold the state of the cluster. However clients that are able to cache the map between keys and nodes can improve the performance in a sensible way. Write safety Redis Cluster uses asynchronous replication between nodes, and last failover wins implicit merge function. This means that the last elected master dataset eventually replaces all the other replicas. There is always a window of time when it is possible to lose writes during partitions. However these windows are very different in the case of a client that is connected to the majority of masters, and a client that is connected to the minority of masters. NOTE: \u4e00\u3001Redis Cluster\u4f7f\u7528\u8282\u70b9\u4e4b\u95f4\u7684\u5f02\u6b65\u590d\u5236\uff0c\u4e0a\u6b21\u6545\u969c\u8f6c\u79fb\u8d62\u5f97\u9690\u5f0f\u5408\u5e76\u529f\u80fd\u3002 \u8fd9\u610f\u5473\u7740\u6700\u540e\u9009\u51fa\u7684\u4e3b\u6570\u636e\u96c6\u6700\u7ec8\u5c06\u66ff\u6362\u6240\u6709\u5176\u4ed6\u526f\u672c\u3002 \u5728\u5206\u533a\u671f\u95f4\u53ef\u80fd\u4f1a\u4e22\u5931\u5199\u5165\u65f6\uff0c\u603b\u4f1a\u6709\u4e00\u4e2a\u65f6\u95f4\u7a97\u53e3\u3002 \u7136\u800c\uff0c\u5728\u8fde\u63a5\u5230\u5927\u591a\u6570\u4e3b\u8bbe\u5907\u7684\u5ba2\u6237\u7aef\u548c\u8fde\u63a5\u5230\u5c11\u6570\u4e3b\u8bbe\u5907\u7684\u5ba2\u6237\u7aef\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u7a97\u53e3\u662f\u975e\u5e38\u4e0d\u540c\u7684\u3002 \u4e8c\u3001\" last failover wins implicit merge function\"\u662f\u4ec0\u4e48\u542b\u4e49\u5462\uff1f \"the last elected master dataset eventually replaces all the other replicas\" \u5373 \u6240\u6709\u7684\u6570\u636e\u4ee5\u6700\u7ec8\"the last elected master\"\u7684\u6570\u636e\u4e3a\u51c6 Redis Cluster tries harder to retain\uff08\u4fdd\u6301\uff09 writes that are performed by clients connected to the majority of masters , compared to writes performed in the minority side . NOTE: \u9700\u8981\u6ce8\u610f\u539f\u6587\u7684\u5199\u4f5c\u601d\u8def: \u539f\u6587\u662f\u6839\u636e majority partition \u548c minority partition \u6765\u8fdb\u884c\u5206\u7c7b\u8ba8\u8bba\u7684 In the majority partitions The following are examples of scenarios that lead to loss of acknowledged writes received in the majority partitions during failures: NOTE: \"acknowledged writes\" \u6307\u7684\u662f master\u63a5\u6536\u5230\u4e86write\u8bf7\u6c42 1\u3001A write may reach a master, but while the master may be able to reply to the client, the write may not be propagated to slaves via the asynchronous replication used between master and slave nodes . If the master dies without the write reaching the slaves, the write is lost forever if the master is unreachable for a long enough period that one of its slaves is promoted. This is usually hard to observe in the case of a total, sudden failure of a master node since masters try to reply to clients (with the acknowledge of the write) and slaves (propagating the write) at about the same time. However it is a real world failure mode . NOTE: \u4e00\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5176\u5b9e\u63cf\u8ff0\u5730\u5e76\u4e0d\u7b80\u4ecb\uff0c\u8ba9\u4eba\u96be\u4ee5\u6293\u4f4f\u91cd\u8981\u610f\u601d\uff0c\u5176\u5b9e\u573a\u666f\u975e\u5e38\u7b80\u5355: master\u63a5\u6536\u5230\u4e86write\uff0c\u5e76\"reply to the client\"\uff0c\u4f46\u662f \"the write may not be propagated to slaves via the asynchronous replication used between master and slave nodes \"\uff1b\u5982\u679c\u6b64\u65f6 \"the master dies without the write reaching the slaves\"\uff0c\u663e\u7136\u4f1a\u8fdb\u5165failover\u6d41\u7a0b\uff0c\u90a3\u4e48\u5c31\u5b58\u5728\u8fd9\u6837\u7684\u53ef\u80fd\u6027: \u4e00\u4e2a\u6ca1\u6709\u540c\u6b65\u521a\u521a\u7684write\u7684slave\uff0c\u88ab\u9009\u4e3e\u4e3a\u65b0\u7684master\uff0c\u90a3\u4e48 \"the write is lost forever if the master is unreachable for a long enough period that one of its slaves is promoted\" \u5728 csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f \u4e2d\uff0c\u5bf9\u8fd9\u4e2a\u573a\u666f\u6709\u5206\u6790 \u4e8c\u3001\u601d\u8003: \u5982\u4f55\u4fdd\u8bc1\u4e0d\u4e22\u5931\u5462\uff1fsynchronous replication 2\u3001Another theoretically possible failure mode where writes are lost is the following: A master is unreachable because of a partition. It gets failed over by one of its slaves. After some time it may be reachable again. A client with an out-of-date routing table may write to the old master before it is converted into a slave (of the new master) by the cluster. The second failure mode is unlikely to happen because master nodes unable to communicate with the majority of the other masters for enough time to be failed over will no longer accept writes, and when the partition is fixed writes are still refused for a small amount of time to allow other nodes to inform about configuration changes. This failure mode also requires that the client's routing table has not yet been updated. NOTE: \u4e00\u3001\" master nodes unable to communicate with the majority of the other masters for enough time to be failed over will no longer accept writes\" \u4e3b\u8bed\u662f master nodes \u5b9a\u8bed\u662f \"unable to communicate with the majority of the other masters for enough time to be failed over\" \u4e8c\u3001\u5728 \"csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f # \u7f51\u7edc\u5206\u533a\" \u6bb5\u4e2d\uff0c\u7ed9\u51fa\u4e86\u6bd4\u8f83\u597d\u7684\u56fe\u793a \u4e09\u3001\u5728partition\u6062\u590d\u540e\uff0c\"writes are still refused for a small amount of time to allow other nodes to inform about configuration changes\" \u662f\u8c01\u7ee7\u7eed\"refuse write\"\uff1f In the minority side of a partition Writes targeting the minority side of a partition have a larger window in which to get lost. For example, Redis Cluster loses a non-trivial number of writes on partitions where there is a minority of masters and at least one or more clients, since all the writes sent to the masters may potentially get lost if the masters are failed over in the majority side. Specifically, for a master to be failed over it must be unreachable by the majority of masters for at least NODE_TIMEOUT , so if the partition is fixed before that time, no writes are lost. When the partition lasts for more than NODE_TIMEOUT , all the writes performed in the minority side up to that point may be lost. However the minority side of a Redis Cluster will start refusing writes as soon as NODE_TIMEOUT time has elapsed without contact with the majority, so there is a maximum window after which the minority becomes no longer available. Hence, no writes are accepted or lost after that time. NOTE: \u611f\u89c9\u8fd9\u79cd\u60c5\u51b5\u548c\u524d\u9762\u7684\"2\"\u662f\u7c7b\u4f3c\u7684 Availability NOTE: \u4e00\u3001\"Redis Cluster is not available in the minority side of the partition\" \u9700\u8981\u5bf9\u5b83\u8fdb\u884c\u89e3\u91ca\uff0c\u7ed3\u5408\u524d\u9762\u5bf9\u5185\u5bb9\u3001\"csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f # \u7f51\u7edc\u5206\u533a\" \u4e2d\u7684\u5185\u5bb9\uff0c\u53ef\u77e5: \u5728 NODE_TIMEOUT \u540e\uff0c \"the minority side of the partition\"\u5c31\u4e0d\u53ef\u7528\u4e86 Redis Cluster is not available in the minority side of the partition. In the majority side of the partition assuming that there are at least the majority of masters and a slave for every unreachable master, the cluster becomes available again after NODE_TIMEOUT time plus a few more seconds required for a slave to get elected and failover its master (failovers are usually executed in a matter of 1 or 2 seconds). NOTE: \u4e00\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f: majority side of the partition\u80fd\u591f\u6062\u590d\u662f\u6709\u524d\u63d0\u6761\u4ef6\u7684: 1\u3001the majority of masters 2\u3001slave for every unreachable master This means that Redis Cluster is designed to survive failures of a few nodes in the cluster, but it is not a suitable solution for applications that require availability in the event of large net splits. In the example of a cluster composed of N master nodes where every node has a single slave, the majority side of the cluster will remain available as long as a single node is partitioned away, and will remain available with a probability of 1-(1/(N*2-1)) when two nodes are partitioned away (after the first node fails we are left with N*2-1 nodes in total, and the probability of the only master without a replica to fail is 1/(N*2-1)) . For example, in a cluster with 5 nodes and a single slave per node, there is a 1/(5*2-1) = 11.11% probability that after two nodes are partitioned away from the majority, the cluster will no longer be available. NOTE: \u5728 segmentfault Redis Cluster availability \u5206\u6790 \u4e2d\uff0c\u5bf9\u4e0a\u8ff0\u8ba1\u7b97\u8fc7\u7a0b\u8fdb\u884c\u4e86\u8bf4\u660e: \u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5305\u542b N \u4e2a master \u7684\u96c6\u7fa4\uff0c\u6bcf\u4e2a master \u6709\u552f\u4e00 slave\u3002 \u5355\u4e2a node \u51fa\u73b0\u6545\u969c\uff0ccluster \u4ecd\u7136\u53ef\u7528\uff0c\u7b2c\u4e8c\u4e2a node \u518d\u51fa\u73b0\u6545\u969c\uff0c\u96c6\u7fa4\u4ecd\u7136\u53ef\u7528\u7684\u6982\u7387\u662f 1-(1/(N*2-1) \u3002 \u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff0c\u7b2c\u4e00\u4e2a node fail \u540e\uff0c\u96c6\u7fa4\u5269\u4e0b N*2-1 \u4e2a\u5065\u5eb7\u8282\u70b9\uff0c\u6b64\u65f6 orphan master \u6070\u597d fail \u7684\u6982\u7387\u662f 1/(N*2-1) \u3002 NOTE: orphan master \u6307\u7684\u662f\u6ca1\u6709slave\u7684master\uff0c\u4e0b\u9762\u8fdb\u884c\u4e86\u8bf4\u660e \u5957\u7528\u516c\u5f0f\uff0c\u5047\u8bbe N = 5\uff0c\u90a3\u4e48\uff0c2 \u4e2a\u8282\u70b9\u4ece majority partition \u51fa\u53bb\uff0c\u96c6\u7fa4\u4e0d\u53ef\u7528\u7684\u6982\u7387\u662f 11.11%\u3002 Thanks to a Redis Cluster feature called replicas migration the Cluster availability is improved in many real world scenarios by the fact that replicas migrate to orphaned masters (masters no longer having replicas). So at every successful failure event, the cluster may reconfigure the slaves layout in order to better resist the next failure. NOTE: \u5728 segmentfault Redis Cluster availability \u5206\u6790 \u4e2d\uff0c\u5bf9 replicas migration \u7684\u8fc7\u7a0b\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e Performance In Redis Cluster nodes don't proxy commands to the right node in charge for a given key, but instead they redirect clients to the right nodes serving a given portion of the key space. Eventually clients obtain an up-to-date representation of the cluster and which node serves which subset of keys, so during normal operations clients directly contact the right nodes in order to send a given command. Because of the use of asynchronous replication, nodes do not wait for other nodes' acknowledgment of writes (if not explicitly requested using the WAIT command). Also, because multi-key commands are only limited to near keys, data is never moved between nodes except when resharding. Normal operations are handled exactly as in the case of a single Redis instance. This means that in a Redis Cluster with N master nodes you can expect the same performance as a single Redis instance multiplied by N as the design scales linearly. At the same time the query is usually performed in a single round trip, since clients usually retain persistent connections with the nodes, so latency figures are also the same as the single standalone Redis node case. Very high performance and scalability while preserving weak but reasonable forms of data safety and availability is the main goal of Redis Cluster. Why merge operations are avoided NOTE: \u4e00\u3001\u5728 \"Write safety\"\u4e2d\u4ecb\u7ecd\u4e86\uff0cRedis\u91c7\u7528\u7684\u662f\" last failover wins implicit merge function\" \u4e8c\u3001\u8fd9\u4e00\u6bb5\u5219\u662f\u4ecb\u7ecd: \u4e3a\u4ec0\u4e48Redis\u8981\u8fd9\u6837\u505a\uff0c\u901a\u8fc7\u539f\u6587\u7684\u5185\u5bb9\u53ef\u77e5: \u56e0\u4e3aRedis\u7684data type\u662f\u6bd4\u8f83\u590d\u6742\u7684\u3001Redis\u7684\u6570\u636e\u91cf\u662f\u975e\u5e38\u5927\u7684\uff0c\u5982\u679c\u91c7\u7528merge\u7684\u65b9\u5f0f\uff0c\u90a3\u4e48\u590d\u6742\u5ea6\u5c06\u662f\u975e\u5e38\u9ad8\u7684\uff0c\u5c06\u9020\u6210 \"bottleneck\" Redis Cluster design avoids conflicting versions of the same key-value pair in multiple nodes as in the case of the Redis data model this is not always desirable. Values in Redis are often very large; it is common to see lists or sorted sets with millions of elements. Also data types are semantically(\u8bed\u4e49) complex. Transferring and merging these kind of values can be a major bottleneck and/or may require the non-trivial involvement of application-side logic, additional memory to store meta-data, and so forth. There are no strict technological limits here. CRDTs or synchronously replicated state machines can model complex data types similar to Redis. However, the actual run time behavior of such systems would not be similar to Redis Cluster. Redis Cluster was designed in order to cover the exact use cases of the non-clustered Redis version. Overview of Redis Cluster main components Keys distribution model The key space is split into 16384 slots , effectively setting an upper limit for the cluster size of 16384 master nodes (however the suggested max size of nodes is in the order of ~ 1000 nodes). NOTE: Each master node in a cluster handles a subset of the 16384 hash slots. cluster\u4e2d\u7684\u6240\u6709node\u5747\u5206\u8fd916384\u4e2aslot\uff0c\u6bcf\u4e2anode\u81f3\u5c11\u9700\u8981\u5206\u5f97\u4e00\u4e2aslot\uff0c\u6240\u4ee5cluster size\u7684\u4e0a\u9650\u5c31\u662f16374. struct clusterNode \u7684 slots \u6210\u5458\u53d8\u91cf\u4fdd\u5b58\u8fd9\u4e2anode\u7684\u6240\u6709\u7684slot\u3002 Each master node in a cluster handles a subset of the 16384 hash slots. The cluster is stable when there is no cluster reconfiguration in progress (i.e. where hash slots are being moved from one node to another). When the cluster is stable, a single hash slot will be served by a single node (however the serving node can have one or more slaves that will replace it in the case of net splits or failures, and that can be used in order to scale read operations where reading stale(\u65e7) data is acceptable). The base algorithm used to map keys to hash slots is the following (read the next paragraph for the hash tag exception to this rule): HASH_SLOT = CRC16 ( key ) mod 16384 The CRC16 is specified as follows: Name: XMODEM (also known as ZMODEM or CRC-16/ACORN) Width: 16 bit Poly: 1021 (That is actually x^{16} + x^{12} + x^{5} + 1 x^{16} + x^{12} + x^{5} + 1 ) Initialization: 0000 Reflect Input byte: False Reflect Output CRC: False Xor constant to output CRC: 0000 Output for \"123456789\": 31C3 14 out of 16 CRC16 output bits are used (this is why there is a modulo 16384 operation in the formula above). In our tests CRC16 behaved remarkably well in distributing different kinds of keys evenly across the 16384 slots. Note : A reference implementation of the CRC16 algorithm used is available in the Appendix A of this document. NOTE: \u5728\u4ee3\u7801\u4e2d\u662f\u7531 cluser.c:keyHashSlot \u51fd\u6570\u5b9e\u73b0\u7684key space\u5230hash slot\u7684\u6620\u5c04\u7684\uff1b Keys hash tags There is an exception for the computation of the hash slot that is used in order to implement hash tags . Hash tags are a way to ensure that multiple keys are allocated in the same hash slot. This is used in order to implement multi-key operations in Redis Cluster. In order to implement hash tags, the hash slot for a key is computed in a slightly different way in certain conditions. If the key contains a \"{...}\" pattern only the substring between { and } is hashed in order to obtain the hash slot. However since it is possible that there are multiple occurrences of { or } the algorithm is well specified by the following rules: IF the key contains a { character. AND IF there is a } character to the right of { AND IF there are one or more characters between the first occurrence of { and the first occurrence of } . Then instead of hashing the key, only what is between the first occurrence of { and the following first occurrence of } is hashed. Examples: The two keys {user1000}.following and {user1000}.followers will hash to the same hash slot since only the substring user1000 will be hashed in order to compute the hash slot. For the key foo{}{bar} the whole key will be hashed as usually since the first occurrence of { is followed by } on the right without characters in the middle. For the key foo{{bar}}zap the substring {bar will be hashed, because it is the substring between the first occurrence of { and the first occurrence of } on its right. For the key foo{bar}{zap} the substring bar will be hashed, since the algorithm stops at the first valid or invalid (without bytes inside) match of { and } . What follows from the algorithm is that if the key starts with {} , it is guaranteed to be hashed as a whole. This is useful when using binary data as key names. Adding the hash tags exception, the following is an implementation of the HASH_SLOT function in Ruby and C language. Ruby example code: def HASH_SLOT ( key ) s = key . index \"{\" if s e = key . index \"}\" , s + 1 if e && e != s + 1 key = key [ s + 1. . e - 1 ] end end crc16 ( key ) % 16384 end C example code: unsigned int HASH_SLOT ( char * key , int keylen ) { int s , e ; /* start-end indexes of { and } */ /* Search the first occurrence of '{'. */ for ( s = 0 ; s < keylen ; s ++ ) if ( key [ s ] == '{' ) break ; /* No '{' ? Hash the whole key. This is the base case. */ if ( s == keylen ) return crc16 ( key , keylen ) & 16383 ; /* '{' found? Check if we have the corresponding '}'. */ for ( e = s + 1 ; e < keylen ; e ++ ) if ( key [ e ] == '}' ) break ; /* No '}' or nothing between {} ? Hash the whole key. */ if ( e == keylen || e == s + 1 ) return crc16 ( key , keylen ) & 16383 ; /* If we are here there is both a { and a } on its right. Hash * what is in the middle between { and }. */ return crc16 ( key + s + 1 , e - s -1 ) & 16383 ; } Cluster nodes attributes node ID / node name NOTE: \u9700\u8981\u6ce8\u610f\uff0cnode ID\u5c31\u662fnode name Every node has a unique name in the cluster. The node name is the hex representation of a 160 bit random number, obtained the first time a node is started (usually using /dev/urandom ). The node will save its ID in the node configuration file , and will use the same ID forever, or at least as long as the node configuration file is not deleted by the system administrator, or a hard reset is requested via the CLUSTER RESET command. NOTE: \u4e00\u3001\u663e\u7136\uff0cnode name\u9700\u8981\u4fdd\u8bc1\u4e0d\u80fd\u591f\u51b2\u7a81\uff1b cluster.c:struct clusterNode \u7684 name \u6210\u5458\u53d8\u91cf\u5c31\u8868\u793a\u5b83\u7684node name\u3002 \u4e8c\u3001\u5982\u4f55\u4fdd\u8bc1\u751f\u6210\u7684node name\u4e0d\u51b2\u7a81\u5462\uff1f The node ID is used to identify every node across the whole cluster. It is possible for a given node to change its IP address without any need to also change the node ID . The cluster is also able to detect the change in IP/port and reconfigure using the gossip protocol running over the cluster bus. NOTE: reconfigure cluster The node ID is not the only information associated with each node, but is the only one that is always globally consistent \uff08\u4e0d\u4f1a\u518d\u6539\u53d8\uff09. Every node has also the following set of information associated. Some information is about the cluster configuration detail of this specific node, and is eventually consistent across the cluster. Some other information, like the last time a node was pinged, is instead local to each node. Every node maintains the following information about other nodes that it is aware of in the cluster: The node ID, IP and port of the node, a set of flags, what is the master of the node if it is flagged as slave , last time the node was pinged and the last time the pong was received, the current configuration epoch of the node (explained later in this specification), the link state and finally the set of hash slots served. NOTE: \u4e00\u3001cluster\u4e2d\u7684\u6bcf\u4e2anode\u8fd8\u9700\u8981\u4fdd\u5b58\u5176\u4ed6node\u7684\u4e00\u4e9b\u914d\u7f6e\u4fe1\u606f\uff1b\u90a3\u662f\u54ea\u4e9bnode\u5462\uff1f\u662fcluster\u4e2d\u7684\u6240\u6709\u5176\u4ed6\u7684node\u8fd8\u662f\u4ec5\u4ec5\u90a3\u4e9b\u548c\u5b83connect\u7684node\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u9605\u8bfb\u4e0b\u9762\u7684Cluster topology\u7ae0\u8282 A detailed explanation of all the node fields is described in the CLUSTER NODES documentation. The CLUSTER NODES command can be sent to any node in the cluster and provides the state of the cluster and the information for each node according to the local view the queried node has of the cluster. The following is sample output of the CLUSTER NODES command sent to a master node in a small cluster of three nodes. The Cluster bus Every Redis Cluster node has an additional TCP port for receiving incoming connections from other Redis Cluster nodes. This port is at a fixed offset from the normal TCP port used to receive incoming connections from clients. To obtain the Redis Cluster port, 10000 should be added to the normal commands port. For example, if a Redis node is listening for client connections on port 6379, the Cluster bus port 16379 will also be opened. Node-to-node communication happens exclusively using the Cluster bus and the Cluster bus protocol : a binary protocol composed of frames of different types and sizes. The Cluster bus binary protocol is not publicly documented since it is not intended for external software devices to talk with Redis Cluster nodes using this protocol. However you can obtain more details about the Cluster bus protocol by reading the cluster.h and cluster.c files in the Redis Cluster source code. Cluster topology Redis Cluster is a full mesh\uff08\u7f51\u683c\uff09 where every node is connected with every other node using a TCP connection. NOTE: \u5173\u4e8efull mesh\uff0c\u53c2\u89c1https:// www.webopedia.com/TERM/M/mesh.html In a cluster of N nodes, every node has N-1 outgoing\uff08\u4f20\u51fa\uff09 TCP connections, and N-1 incoming\uff08\u4f20\u5165\uff09 connections. NOTE: \u6bcf\u4e2anode\u9700\u8981\u8fde\u63a5\u5230cluster\u4e2d\u7684\u5269\u4f59N-1\u4e2anode\uff0c\u540c\u65f6cluster\u4e2d\u7684\u53e6\u5916\u7684N-1\u4e2anode\u4e5f\u8981\u8fde\u63a5\u5230\u5b83\uff1b slave\u662f\u5426\u4e5f\u7b97\u662fcluster\u4e2d\u7684node\uff1fslave\u662f\u5426\u4e5f\u9700\u8981\u548ccluster\u4e2d\u7684\u53e6\u5916\u7684node interconnect\u5462\uff1f These TCP connections are kept alive all the time and are not created on demand. When a node expects a pong reply in response to a ping in the cluster bus, before waiting long enough to mark the node as unreachable, it will try to refresh the connection with the node by reconnecting from scratch. While Redis Cluster nodes form a full mesh, nodes use a gossip protocol and a configuration update mechanism in order to avoid exchanging too many messages between nodes during normal conditions , so the number of messages exchanged is not exponential. NOTE: \u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f\u5728\u540e\u9762\u7684\"Fault Tolerance # Heartbeat and gossip messages\"\u7ae0\u8282\u4e2d\uff0c \u8fdb\u884c\u4e86\u8bf4\u660e: Usually a node will ping a few random nodes every second so that the total number of ping packets sent (and pong packets received) by each node is a constant amount regardless of the number of nodes in the cluster. Nodes handshake NOTE: \u8fd9\u4e00\u6bb5\u5176\u5b9e\u544a\u8bc9\u6211\u4eec\u5982\u4f55\u7ec4\u5efaRedis cluster Nodes always accept connections on the cluster bus port, and even reply to pings when received, even if the pinging node is not trusted. However, all other packets will be discarded by the receiving node if the sending node is not considered part of the cluster. A node will accept another node as part of the cluster only in two ways: 1\u3001If a node presents itself with a MEET message. A meet message is exactly like a PING message, but forces the receiver to accept the node as part of the cluster. Nodes will send MEET messages to other nodes only if the system administrator requests this via the following command: CLUSTER MEET ip port NOTE: \u4e00\u3001 CLUSTER MEET ip port \u4e8c\u3001\u663e\u7136\uff0c\u901a\u8fc7\"CLUSTER MEET ip port\" command\u6765\u7ec4\u5efaRedis cluster \u4e09\u3001\u8fd9\u79cd\u65b9\u5f0f\u662fadministrator\u6765\u7ec4\u5efaRedis cluster 2\u3001A node will also register another node as part of the cluster if a node that is already trusted will gossip about this other node. So if A knows B, and B knows C, eventually B will send gossip messages to A about C. When this happens, A will register C as part of the network, and will try to connect with C. NOTE: \u4e00\u3001\u8fd9\u79cd\u65b9\u5f0f\u662fRedis node\u901a\u8fc7gossip\u534f\u8bae\u6765auto discover\u5176\u4ed6node\uff0c\u7136\u540e\u4e0e\u5b83\u5efa\u7acb\u8fde\u63a5 This means that as long as we join nodes in any connected graph, they'll eventually form a fully connected graph automatically. This means that the cluster is able to auto-discover other nodes, but only if there is a trusted relationship that was forced by the system administrator. NOTE: \u4e00\u3001\u5fc5\u987b\u8981\u7531administrator\u9996\u5148\u5efa\u7acb\u57fa\u672c\u7684\u8fde\u63a5\uff0c\u7136\u540e\u624d\u80fd\u591f\u5b9e\u73b0\u5168\u8fde\u63a5\u56fe This mechanism makes the cluster more robust but prevents different Redis clusters from accidentally mixing after change of IP addresses or other network related events. Redirection and resharding MOVED Redirection A Redis client is free to send queries to every node in the cluster, including slave nodes . The node will analyze the query, and if it is acceptable (that is, only a single key is mentioned in the query, or the multiple keys mentioned are all to the same hash slot ) it will lookup what node is responsible for the hash slot where the key or keys belong. If the hash slot is served by the node, the query is simply processed, otherwise the node will check its internal hash slot to node map \uff08hash slot\u5230node\u7684\u6620\u5c04\u5173\u7cfb\uff09, and will reply to the client with a MOVED error, like in the following example: GET x -MOVED 3999 127.0.0.1:6381 The error includes the hash slot of the key (3999) and the ip:port of the instance that can serve the query. The client needs to reissue the query to the specified node's IP address and port. Note that even if the client waits a long time before reissuing the query, and in the meantime the cluster configuration changed, the destination node will reply again with a MOVED error if the hash slot 3999 is now served by another node. The same happens if the contacted node had no updated information. So while from the point of view of the cluster nodes are identified by IDs we try to simplify our interface with the client just exposing a map between hash slots and Redis nodes identified by IP:port pairs. The client is not required to, but should try to memorize that hash slot 3999 is served by 127.0.0.1:6381. This way once a new command needs to be issued it can compute the hash slot of the target key and have a greater chance of choosing the right node. An alternative is to just refresh the whole client-side cluster layout using the CLUSTER NODES or CLUSTER SLOTS commands when a MOVED redirection is received. When a redirection is encountered, it is likely multiple slots were reconfigured rather than just one, so updating the client configuration as soon as possible is often the best strategy. Note that when the Cluster is stable (no ongoing changes in the configuration), eventually all the clients will obtain a map of hash slots -> nodes, making the cluster efficient, with clients directly addressing the right nodes without redirections, proxies or other single point of failure entities. A client must be also able to handle -ASK redirections that are described later in this document, otherwise it is not a complete Redis Cluster client. Cluster live reconfiguration NOTE: \u4e00\u3001\u8fd9\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u5c31\u662fresharding \u4e8c\u3001Redis\u4f7f\u7528\u7684\u662fhash tag\uff0cresharding\u7684\u8fc7\u7a0b\u5176\u5b9e\u5c31\u662f\u5bf9hash slot\u7684\u79fb\u52a8 Redis Cluster supports the ability to add and remove nodes while the cluster is running. Adding or removing a node is abstracted into the same operation: moving a hash slot from one node to another. This means that the same basic mechanism can be used in order to rebalance the cluster, add or remove nodes, and so forth. 1\u3001To add a new node to the cluster an empty node is added to the cluster and some set of hash slots are moved from existing nodes to the new node. 2\u3001To remove a node from the cluster the hash slots assigned to that node are moved to other existing nodes. 3\u3001To rebalance the cluster a given set of hash slots are moved between nodes. NOTE: \u4e00\u3001\u5bf9Redis cluster\u8fdb\u884cCRUD: 1\u3001\u9009\u62e9\u8282\u70b9 2\u3001\u5220\u9664\u8282\u70b9 3\u3001\u4fee\u6539\u8282\u70b9 4\u3001\u5bf9\u8282\u70b9\u8fdb\u884c\u67e5\u8be2 The core of the implementation is the ability to move hash slots around. From a practical point of view a hash slot is just a set of keys, so what Redis Cluster really does during resharding is to move keys from an instance to another instance. Moving a hash slot means moving all the keys that happen to hash into this hash slot. To understand how this works we need to show the CLUSTER subcommands that are used to manipulate the slots translation table in a Redis Cluster node . NOTE: slots translation table\u5e94\u8be5\u662f\u7531 cluster.h:struct clusterState \u7684 migrating_slots_to \u3001 importing_slots_from \u3001 slots \u6210\u5458\u53d8\u91cf\u5b9e\u73b0\u7684\uff1b The following subcommands are available (among others not useful in this case): 1\u3001 CLUSTER ADDSLOTS slot1 [slot2] ... [slotN] 2\u3001 CLUSTER DELSLOTS slot1 [slot2] ... [slotN] 3\u3001 CLUSTER SETSLOT slot NODE node 4\u3001 CLUSTER SETSLOT slot MIGRATING node 5\u3001 CLUSTER SETSLOT slot IMPORTING node The first two commands, ADDSLOTS and DELSLOTS , are simply used to assign (or remove) slots to a Redis node. Assigning a slot means to tell a given master node that it will be in charge of storing and serving content for the specified hash slot. After the hash slots are assigned they will propagate across the cluster using the gossip protocol, as specified later in the configuration propagation section. NOTE: \u4e00\u3001\u662f \"Hash slots configuration propagation\" \u6bb5 ADDSLOTS The ADDSLOTS command is usually used when a new cluster is created from scratch to assign each master node a subset of all the 16384 hash slots available. DELSLOTS The DELSLOTS is mainly used for manual modification of a cluster configuration or for debugging tasks: in practice it is rarely used. SETSLOT The SETSLOT subcommand is used to assign a slot to a specific node ID if the SETSLOT <slot> NODE form is used. Otherwise the slot can be set in the two special states MIGRATING and IMPORTING . Those two special states are used in order to migrate a hash slot from one node to another. 1\u3001When a slot is set as MIGRATING , the node will accept all queries that are about this hash slot , but only if the key in question exists, otherwise the query is forwarded using a -ASK redirection to the node that is target of the migration. 2\u3001When a slot is set as IMPORTING , the node will accept all queries that are about this hash slot , but only if the request is preceded by an ASKING command. If the ASKING command was not given by the client, the query is redirected to the real hash slot owner via a -MOVED redirection error, as would happen normally. Hash slot migration Let's make this clearer with an example of hash slot migration . Assume that we have two Redis master nodes, called A and B. We want to move hash slot 8 from A to B, so we issue commands like this: We send B: CLUSTER SETSLOT 8 IMPORTING A We send A: CLUSTER SETSLOT 8 MIGRATING B All the other nodes will continue to point clients to node \"A\" every time they are queried with a key that belongs to hash slot 8, so what happens is that: All queries about existing keys are processed by \"A\". All queries about non-existing keys in A are processed by \"B\", because \"A\" will redirect clients to \"B\". This way we no longer create new keys in \"A\". In the meantime, a special program called redis-trib used during reshardings and Redis Cluster configuration will migrate existing keys in hash slot 8 from A to B. This is performed using the following command: CLUSTER GETKEYSINSLOT slot count The above command will return count keys in the specified hash slot . For every key returned, redis-trib sends node \"A\" a MIGRATE command, that will migrate the specified key from A to B in an atomic way (both instances are locked for the time (usually very small time) needed to migrate a key so there are no race conditions). This is how MIGRATE works: MIGRATE target_host target_port key target_database id timeout MIGRATE will connect to the target instance , send a serialized version of the key, and once an OK code is received, the old key from its own dataset will be deleted. From the point of view of an external client a key exists either in A or B at any given time. In Redis Cluster there is no need to specify a database other than 0, but MIGRATE is a general command that can be used for other tasks not involving Redis Cluster. MIGRATE is optimized to be as fast as possible even when moving complex keys such as long lists, but in Redis Cluster reconfiguring the cluster where big keys are present is not considered a wise procedure if there are latency constraints in the application using the database. When the migration process is finally finished, the SETSLOT <slot> NODE <node-id> command is sent to the two nodes involved in the migration in order to set the slots to their normal state again. The same command is usually sent to all other nodes to avoid waiting for the natural propagation of the new configuration across the cluster. NOTE: \u79fb\u52a8key\uff0c\u662f\u5426\u9700\u8981\u79fb\u52a8\u8be5key\u5bf9\u5e94\u7684\u6570\u636e\uff1f\u9700\u8981\u7684\uff0c\u8981\u60f3\u5b8c\u6574\u5730\u7406\u89e3\u8fd9\u4e00\u8282\u7684\u5185\u5bb9\uff0c\u9700\u8981\u9605\u8bfb\uff1a Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528 ASK redirection Clients first connection and handling of redirections Multiple keys operations Scaling reads using slave nodes NOTE: \u8fd9\u548c\u8bfb\u5199\u5206\u79bb\u6709\u5173\uff0c\u53c2\u89c1: zhihu redis\u9700\u8981\u8bfb\u5199\u5206\u79bb\u5417\uff1f cnblogs Redis\u8bfb\u5199\u5206\u79bb\u6280\u672f\u89e3\u6790 Normally slave nodes will redirect clients to the authoritative master for the hash slot involved in a given command, however clients can use slaves in order to scale reads using the READONLY command. READONLY tells a Redis Cluster slave node that the client is ok reading possibly stale data and is not interested in running write queries. When the connection is in readonly mode , the cluster will send a redirection to the client only if the operation involves keys not served by the slave's master node. This may happen because: 1\u3001The client sent a command about hash slots never served by the master of this slave. 2\u3001The cluster was reconfigured (for example resharded) and the slave is no longer able to serve commands for a given hash slot. When this happens the client should update its hashslot map as explained in the previous sections. The readonly state of the connection can be cleared using the READWRITE command. Fault Tolerance Heartbeat and gossip messages Redis Cluster nodes continuously exchange ping and pong packets. Those two kind of packets have the same structure, and both carry important configuration information. The only actual difference is the message type field . We'll refer to the sum of ping and pong packets as heartbeat packets . Usually nodes send ping packets that will trigger the receivers to reply with pong packets . However this is not necessarily true. It is possible for nodes to just send pong packets to send information to other nodes about their configuration, without triggering a reply. This is useful, for example, in order to broadcast a new configuration as soon as possible. Usually a node will ping a few random nodes every second so that the total number of ping packets sent (and pong packets received) by each node is a constant amount regardless of the number of nodes in the cluster. NOTE: \u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5728 \"Cluster topology\" \u7ae0\u8282\u4e2d\uff0c\u5c31\u8c08\u5230\u4e86\u8fd9\u4e2a\u95ee\u9898 However every node makes sure to ping every other node that hasn't sent a ping or received a pong for longer than half the NODE_TIMEOUT time. Before NODE_TIMEOUT has elapsed, nodes also try to reconnect the TCP link with another node to make sure nodes are not believed to be unreachable only because there is a problem in the current TCP connection. NOTE: \u4e00\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6bcf\u4e2anode\u4e0d\u662f\u6bcf\u79d2\u949f\u90fd\u53bbping cluster\u4e2d\u5269\u4f59\u7684\u5176\u4ed6\u7684\u6240\u6709\u7684node\uff0c\u800c\u662f\u4fdd\u8bc1\u80fd\u591f\u5728half of the NODE_TIMEOUT \u4e2d\u80fd\u591fping\u5230cluster\u4e2d\u5269\u4f59\u7684\u5176\u4ed6\u7684\u6240\u6709\u7684node\uff0c\u6240\u4ee5\u5b83\u9700\u8981\u505a\u7684\u662f\u6bcf\u79d2\u949f\u53eaping\u4e00\u90e8\u5206\u3002 \u4e8c\u3001\u4e3a\u4ec0\u4e48\u662fhalf the NODE_TIMEOUT time\uff1f\u5728\u4e0b\u9762\u7684Failure detection\u7ae0\u8282\u4e2d\u7ed9\u51fa\u4e86\u8fd9\u6837\u505a\u7684\u539f\u56e0\u3002 The number of messages globally exchanged can be sizable if NODE_TIMEOUT is set to a small figure and the number of nodes (N) is very large, since every node will try to ping every other node for which they don't have fresh information every half the NODE_TIMEOUT time. For example in a 100 node cluster with a node timeout set to 60 seconds, every node will try to send 99 pings every 30 seconds, with a total amount of pings of 3.3 per second. Multiplied by 100 nodes, this is 330 pings per second in the total cluster. There are ways to lower the number of messages, however there have been no reported issues with the bandwidth currently used by Redis Cluster failure detection , so for now the obvious and direct design is used. Note that even in the above example, the 330 packets per second exchanged are evenly divided among 100 different nodes, so the traffic each node receives is acceptable. Heartbeat packet content Ping and pong packets contain a header that is common to all types of packets (for instance packets to request a failover vote ), and a special Gossip Section that is specific of Ping and Pong packets. The common header has the following information: 1\u3001Node ID, a 160 bit pseudorandom string that is assigned the first time a node is created and remains the same for all the life of a Redis Cluster node. 2\u3001The currentEpoch and configEpoch fields of the sending node that are used to mount the distributed algorithms used by Redis Cluster (this is explained in detail in the next sections). If the node is a slave the configEpoch is the last known configEpoch of its master. 3\u3001The node flags , indicating if the node is a slave, a master, and other single-bit node information. 4\u3001A bitmap of the hash slots served by the sending node, or if the node is a slave, a bitmap of the slots served by its master. 5\u3001The sender TCP base port (that is, the port used by Redis to accept client commands; add 10000 to this to obtain the cluster bus port). 6\u3001The state of the cluster from the point of view of the sender (down or ok). 7\u3001The master node ID of the sending node, if it is a slave. Failure detection Redis Cluster failure detection is used to recognize when a master or slave node is no longer reachable by the majority of nodes and then respond by promoting a slave to the role of master. When slave promotion is not possible the cluster is put in an error state to stop receiving queries from clients. As already mentioned, every node takes a list of flags associated with other known nodes. There are two flags that are used for failure detection that are called PFAIL and FAIL . PFAIL means Possible failure , and is a non-acknowledged failure type. FAIL means that a node is failing and that this condition was confirmed by a majority of masters within a fixed amount of time. NOTE: \u9605\u8bfb\u4e86\u4e0b\u9762\u7684\u5185\u5bb9\u5c31\u80fd\u591f\u660e\u767d\u6700\u540e\u8fd9\u6bb5\u8bdd\u4e2d\u5173\u4e8e FAIL \u7684\u89e3\u91ca\u4e2d\u7684\u4e24\u4e2a\u9650\u5236\u6761\u4ef6\uff1a 1\u3001confirmed by a majority of masters 2\u3001within a fixed amount of time PFAIL flag: A node flags another node with the PFAIL flag when the node is not reachable for more than NODE_TIMEOUT time. Both master and slave nodes can flag another node as PFAIL , regardless of its type. The concept of non-reachability for a Redis Cluster node is that we have an active ping (a ping that we sent for which we have yet to get a reply) pending for longer than NODE_TIMEOUT . For this mechanism to work the NODE_TIMEOUT must be large compared to the network round trip time . In order to add reliability during normal operations, nodes will try to reconnect with other nodes in the cluster as soon as half of the NODE_TIMEOUT has elapsed without a reply to a ping. This mechanism ensures that connections are kept alive so broken connections usually won't result in false failure reports between nodes. FAIL flag: The PFAIL flag alone is just local information every node has about other nodes, but it is not sufficient to trigger a slave promotion . For a node to be considered down\uff08down\u8868\u793afail\u4e86\uff09 the PFAIL condition needs to be escalated to a FAIL condition. As outlined in the node heartbeats section of this document, every node sends gossip messages to every other node including the state of a few random known nodes. Every node eventually receives a set of node flags for every other node. This way every node has a mechanism to signal other nodes about failure conditions they have detected. A PFAIL condition is escalated to a FAIL condition when the following set of conditions are met: Some node, that we'll call A, has another node B flagged as PFAIL . Node A collected, via gossip sections, information about the state of B from the point of view of the majority of masters in the cluster. The majority of masters signaled the PFAIL or FAIL condition within NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT time. (The validity factor is set to 2 in the current implementation, so this is just two times the NODE_TIMEOUT time). If all the above conditions are true, Node A will: Mark the node as FAIL . Send a FAIL message to all the reachable nodes. The FAIL message will force every receiving node to mark the node in FAIL state, whether or not it already flagged the node in PFAIL state. Note that the FAIL flag is mostly one way . That is, a node can go from PFAIL to FAIL , but a FAIL flag can only be cleared in the following situations: The node is already reachable and is a slave. In this case the FAIL flag can be cleared as slaves are not failed over. The node is already reachable and is a master not serving any slot. In this case the FAIL flag can be cleared as masters without slots do not really participate in the cluster and are waiting to be configured in order to join the cluster. The node is already reachable and is a master, but a long time (N times the NODE_TIMEOUT ) has elapsed without any detectable slave promotion. It's better for it to rejoin the cluster and continue in this case. It is useful to note that while the PFAIL -> FAIL transition uses a form of agreement , the agreement used is weak : Nodes collect views of other nodes over some time period, so even if the majority of master nodes need to \"agree\", actually this is just state that we collected from different nodes at different times and we are not sure, nor we require, that at a given moment the majority of masters agreed. However we discard failure reports which are old , so the failure was signaled by the majority of masters within a window of time . While every node detecting the FAIL condition will force that condition on other nodes in the cluster using the FAIL message, there is no way to ensure the message will reach all the nodes. For instance a node may detect the FAIL condition and because of a partition will not be able to reach any other node. However the Redis Cluster failure detection has a liveness requirement : eventually all the nodes should agree about the state of a given node. There are two cases that can originate from split brain conditions. Either some minority of nodes believe the node is in FAIL state, or a minority of nodes believe the node is not in FAIL state. In both the cases eventually the cluster will have a single view of the state of a given node: Case 1 : If a majority of masters have flagged a node as FAIL , because of failure detection and the chain effect it generates, every other node will eventually flag the master as FAIL , since in the specified window of time enough failures will be reported. Case 2 : When only a minority of masters have flagged a node as FAIL , the slave promotion will not happen (as it uses a more formal algorithm that makes sure everybody knows about the promotion eventually) and every node will clear the FAIL state as per the FAIL state clearing rules above (i.e. no promotion after N times the NODE_TIMEOUT has elapsed). The FAIL flag is only used as a trigger to run the safe part of the algorithm for the slave promotion . In theory a slave may act independently and start a slave promotion when its master is not reachable, and wait for the masters to refuse to provide the acknowledgment if the master is actually reachable by the majority. However the added complexity of the PFAIL -> FAIL state, the weak agreement, and the FAIL message forcing the propagation of the state in the shortest amount of time in the reachable part of the cluster, have practical advantages. Because of these mechanisms, usually all the nodes will stop accepting writes at about the same time if the cluster is in an error state. This is a desirable feature from the point of view of applications using Redis Cluster. Also erroneous election attempts initiated by slaves that can't reach its master due to local problems (the master is otherwise reachable by the majority of other master nodes) are avoided. Configuration handling, propagation, and failovers NOTE: currentEpoch Cluster current epoch \u662f cluster \u4e2d\u6240\u6709\u7684node\u7684consensus\u5171\u8bc6 configEpoch Configuration epoch \u6bcf\u4e2anode\u7684configuration Cluster current epoch Redis Cluster uses a concept similar to the Raft algorithm \"term\". In Redis Cluster the term is called epoch instead, and it is used in order to give incremental versioning to events. When multiple nodes provide conflicting information, it becomes possible for another node to understand which state is the most up to date. The currentEpoch is a 64 bit unsigned number. At node creation every Redis Cluster node, both slaves and master nodes, set the currentEpoch to 0. Every time a packet is received from another node, if the epoch of the sender (part of the cluster bus messages header) is greater than the local node epoch, the currentEpoch is updated to the sender epoch. Because of these semantics, eventually all the nodes will agree to the greatest configEpoch in the cluster. This information is used when the state of the cluster is changed and a node seeks agreement in order to perform some action. Currently this happens only during slave promotion , as described in the next section. Basically the epoch is a logical clock for the cluster and dictates(\u8868\u660e) that given information wins over one with a smaller epoch. NOTE: \u4e00\u3001raft\u7b97\u6cd5 \u4e8c\u3001\"epoch is a logical clock for the cluster\" \u4e09\u3001consensus\u5171\u8bc6 Configuration epoch Every master always advertises(\u5e7f\u4e3a\u544a\u77e5) its configEpoch in ping and pong packets along with a bitmap advertising the set of slots it serves. The configEpoch is set to zero in masters when a new node is created. A new configEpoch is created during slave election . Slaves trying to replace failing masters increment their epoch and try to get authorization from a majority of masters. When a slave is authorized, a new unique configEpoch is created and the slave turns into a master using the new configEpoch . As explained in the next sections, the configEpoch helps to resolve conflicts when different nodes claim divergent\uff08\u76f8\u5f02\u7684\uff09 configurations (a condition that may happen because of network partitions and node failures). Slave nodes also advertise(\u5e7f\u4e3a\u544a\u77e5) the configEpoch field in ping and pong packets, but in the case of slaves the field represents the configEpoch of its master as of the last time they exchanged packets. This allows other instances to detect when a slave has an old configuration that needs to be updated (master nodes will not grant votes to slaves with an old configuration). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u6bd4\u8f83\u4e0d\u597d\u7406\u89e3\u7684\uff0c\u5b83\u7684\u610f\u601d\u662f\uff1aSlave node\u5728\u5b83\u4eec\u7684ping and pong packets\u4e2d\u4e5f\u4f1a\u5e26\u4e0a configEpoch \u5b57\u6bb5\uff0c\u4f46\u662f\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e2a configEpoch \u5b57\u6bb5 Every time the configEpoch changes for some known node, it is permanently stored in the nodes.conf file by all the nodes that receive this information. The same also happens for the currentEpoch value. These two variables are guaranteed to be saved and fsync-ed to disk when updated before a node continues its operations. The configEpoch values generated using a simple algorithm during failovers are guaranteed to be new, incremental, and unique. Slave election and promotion Slave election and promotion is handled by slave nodes , with the help of master nodes that vote for the slave to promote. A slave election happens when a master is in FAIL state from the point of view of at least one of its slaves that has the prerequisites\uff08\u524d\u63d0\u6761\u4ef6\uff09 in order to become a master. In order for a slave to promote itself to master, it needs to start an election and win it. All the slaves for a given master can start an election if the master is in FAIL state, however only one slave will win the election and promote itself to master. A slave starts an election when the following conditions are met: The slave's master is in FAIL state. The master was serving a non-zero number of slots. The slave replication link was disconnected from the master for no longer than a given amount of time, in order to ensure the promoted slave's data is reasonably fresh. This time is user configurable. In order to be elected, the first step for a slave is to increment its currentEpoch counter, and request votes from master instances. Votes are requested by the slave by broadcasting a FAILOVER_AUTH_REQUEST packet to every master node of the cluster. Then it waits for a maximum time of two times the NODE_TIMEOUT for replies to arrive (but always for at least 2 seconds). Once a master has voted for a given slave, replying positively with a FAILOVER_AUTH_ACK , it can no longer vote for another slave of the same master for a period of NODE_TIMEOUT * 2 . In this period it will not be able to reply to other authorization requests for the same master. This is not needed to guarantee safety, but useful for preventing multiple slaves from getting elected (even if with a different configEpoch ) at around the same time, which is usually not wanted. A slave discards any AUTH_ACK replies with an epoch that is less than the currentEpoch at the time the vote request was sent. This ensures it doesn't count votes intended for a previous election. Once the slave receives ACKs from the majority of masters, it wins the election. Otherwise if the majority is not reached within the period of two times NODE_TIMEOUT (but always at least 2 seconds), the election is aborted and a new one will be tried again after NODE_TIMEOUT * 4 (and always at least 4 seconds). Slave rank Masters reply to slave vote request Practical example of configuration epoch usefulness during partitions Hash slots configuration propagation UPDATE messages, a closer look How nodes rejoin the cluster Replica migration Replica migration algorithm configEpoch conflicts resolution algorithm Node resets Removing nodes from a cluster Publish/Subscribe Appendix","title":"Introduction"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#redis#redis#cluster#specification","text":"Welcome to the Redis Cluster Specification . Here you'll find information about algorithms and design rationales of Redis Cluster . This document is a work in progress as it is continuously synchronized with the actual implementation of Redis.","title":"redis Redis Cluster Specification"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#main#properties#and#rationales#of#the#design","text":"","title":"Main properties and rationales of the design"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#redis#cluster#goals","text":"Redis Cluster is a distributed implementation of Redis with the following goals, in order of importance in the design: 1\u3001High performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values. NOTE: \u4e00\u3001Redis\u90fd\u662f\u4f7f\u7528\u7684**asynchronous replication** \u4e8c\u3001\u5173\u4e8e \"no merge operations are performed on values\"\uff0c\u53c2\u89c1 \"Why merge operations are avoided\" \u7ae0\u8282 2\u3001Acceptable degree of write safety : the system tries (in a best-effort way) to retain all the writes originating from clients connected with the majority of the master nodes . Usually there are small windows where acknowledged writes can be lost. Windows to lose acknowledged writes are larger when clients are in a minority partition . NOTE: \u53ef\u63a5\u53d7\u7684\u5199\u5b89\u5168\u6027:\u7cfb\u7edf\u5c1d\u8bd5(\u4ee5\u6700\u4f73\u65b9\u5f0f)\u4fdd\u7559\u6765\u81ea\u8fde\u63a5\u5230\u5927\u591a\u6570\u4e3b\u8282\u70b9\u7684\u5ba2\u6237\u673a\u7684\u6240\u6709\u5199\u3002\u901a\u5e38\u6709\u4e00\u4e9b\u5c0f\u7a97\u53e3\uff0c\u5728\u90a3\u91cc\u53ef\u4ee5\u4e22\u5931\u5df2\u786e\u8ba4\u7684\u5199\u64cd\u4f5c\u3002\u5f53\u5ba2\u6237\u7aef\u4f4d\u4e8e\u5c11\u6570\u5206\u533a\u65f6\uff0c\u4e22\u5931\u5df2\u786e\u8ba4\u5199\u64cd\u4f5c\u7684\u7a97\u53e3\u4f1a\u66f4\u5927\u3002 3\u3001Availability: Redis Cluster is able to survive partitions where the majority of the master nodes are reachable and there is at least one reachable slave for every master node that is no longer reachable. Moreover using replicas migration , masters no longer replicated by any slave will receive one from a master which is covered by multiple slaves. NOTE: \u4e00\u3001\u53ef\u7528\u6027:Redis\u96c6\u7fa4\u80fd\u591f\u5728\u5927\u591a\u6570\u4e3b\u8282\u70b9\u90fd\u53ef\u8bbf\u95ee\u7684\u5206\u533a\u4e2d\u5b58\u6d3b\uff0c\u5e76\u4e14\u5bf9\u4e8e\u6bcf\u4e2a\u4e0d\u518d\u53ef\u8bbf\u95ee\u7684\u4e3b\u8282\u70b9\uff0c\u81f3\u5c11\u6709\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u4ece\u5c5e\u8282\u70b9\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u526f\u672c\u8fc1\u79fb\uff0c\u4e0d\u518d\u7531\u4efb\u4f55\u5974\u96b6\u590d\u5236\u7684\u4e3b\u4eba\u5c06\u4ece\u4e00\u4e2a\u7531\u591a\u4e2a\u5974\u96b6\u8986\u76d6\u7684\u4e3b\u4eba\u90a3\u91cc\u63a5\u6536\u4e00\u4e2a\u3002 \u4e8c\u3001\"replicas migration\"\u6307\u7684\u662fRedis cluster\u5c06\u4e00\u4e2a\u62e5\u6709\u591a\u4e2aslave\u7684master\u7684\u5176\u4e2d\u4e00\u4e2aslave\u6307\u5b9a\u4e3a\u53e6\u5916\u4e00\u4e2a\u6ca1\u6709slave\u7684master\u7684slave What is described in this document is implemented in Redis 3.0 or greater.","title":"Redis Cluster goals"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#implemented#subset","text":"Redis Cluster implements all the single key commands available in the non-distributed version of Redis. Commands performing complex multi-key operations like Set type unions or intersections are implemented as well as long as the keys all belong to the same node . Redis Cluster implements a concept called hash tags that can be used in order to force certain keys to be stored in the same node. However during manual reshardings, multi-key operations may become unavailable for some time while single key operations are always available. Redis Cluster does not support multiple databases like the stand alone version of Redis. There is just database 0 and the SELECT command is not allowed. NOTE: why?","title":"Implemented subset"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#clients#and#servers#roles#in#the#redis#cluster#protocol","text":"In Redis Cluster nodes are responsible for holding the data, and taking the state of the cluster, including mapping keys to the right nodes. Cluster nodes are also able to auto-discover other nodes, detect non-working nodes, and promote slave nodes to master when needed in order to continue to operate when a failure occurs. NOTE: \u5b9e\u73b0\u4e0a\uff0c cluster.h:struct clusterNode \u7528\u4e8e\u63cf\u8ff0cluster node\uff0c cluster.h:struct clusterState \u7528\u4e8e\u63cf\u8ff0the state of the cluster\uff1b\u6b63\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63cf\u8ff0\u7684\uff0c\u6bcf\u4e2acluster node\u90fd\u9700\u8981\u4f7f\u7528 cluster.h:struct clusterState \u6765\u8bb0\u5f55the state of the cluster\u3002 To perform their tasks all the cluster nodes are connected using a TCP bus and a binary protocol , called the Redis Cluster Bus . Every node is connected to every other node in the cluster using the cluster bus . Nodes use a gossip protocol 1\u3001to propagate information about the cluster in order to discover new nodes, 2\u3001to send ping packets to make sure all the other nodes are working properly, and 3\u3001to send cluster messages needed to signal specific conditions. The cluster bus is also used in order to propagate Pub/Sub messages across the cluster and to orchestrate manual failovers when requested by users ( manual failovers are failovers which are not initiated by the Redis Cluster failure detector, but by the system administrator directly). NOTE: \u4e00\u3001cluster node\u4e4b\u95f4\u7684\u8fde\u63a5\u65b9\u5f0f\u662f\u600e\u6837\u7684\uff1f \"Every node is connected to every other node in the cluster using the cluster bus \" \u8282\u70b9\u4e4b\u95f4\u662f\u76f8\u4e92\u8fde\u63a5\u7684 \u4e0a\u9762\u8bf4node use a gossip protocol\uff0c\u90a3\u80fd\u5426\u636e\u6b64\u63a8\u65ad\u51fa\u662f\u4e24\u4e24\u4e92\u8054\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\u53c2\u89c1\u4e0b\u9762\u7684topology\u3002 \u4e8c\u3001\u603b\u7684\u6765\u8bf4\uff0ccluster node\u4e4b\u95f4\u7684\u901a\u4fe1: 1\u3001 Redis Cluster Bus 2\u3001 gossip protocol Since cluster nodes are not able to proxy requests, clients may be redirected to other nodes using redirection errors -MOVED and -ASK . The client is in theory free to send requests to all the nodes in the cluster, getting redirected if needed, so the client is not required to hold the state of the cluster. However clients that are able to cache the map between keys and nodes can improve the performance in a sensible way.","title":"Clients and Servers roles in the Redis Cluster protocol"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#write#safety","text":"Redis Cluster uses asynchronous replication between nodes, and last failover wins implicit merge function. This means that the last elected master dataset eventually replaces all the other replicas. There is always a window of time when it is possible to lose writes during partitions. However these windows are very different in the case of a client that is connected to the majority of masters, and a client that is connected to the minority of masters. NOTE: \u4e00\u3001Redis Cluster\u4f7f\u7528\u8282\u70b9\u4e4b\u95f4\u7684\u5f02\u6b65\u590d\u5236\uff0c\u4e0a\u6b21\u6545\u969c\u8f6c\u79fb\u8d62\u5f97\u9690\u5f0f\u5408\u5e76\u529f\u80fd\u3002 \u8fd9\u610f\u5473\u7740\u6700\u540e\u9009\u51fa\u7684\u4e3b\u6570\u636e\u96c6\u6700\u7ec8\u5c06\u66ff\u6362\u6240\u6709\u5176\u4ed6\u526f\u672c\u3002 \u5728\u5206\u533a\u671f\u95f4\u53ef\u80fd\u4f1a\u4e22\u5931\u5199\u5165\u65f6\uff0c\u603b\u4f1a\u6709\u4e00\u4e2a\u65f6\u95f4\u7a97\u53e3\u3002 \u7136\u800c\uff0c\u5728\u8fde\u63a5\u5230\u5927\u591a\u6570\u4e3b\u8bbe\u5907\u7684\u5ba2\u6237\u7aef\u548c\u8fde\u63a5\u5230\u5c11\u6570\u4e3b\u8bbe\u5907\u7684\u5ba2\u6237\u7aef\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u7a97\u53e3\u662f\u975e\u5e38\u4e0d\u540c\u7684\u3002 \u4e8c\u3001\" last failover wins implicit merge function\"\u662f\u4ec0\u4e48\u542b\u4e49\u5462\uff1f \"the last elected master dataset eventually replaces all the other replicas\" \u5373 \u6240\u6709\u7684\u6570\u636e\u4ee5\u6700\u7ec8\"the last elected master\"\u7684\u6570\u636e\u4e3a\u51c6 Redis Cluster tries harder to retain\uff08\u4fdd\u6301\uff09 writes that are performed by clients connected to the majority of masters , compared to writes performed in the minority side . NOTE: \u9700\u8981\u6ce8\u610f\u539f\u6587\u7684\u5199\u4f5c\u601d\u8def: \u539f\u6587\u662f\u6839\u636e majority partition \u548c minority partition \u6765\u8fdb\u884c\u5206\u7c7b\u8ba8\u8bba\u7684","title":"Write safety"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#in#the#majority#partitions","text":"The following are examples of scenarios that lead to loss of acknowledged writes received in the majority partitions during failures: NOTE: \"acknowledged writes\" \u6307\u7684\u662f master\u63a5\u6536\u5230\u4e86write\u8bf7\u6c42 1\u3001A write may reach a master, but while the master may be able to reply to the client, the write may not be propagated to slaves via the asynchronous replication used between master and slave nodes . If the master dies without the write reaching the slaves, the write is lost forever if the master is unreachable for a long enough period that one of its slaves is promoted. This is usually hard to observe in the case of a total, sudden failure of a master node since masters try to reply to clients (with the acknowledge of the write) and slaves (propagating the write) at about the same time. However it is a real world failure mode . NOTE: \u4e00\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5176\u5b9e\u63cf\u8ff0\u5730\u5e76\u4e0d\u7b80\u4ecb\uff0c\u8ba9\u4eba\u96be\u4ee5\u6293\u4f4f\u91cd\u8981\u610f\u601d\uff0c\u5176\u5b9e\u573a\u666f\u975e\u5e38\u7b80\u5355: master\u63a5\u6536\u5230\u4e86write\uff0c\u5e76\"reply to the client\"\uff0c\u4f46\u662f \"the write may not be propagated to slaves via the asynchronous replication used between master and slave nodes \"\uff1b\u5982\u679c\u6b64\u65f6 \"the master dies without the write reaching the slaves\"\uff0c\u663e\u7136\u4f1a\u8fdb\u5165failover\u6d41\u7a0b\uff0c\u90a3\u4e48\u5c31\u5b58\u5728\u8fd9\u6837\u7684\u53ef\u80fd\u6027: \u4e00\u4e2a\u6ca1\u6709\u540c\u6b65\u521a\u521a\u7684write\u7684slave\uff0c\u88ab\u9009\u4e3e\u4e3a\u65b0\u7684master\uff0c\u90a3\u4e48 \"the write is lost forever if the master is unreachable for a long enough period that one of its slaves is promoted\" \u5728 csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f \u4e2d\uff0c\u5bf9\u8fd9\u4e2a\u573a\u666f\u6709\u5206\u6790 \u4e8c\u3001\u601d\u8003: \u5982\u4f55\u4fdd\u8bc1\u4e0d\u4e22\u5931\u5462\uff1fsynchronous replication 2\u3001Another theoretically possible failure mode where writes are lost is the following: A master is unreachable because of a partition. It gets failed over by one of its slaves. After some time it may be reachable again. A client with an out-of-date routing table may write to the old master before it is converted into a slave (of the new master) by the cluster. The second failure mode is unlikely to happen because master nodes unable to communicate with the majority of the other masters for enough time to be failed over will no longer accept writes, and when the partition is fixed writes are still refused for a small amount of time to allow other nodes to inform about configuration changes. This failure mode also requires that the client's routing table has not yet been updated. NOTE: \u4e00\u3001\" master nodes unable to communicate with the majority of the other masters for enough time to be failed over will no longer accept writes\" \u4e3b\u8bed\u662f master nodes \u5b9a\u8bed\u662f \"unable to communicate with the majority of the other masters for enough time to be failed over\" \u4e8c\u3001\u5728 \"csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f # \u7f51\u7edc\u5206\u533a\" \u6bb5\u4e2d\uff0c\u7ed9\u51fa\u4e86\u6bd4\u8f83\u597d\u7684\u56fe\u793a \u4e09\u3001\u5728partition\u6062\u590d\u540e\uff0c\"writes are still refused for a small amount of time to allow other nodes to inform about configuration changes\" \u662f\u8c01\u7ee7\u7eed\"refuse write\"\uff1f","title":"In the majority partitions"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#in#the#minority#side#of#a#partition","text":"Writes targeting the minority side of a partition have a larger window in which to get lost. For example, Redis Cluster loses a non-trivial number of writes on partitions where there is a minority of masters and at least one or more clients, since all the writes sent to the masters may potentially get lost if the masters are failed over in the majority side. Specifically, for a master to be failed over it must be unreachable by the majority of masters for at least NODE_TIMEOUT , so if the partition is fixed before that time, no writes are lost. When the partition lasts for more than NODE_TIMEOUT , all the writes performed in the minority side up to that point may be lost. However the minority side of a Redis Cluster will start refusing writes as soon as NODE_TIMEOUT time has elapsed without contact with the majority, so there is a maximum window after which the minority becomes no longer available. Hence, no writes are accepted or lost after that time. NOTE: \u611f\u89c9\u8fd9\u79cd\u60c5\u51b5\u548c\u524d\u9762\u7684\"2\"\u662f\u7c7b\u4f3c\u7684","title":"In the minority side of a partition"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#availability","text":"NOTE: \u4e00\u3001\"Redis Cluster is not available in the minority side of the partition\" \u9700\u8981\u5bf9\u5b83\u8fdb\u884c\u89e3\u91ca\uff0c\u7ed3\u5408\u524d\u9762\u5bf9\u5185\u5bb9\u3001\"csdn Redis Cluster \u4f1a\u4e22\u6570\u636e\u5417\uff1f # \u7f51\u7edc\u5206\u533a\" \u4e2d\u7684\u5185\u5bb9\uff0c\u53ef\u77e5: \u5728 NODE_TIMEOUT \u540e\uff0c \"the minority side of the partition\"\u5c31\u4e0d\u53ef\u7528\u4e86 Redis Cluster is not available in the minority side of the partition. In the majority side of the partition assuming that there are at least the majority of masters and a slave for every unreachable master, the cluster becomes available again after NODE_TIMEOUT time plus a few more seconds required for a slave to get elected and failover its master (failovers are usually executed in a matter of 1 or 2 seconds). NOTE: \u4e00\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f: majority side of the partition\u80fd\u591f\u6062\u590d\u662f\u6709\u524d\u63d0\u6761\u4ef6\u7684: 1\u3001the majority of masters 2\u3001slave for every unreachable master This means that Redis Cluster is designed to survive failures of a few nodes in the cluster, but it is not a suitable solution for applications that require availability in the event of large net splits. In the example of a cluster composed of N master nodes where every node has a single slave, the majority side of the cluster will remain available as long as a single node is partitioned away, and will remain available with a probability of 1-(1/(N*2-1)) when two nodes are partitioned away (after the first node fails we are left with N*2-1 nodes in total, and the probability of the only master without a replica to fail is 1/(N*2-1)) . For example, in a cluster with 5 nodes and a single slave per node, there is a 1/(5*2-1) = 11.11% probability that after two nodes are partitioned away from the majority, the cluster will no longer be available. NOTE: \u5728 segmentfault Redis Cluster availability \u5206\u6790 \u4e2d\uff0c\u5bf9\u4e0a\u8ff0\u8ba1\u7b97\u8fc7\u7a0b\u8fdb\u884c\u4e86\u8bf4\u660e: \u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5305\u542b N \u4e2a master \u7684\u96c6\u7fa4\uff0c\u6bcf\u4e2a master \u6709\u552f\u4e00 slave\u3002 \u5355\u4e2a node \u51fa\u73b0\u6545\u969c\uff0ccluster \u4ecd\u7136\u53ef\u7528\uff0c\u7b2c\u4e8c\u4e2a node \u518d\u51fa\u73b0\u6545\u969c\uff0c\u96c6\u7fa4\u4ecd\u7136\u53ef\u7528\u7684\u6982\u7387\u662f 1-(1/(N*2-1) \u3002 \u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff0c\u7b2c\u4e00\u4e2a node fail \u540e\uff0c\u96c6\u7fa4\u5269\u4e0b N*2-1 \u4e2a\u5065\u5eb7\u8282\u70b9\uff0c\u6b64\u65f6 orphan master \u6070\u597d fail \u7684\u6982\u7387\u662f 1/(N*2-1) \u3002 NOTE: orphan master \u6307\u7684\u662f\u6ca1\u6709slave\u7684master\uff0c\u4e0b\u9762\u8fdb\u884c\u4e86\u8bf4\u660e \u5957\u7528\u516c\u5f0f\uff0c\u5047\u8bbe N = 5\uff0c\u90a3\u4e48\uff0c2 \u4e2a\u8282\u70b9\u4ece majority partition \u51fa\u53bb\uff0c\u96c6\u7fa4\u4e0d\u53ef\u7528\u7684\u6982\u7387\u662f 11.11%\u3002 Thanks to a Redis Cluster feature called replicas migration the Cluster availability is improved in many real world scenarios by the fact that replicas migrate to orphaned masters (masters no longer having replicas). So at every successful failure event, the cluster may reconfigure the slaves layout in order to better resist the next failure. NOTE: \u5728 segmentfault Redis Cluster availability \u5206\u6790 \u4e2d\uff0c\u5bf9 replicas migration \u7684\u8fc7\u7a0b\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e","title":"Availability"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#performance","text":"In Redis Cluster nodes don't proxy commands to the right node in charge for a given key, but instead they redirect clients to the right nodes serving a given portion of the key space. Eventually clients obtain an up-to-date representation of the cluster and which node serves which subset of keys, so during normal operations clients directly contact the right nodes in order to send a given command. Because of the use of asynchronous replication, nodes do not wait for other nodes' acknowledgment of writes (if not explicitly requested using the WAIT command). Also, because multi-key commands are only limited to near keys, data is never moved between nodes except when resharding. Normal operations are handled exactly as in the case of a single Redis instance. This means that in a Redis Cluster with N master nodes you can expect the same performance as a single Redis instance multiplied by N as the design scales linearly. At the same time the query is usually performed in a single round trip, since clients usually retain persistent connections with the nodes, so latency figures are also the same as the single standalone Redis node case. Very high performance and scalability while preserving weak but reasonable forms of data safety and availability is the main goal of Redis Cluster.","title":"Performance"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#why#merge#operations#are#avoided","text":"NOTE: \u4e00\u3001\u5728 \"Write safety\"\u4e2d\u4ecb\u7ecd\u4e86\uff0cRedis\u91c7\u7528\u7684\u662f\" last failover wins implicit merge function\" \u4e8c\u3001\u8fd9\u4e00\u6bb5\u5219\u662f\u4ecb\u7ecd: \u4e3a\u4ec0\u4e48Redis\u8981\u8fd9\u6837\u505a\uff0c\u901a\u8fc7\u539f\u6587\u7684\u5185\u5bb9\u53ef\u77e5: \u56e0\u4e3aRedis\u7684data type\u662f\u6bd4\u8f83\u590d\u6742\u7684\u3001Redis\u7684\u6570\u636e\u91cf\u662f\u975e\u5e38\u5927\u7684\uff0c\u5982\u679c\u91c7\u7528merge\u7684\u65b9\u5f0f\uff0c\u90a3\u4e48\u590d\u6742\u5ea6\u5c06\u662f\u975e\u5e38\u9ad8\u7684\uff0c\u5c06\u9020\u6210 \"bottleneck\" Redis Cluster design avoids conflicting versions of the same key-value pair in multiple nodes as in the case of the Redis data model this is not always desirable. Values in Redis are often very large; it is common to see lists or sorted sets with millions of elements. Also data types are semantically(\u8bed\u4e49) complex. Transferring and merging these kind of values can be a major bottleneck and/or may require the non-trivial involvement of application-side logic, additional memory to store meta-data, and so forth. There are no strict technological limits here. CRDTs or synchronously replicated state machines can model complex data types similar to Redis. However, the actual run time behavior of such systems would not be similar to Redis Cluster. Redis Cluster was designed in order to cover the exact use cases of the non-clustered Redis version.","title":"Why merge operations are avoided"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#overview#of#redis#cluster#main#components","text":"","title":"Overview of Redis Cluster main components"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#keys#distribution#model","text":"The key space is split into 16384 slots , effectively setting an upper limit for the cluster size of 16384 master nodes (however the suggested max size of nodes is in the order of ~ 1000 nodes). NOTE: Each master node in a cluster handles a subset of the 16384 hash slots. cluster\u4e2d\u7684\u6240\u6709node\u5747\u5206\u8fd916384\u4e2aslot\uff0c\u6bcf\u4e2anode\u81f3\u5c11\u9700\u8981\u5206\u5f97\u4e00\u4e2aslot\uff0c\u6240\u4ee5cluster size\u7684\u4e0a\u9650\u5c31\u662f16374. struct clusterNode \u7684 slots \u6210\u5458\u53d8\u91cf\u4fdd\u5b58\u8fd9\u4e2anode\u7684\u6240\u6709\u7684slot\u3002 Each master node in a cluster handles a subset of the 16384 hash slots. The cluster is stable when there is no cluster reconfiguration in progress (i.e. where hash slots are being moved from one node to another). When the cluster is stable, a single hash slot will be served by a single node (however the serving node can have one or more slaves that will replace it in the case of net splits or failures, and that can be used in order to scale read operations where reading stale(\u65e7) data is acceptable). The base algorithm used to map keys to hash slots is the following (read the next paragraph for the hash tag exception to this rule): HASH_SLOT = CRC16 ( key ) mod 16384 The CRC16 is specified as follows: Name: XMODEM (also known as ZMODEM or CRC-16/ACORN) Width: 16 bit Poly: 1021 (That is actually x^{16} + x^{12} + x^{5} + 1 x^{16} + x^{12} + x^{5} + 1 ) Initialization: 0000 Reflect Input byte: False Reflect Output CRC: False Xor constant to output CRC: 0000 Output for \"123456789\": 31C3 14 out of 16 CRC16 output bits are used (this is why there is a modulo 16384 operation in the formula above). In our tests CRC16 behaved remarkably well in distributing different kinds of keys evenly across the 16384 slots. Note : A reference implementation of the CRC16 algorithm used is available in the Appendix A of this document. NOTE: \u5728\u4ee3\u7801\u4e2d\u662f\u7531 cluser.c:keyHashSlot \u51fd\u6570\u5b9e\u73b0\u7684key space\u5230hash slot\u7684\u6620\u5c04\u7684\uff1b","title":"Keys distribution model"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#keys#hash#tags","text":"There is an exception for the computation of the hash slot that is used in order to implement hash tags . Hash tags are a way to ensure that multiple keys are allocated in the same hash slot. This is used in order to implement multi-key operations in Redis Cluster. In order to implement hash tags, the hash slot for a key is computed in a slightly different way in certain conditions. If the key contains a \"{...}\" pattern only the substring between { and } is hashed in order to obtain the hash slot. However since it is possible that there are multiple occurrences of { or } the algorithm is well specified by the following rules: IF the key contains a { character. AND IF there is a } character to the right of { AND IF there are one or more characters between the first occurrence of { and the first occurrence of } . Then instead of hashing the key, only what is between the first occurrence of { and the following first occurrence of } is hashed. Examples: The two keys {user1000}.following and {user1000}.followers will hash to the same hash slot since only the substring user1000 will be hashed in order to compute the hash slot. For the key foo{}{bar} the whole key will be hashed as usually since the first occurrence of { is followed by } on the right without characters in the middle. For the key foo{{bar}}zap the substring {bar will be hashed, because it is the substring between the first occurrence of { and the first occurrence of } on its right. For the key foo{bar}{zap} the substring bar will be hashed, since the algorithm stops at the first valid or invalid (without bytes inside) match of { and } . What follows from the algorithm is that if the key starts with {} , it is guaranteed to be hashed as a whole. This is useful when using binary data as key names. Adding the hash tags exception, the following is an implementation of the HASH_SLOT function in Ruby and C language. Ruby example code: def HASH_SLOT ( key ) s = key . index \"{\" if s e = key . index \"}\" , s + 1 if e && e != s + 1 key = key [ s + 1. . e - 1 ] end end crc16 ( key ) % 16384 end C example code: unsigned int HASH_SLOT ( char * key , int keylen ) { int s , e ; /* start-end indexes of { and } */ /* Search the first occurrence of '{'. */ for ( s = 0 ; s < keylen ; s ++ ) if ( key [ s ] == '{' ) break ; /* No '{' ? Hash the whole key. This is the base case. */ if ( s == keylen ) return crc16 ( key , keylen ) & 16383 ; /* '{' found? Check if we have the corresponding '}'. */ for ( e = s + 1 ; e < keylen ; e ++ ) if ( key [ e ] == '}' ) break ; /* No '}' or nothing between {} ? Hash the whole key. */ if ( e == keylen || e == s + 1 ) return crc16 ( key , keylen ) & 16383 ; /* If we are here there is both a { and a } on its right. Hash * what is in the middle between { and }. */ return crc16 ( key + s + 1 , e - s -1 ) & 16383 ; }","title":"Keys hash tags"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#cluster#nodes#attributes","text":"","title":"Cluster nodes attributes"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#node#id#node#name","text":"NOTE: \u9700\u8981\u6ce8\u610f\uff0cnode ID\u5c31\u662fnode name Every node has a unique name in the cluster. The node name is the hex representation of a 160 bit random number, obtained the first time a node is started (usually using /dev/urandom ). The node will save its ID in the node configuration file , and will use the same ID forever, or at least as long as the node configuration file is not deleted by the system administrator, or a hard reset is requested via the CLUSTER RESET command. NOTE: \u4e00\u3001\u663e\u7136\uff0cnode name\u9700\u8981\u4fdd\u8bc1\u4e0d\u80fd\u591f\u51b2\u7a81\uff1b cluster.c:struct clusterNode \u7684 name \u6210\u5458\u53d8\u91cf\u5c31\u8868\u793a\u5b83\u7684node name\u3002 \u4e8c\u3001\u5982\u4f55\u4fdd\u8bc1\u751f\u6210\u7684node name\u4e0d\u51b2\u7a81\u5462\uff1f The node ID is used to identify every node across the whole cluster. It is possible for a given node to change its IP address without any need to also change the node ID . The cluster is also able to detect the change in IP/port and reconfigure using the gossip protocol running over the cluster bus. NOTE: reconfigure cluster The node ID is not the only information associated with each node, but is the only one that is always globally consistent \uff08\u4e0d\u4f1a\u518d\u6539\u53d8\uff09. Every node has also the following set of information associated. Some information is about the cluster configuration detail of this specific node, and is eventually consistent across the cluster. Some other information, like the last time a node was pinged, is instead local to each node. Every node maintains the following information about other nodes that it is aware of in the cluster: The node ID, IP and port of the node, a set of flags, what is the master of the node if it is flagged as slave , last time the node was pinged and the last time the pong was received, the current configuration epoch of the node (explained later in this specification), the link state and finally the set of hash slots served. NOTE: \u4e00\u3001cluster\u4e2d\u7684\u6bcf\u4e2anode\u8fd8\u9700\u8981\u4fdd\u5b58\u5176\u4ed6node\u7684\u4e00\u4e9b\u914d\u7f6e\u4fe1\u606f\uff1b\u90a3\u662f\u54ea\u4e9bnode\u5462\uff1f\u662fcluster\u4e2d\u7684\u6240\u6709\u5176\u4ed6\u7684node\u8fd8\u662f\u4ec5\u4ec5\u90a3\u4e9b\u548c\u5b83connect\u7684node\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u9605\u8bfb\u4e0b\u9762\u7684Cluster topology\u7ae0\u8282 A detailed explanation of all the node fields is described in the CLUSTER NODES documentation. The CLUSTER NODES command can be sent to any node in the cluster and provides the state of the cluster and the information for each node according to the local view the queried node has of the cluster. The following is sample output of the CLUSTER NODES command sent to a master node in a small cluster of three nodes.","title":"node ID /  node name"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#the#cluster#bus","text":"Every Redis Cluster node has an additional TCP port for receiving incoming connections from other Redis Cluster nodes. This port is at a fixed offset from the normal TCP port used to receive incoming connections from clients. To obtain the Redis Cluster port, 10000 should be added to the normal commands port. For example, if a Redis node is listening for client connections on port 6379, the Cluster bus port 16379 will also be opened. Node-to-node communication happens exclusively using the Cluster bus and the Cluster bus protocol : a binary protocol composed of frames of different types and sizes. The Cluster bus binary protocol is not publicly documented since it is not intended for external software devices to talk with Redis Cluster nodes using this protocol. However you can obtain more details about the Cluster bus protocol by reading the cluster.h and cluster.c files in the Redis Cluster source code.","title":"The Cluster bus"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#cluster#topology","text":"Redis Cluster is a full mesh\uff08\u7f51\u683c\uff09 where every node is connected with every other node using a TCP connection. NOTE: \u5173\u4e8efull mesh\uff0c\u53c2\u89c1https:// www.webopedia.com/TERM/M/mesh.html In a cluster of N nodes, every node has N-1 outgoing\uff08\u4f20\u51fa\uff09 TCP connections, and N-1 incoming\uff08\u4f20\u5165\uff09 connections. NOTE: \u6bcf\u4e2anode\u9700\u8981\u8fde\u63a5\u5230cluster\u4e2d\u7684\u5269\u4f59N-1\u4e2anode\uff0c\u540c\u65f6cluster\u4e2d\u7684\u53e6\u5916\u7684N-1\u4e2anode\u4e5f\u8981\u8fde\u63a5\u5230\u5b83\uff1b slave\u662f\u5426\u4e5f\u7b97\u662fcluster\u4e2d\u7684node\uff1fslave\u662f\u5426\u4e5f\u9700\u8981\u548ccluster\u4e2d\u7684\u53e6\u5916\u7684node interconnect\u5462\uff1f These TCP connections are kept alive all the time and are not created on demand. When a node expects a pong reply in response to a ping in the cluster bus, before waiting long enough to mark the node as unreachable, it will try to refresh the connection with the node by reconnecting from scratch. While Redis Cluster nodes form a full mesh, nodes use a gossip protocol and a configuration update mechanism in order to avoid exchanging too many messages between nodes during normal conditions , so the number of messages exchanged is not exponential. NOTE: \u5982\u4f55\u5b9e\u73b0\u7684\u5462\uff1f\u5728\u540e\u9762\u7684\"Fault Tolerance # Heartbeat and gossip messages\"\u7ae0\u8282\u4e2d\uff0c \u8fdb\u884c\u4e86\u8bf4\u660e: Usually a node will ping a few random nodes every second so that the total number of ping packets sent (and pong packets received) by each node is a constant amount regardless of the number of nodes in the cluster.","title":"Cluster topology"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#nodes#handshake","text":"NOTE: \u8fd9\u4e00\u6bb5\u5176\u5b9e\u544a\u8bc9\u6211\u4eec\u5982\u4f55\u7ec4\u5efaRedis cluster Nodes always accept connections on the cluster bus port, and even reply to pings when received, even if the pinging node is not trusted. However, all other packets will be discarded by the receiving node if the sending node is not considered part of the cluster. A node will accept another node as part of the cluster only in two ways: 1\u3001If a node presents itself with a MEET message. A meet message is exactly like a PING message, but forces the receiver to accept the node as part of the cluster. Nodes will send MEET messages to other nodes only if the system administrator requests this via the following command: CLUSTER MEET ip port NOTE: \u4e00\u3001 CLUSTER MEET ip port \u4e8c\u3001\u663e\u7136\uff0c\u901a\u8fc7\"CLUSTER MEET ip port\" command\u6765\u7ec4\u5efaRedis cluster \u4e09\u3001\u8fd9\u79cd\u65b9\u5f0f\u662fadministrator\u6765\u7ec4\u5efaRedis cluster 2\u3001A node will also register another node as part of the cluster if a node that is already trusted will gossip about this other node. So if A knows B, and B knows C, eventually B will send gossip messages to A about C. When this happens, A will register C as part of the network, and will try to connect with C. NOTE: \u4e00\u3001\u8fd9\u79cd\u65b9\u5f0f\u662fRedis node\u901a\u8fc7gossip\u534f\u8bae\u6765auto discover\u5176\u4ed6node\uff0c\u7136\u540e\u4e0e\u5b83\u5efa\u7acb\u8fde\u63a5 This means that as long as we join nodes in any connected graph, they'll eventually form a fully connected graph automatically. This means that the cluster is able to auto-discover other nodes, but only if there is a trusted relationship that was forced by the system administrator. NOTE: \u4e00\u3001\u5fc5\u987b\u8981\u7531administrator\u9996\u5148\u5efa\u7acb\u57fa\u672c\u7684\u8fde\u63a5\uff0c\u7136\u540e\u624d\u80fd\u591f\u5b9e\u73b0\u5168\u8fde\u63a5\u56fe This mechanism makes the cluster more robust but prevents different Redis clusters from accidentally mixing after change of IP addresses or other network related events.","title":"Nodes handshake"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#redirection#and#resharding","text":"","title":"Redirection and resharding"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#moved#redirection","text":"A Redis client is free to send queries to every node in the cluster, including slave nodes . The node will analyze the query, and if it is acceptable (that is, only a single key is mentioned in the query, or the multiple keys mentioned are all to the same hash slot ) it will lookup what node is responsible for the hash slot where the key or keys belong. If the hash slot is served by the node, the query is simply processed, otherwise the node will check its internal hash slot to node map \uff08hash slot\u5230node\u7684\u6620\u5c04\u5173\u7cfb\uff09, and will reply to the client with a MOVED error, like in the following example: GET x -MOVED 3999 127.0.0.1:6381 The error includes the hash slot of the key (3999) and the ip:port of the instance that can serve the query. The client needs to reissue the query to the specified node's IP address and port. Note that even if the client waits a long time before reissuing the query, and in the meantime the cluster configuration changed, the destination node will reply again with a MOVED error if the hash slot 3999 is now served by another node. The same happens if the contacted node had no updated information. So while from the point of view of the cluster nodes are identified by IDs we try to simplify our interface with the client just exposing a map between hash slots and Redis nodes identified by IP:port pairs. The client is not required to, but should try to memorize that hash slot 3999 is served by 127.0.0.1:6381. This way once a new command needs to be issued it can compute the hash slot of the target key and have a greater chance of choosing the right node. An alternative is to just refresh the whole client-side cluster layout using the CLUSTER NODES or CLUSTER SLOTS commands when a MOVED redirection is received. When a redirection is encountered, it is likely multiple slots were reconfigured rather than just one, so updating the client configuration as soon as possible is often the best strategy. Note that when the Cluster is stable (no ongoing changes in the configuration), eventually all the clients will obtain a map of hash slots -> nodes, making the cluster efficient, with clients directly addressing the right nodes without redirections, proxies or other single point of failure entities. A client must be also able to handle -ASK redirections that are described later in this document, otherwise it is not a complete Redis Cluster client.","title":"MOVED Redirection"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#cluster#live#reconfiguration","text":"NOTE: \u4e00\u3001\u8fd9\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u5c31\u662fresharding \u4e8c\u3001Redis\u4f7f\u7528\u7684\u662fhash tag\uff0cresharding\u7684\u8fc7\u7a0b\u5176\u5b9e\u5c31\u662f\u5bf9hash slot\u7684\u79fb\u52a8 Redis Cluster supports the ability to add and remove nodes while the cluster is running. Adding or removing a node is abstracted into the same operation: moving a hash slot from one node to another. This means that the same basic mechanism can be used in order to rebalance the cluster, add or remove nodes, and so forth. 1\u3001To add a new node to the cluster an empty node is added to the cluster and some set of hash slots are moved from existing nodes to the new node. 2\u3001To remove a node from the cluster the hash slots assigned to that node are moved to other existing nodes. 3\u3001To rebalance the cluster a given set of hash slots are moved between nodes. NOTE: \u4e00\u3001\u5bf9Redis cluster\u8fdb\u884cCRUD: 1\u3001\u9009\u62e9\u8282\u70b9 2\u3001\u5220\u9664\u8282\u70b9 3\u3001\u4fee\u6539\u8282\u70b9 4\u3001\u5bf9\u8282\u70b9\u8fdb\u884c\u67e5\u8be2 The core of the implementation is the ability to move hash slots around. From a practical point of view a hash slot is just a set of keys, so what Redis Cluster really does during resharding is to move keys from an instance to another instance. Moving a hash slot means moving all the keys that happen to hash into this hash slot. To understand how this works we need to show the CLUSTER subcommands that are used to manipulate the slots translation table in a Redis Cluster node . NOTE: slots translation table\u5e94\u8be5\u662f\u7531 cluster.h:struct clusterState \u7684 migrating_slots_to \u3001 importing_slots_from \u3001 slots \u6210\u5458\u53d8\u91cf\u5b9e\u73b0\u7684\uff1b The following subcommands are available (among others not useful in this case): 1\u3001 CLUSTER ADDSLOTS slot1 [slot2] ... [slotN] 2\u3001 CLUSTER DELSLOTS slot1 [slot2] ... [slotN] 3\u3001 CLUSTER SETSLOT slot NODE node 4\u3001 CLUSTER SETSLOT slot MIGRATING node 5\u3001 CLUSTER SETSLOT slot IMPORTING node The first two commands, ADDSLOTS and DELSLOTS , are simply used to assign (or remove) slots to a Redis node. Assigning a slot means to tell a given master node that it will be in charge of storing and serving content for the specified hash slot. After the hash slots are assigned they will propagate across the cluster using the gossip protocol, as specified later in the configuration propagation section. NOTE: \u4e00\u3001\u662f \"Hash slots configuration propagation\" \u6bb5","title":"Cluster live reconfiguration"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#addslots","text":"The ADDSLOTS command is usually used when a new cluster is created from scratch to assign each master node a subset of all the 16384 hash slots available.","title":"ADDSLOTS"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#delslots","text":"The DELSLOTS is mainly used for manual modification of a cluster configuration or for debugging tasks: in practice it is rarely used.","title":"DELSLOTS"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#setslot","text":"The SETSLOT subcommand is used to assign a slot to a specific node ID if the SETSLOT <slot> NODE form is used. Otherwise the slot can be set in the two special states MIGRATING and IMPORTING . Those two special states are used in order to migrate a hash slot from one node to another. 1\u3001When a slot is set as MIGRATING , the node will accept all queries that are about this hash slot , but only if the key in question exists, otherwise the query is forwarded using a -ASK redirection to the node that is target of the migration. 2\u3001When a slot is set as IMPORTING , the node will accept all queries that are about this hash slot , but only if the request is preceded by an ASKING command. If the ASKING command was not given by the client, the query is redirected to the real hash slot owner via a -MOVED redirection error, as would happen normally.","title":"SETSLOT"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#hash#slot#migration","text":"Let's make this clearer with an example of hash slot migration . Assume that we have two Redis master nodes, called A and B. We want to move hash slot 8 from A to B, so we issue commands like this: We send B: CLUSTER SETSLOT 8 IMPORTING A We send A: CLUSTER SETSLOT 8 MIGRATING B All the other nodes will continue to point clients to node \"A\" every time they are queried with a key that belongs to hash slot 8, so what happens is that: All queries about existing keys are processed by \"A\". All queries about non-existing keys in A are processed by \"B\", because \"A\" will redirect clients to \"B\". This way we no longer create new keys in \"A\". In the meantime, a special program called redis-trib used during reshardings and Redis Cluster configuration will migrate existing keys in hash slot 8 from A to B. This is performed using the following command: CLUSTER GETKEYSINSLOT slot count The above command will return count keys in the specified hash slot . For every key returned, redis-trib sends node \"A\" a MIGRATE command, that will migrate the specified key from A to B in an atomic way (both instances are locked for the time (usually very small time) needed to migrate a key so there are no race conditions). This is how MIGRATE works: MIGRATE target_host target_port key target_database id timeout MIGRATE will connect to the target instance , send a serialized version of the key, and once an OK code is received, the old key from its own dataset will be deleted. From the point of view of an external client a key exists either in A or B at any given time. In Redis Cluster there is no need to specify a database other than 0, but MIGRATE is a general command that can be used for other tasks not involving Redis Cluster. MIGRATE is optimized to be as fast as possible even when moving complex keys such as long lists, but in Redis Cluster reconfiguring the cluster where big keys are present is not considered a wise procedure if there are latency constraints in the application using the database. When the migration process is finally finished, the SETSLOT <slot> NODE <node-id> command is sent to the two nodes involved in the migration in order to set the slots to their normal state again. The same command is usually sent to all other nodes to avoid waiting for the natural propagation of the new configuration across the cluster. NOTE: \u79fb\u52a8key\uff0c\u662f\u5426\u9700\u8981\u79fb\u52a8\u8be5key\u5bf9\u5e94\u7684\u6570\u636e\uff1f\u9700\u8981\u7684\uff0c\u8981\u60f3\u5b8c\u6574\u5730\u7406\u89e3\u8fd9\u4e00\u8282\u7684\u5185\u5bb9\uff0c\u9700\u8981\u9605\u8bfb\uff1a Redis\u7cfb\u5217\u4e5d\uff1aredis\u96c6\u7fa4\u9ad8\u53ef\u7528","title":"Hash slot migration"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#ask#redirection","text":"","title":"ASK redirection"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#clients#first#connection#and#handling#of#redirections","text":"","title":"Clients first connection and handling of redirections"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#multiple#keys#operations","text":"","title":"Multiple keys operations"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#scaling#reads#using#slave#nodes","text":"NOTE: \u8fd9\u548c\u8bfb\u5199\u5206\u79bb\u6709\u5173\uff0c\u53c2\u89c1: zhihu redis\u9700\u8981\u8bfb\u5199\u5206\u79bb\u5417\uff1f cnblogs Redis\u8bfb\u5199\u5206\u79bb\u6280\u672f\u89e3\u6790 Normally slave nodes will redirect clients to the authoritative master for the hash slot involved in a given command, however clients can use slaves in order to scale reads using the READONLY command. READONLY tells a Redis Cluster slave node that the client is ok reading possibly stale data and is not interested in running write queries. When the connection is in readonly mode , the cluster will send a redirection to the client only if the operation involves keys not served by the slave's master node. This may happen because: 1\u3001The client sent a command about hash slots never served by the master of this slave. 2\u3001The cluster was reconfigured (for example resharded) and the slave is no longer able to serve commands for a given hash slot. When this happens the client should update its hashslot map as explained in the previous sections. The readonly state of the connection can be cleared using the READWRITE command.","title":"Scaling reads using slave nodes"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#fault#tolerance","text":"","title":"Fault Tolerance"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#heartbeat#and#gossip#messages","text":"Redis Cluster nodes continuously exchange ping and pong packets. Those two kind of packets have the same structure, and both carry important configuration information. The only actual difference is the message type field . We'll refer to the sum of ping and pong packets as heartbeat packets . Usually nodes send ping packets that will trigger the receivers to reply with pong packets . However this is not necessarily true. It is possible for nodes to just send pong packets to send information to other nodes about their configuration, without triggering a reply. This is useful, for example, in order to broadcast a new configuration as soon as possible. Usually a node will ping a few random nodes every second so that the total number of ping packets sent (and pong packets received) by each node is a constant amount regardless of the number of nodes in the cluster. NOTE: \u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5728 \"Cluster topology\" \u7ae0\u8282\u4e2d\uff0c\u5c31\u8c08\u5230\u4e86\u8fd9\u4e2a\u95ee\u9898 However every node makes sure to ping every other node that hasn't sent a ping or received a pong for longer than half the NODE_TIMEOUT time. Before NODE_TIMEOUT has elapsed, nodes also try to reconnect the TCP link with another node to make sure nodes are not believed to be unreachable only because there is a problem in the current TCP connection. NOTE: \u4e00\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6bcf\u4e2anode\u4e0d\u662f\u6bcf\u79d2\u949f\u90fd\u53bbping cluster\u4e2d\u5269\u4f59\u7684\u5176\u4ed6\u7684\u6240\u6709\u7684node\uff0c\u800c\u662f\u4fdd\u8bc1\u80fd\u591f\u5728half of the NODE_TIMEOUT \u4e2d\u80fd\u591fping\u5230cluster\u4e2d\u5269\u4f59\u7684\u5176\u4ed6\u7684\u6240\u6709\u7684node\uff0c\u6240\u4ee5\u5b83\u9700\u8981\u505a\u7684\u662f\u6bcf\u79d2\u949f\u53eaping\u4e00\u90e8\u5206\u3002 \u4e8c\u3001\u4e3a\u4ec0\u4e48\u662fhalf the NODE_TIMEOUT time\uff1f\u5728\u4e0b\u9762\u7684Failure detection\u7ae0\u8282\u4e2d\u7ed9\u51fa\u4e86\u8fd9\u6837\u505a\u7684\u539f\u56e0\u3002 The number of messages globally exchanged can be sizable if NODE_TIMEOUT is set to a small figure and the number of nodes (N) is very large, since every node will try to ping every other node for which they don't have fresh information every half the NODE_TIMEOUT time. For example in a 100 node cluster with a node timeout set to 60 seconds, every node will try to send 99 pings every 30 seconds, with a total amount of pings of 3.3 per second. Multiplied by 100 nodes, this is 330 pings per second in the total cluster. There are ways to lower the number of messages, however there have been no reported issues with the bandwidth currently used by Redis Cluster failure detection , so for now the obvious and direct design is used. Note that even in the above example, the 330 packets per second exchanged are evenly divided among 100 different nodes, so the traffic each node receives is acceptable.","title":"Heartbeat and gossip messages"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#heartbeat#packet#content","text":"Ping and pong packets contain a header that is common to all types of packets (for instance packets to request a failover vote ), and a special Gossip Section that is specific of Ping and Pong packets. The common header has the following information: 1\u3001Node ID, a 160 bit pseudorandom string that is assigned the first time a node is created and remains the same for all the life of a Redis Cluster node. 2\u3001The currentEpoch and configEpoch fields of the sending node that are used to mount the distributed algorithms used by Redis Cluster (this is explained in detail in the next sections). If the node is a slave the configEpoch is the last known configEpoch of its master. 3\u3001The node flags , indicating if the node is a slave, a master, and other single-bit node information. 4\u3001A bitmap of the hash slots served by the sending node, or if the node is a slave, a bitmap of the slots served by its master. 5\u3001The sender TCP base port (that is, the port used by Redis to accept client commands; add 10000 to this to obtain the cluster bus port). 6\u3001The state of the cluster from the point of view of the sender (down or ok). 7\u3001The master node ID of the sending node, if it is a slave.","title":"Heartbeat packet content"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#failure#detection","text":"Redis Cluster failure detection is used to recognize when a master or slave node is no longer reachable by the majority of nodes and then respond by promoting a slave to the role of master. When slave promotion is not possible the cluster is put in an error state to stop receiving queries from clients. As already mentioned, every node takes a list of flags associated with other known nodes. There are two flags that are used for failure detection that are called PFAIL and FAIL . PFAIL means Possible failure , and is a non-acknowledged failure type. FAIL means that a node is failing and that this condition was confirmed by a majority of masters within a fixed amount of time. NOTE: \u9605\u8bfb\u4e86\u4e0b\u9762\u7684\u5185\u5bb9\u5c31\u80fd\u591f\u660e\u767d\u6700\u540e\u8fd9\u6bb5\u8bdd\u4e2d\u5173\u4e8e FAIL \u7684\u89e3\u91ca\u4e2d\u7684\u4e24\u4e2a\u9650\u5236\u6761\u4ef6\uff1a 1\u3001confirmed by a majority of masters 2\u3001within a fixed amount of time","title":"Failure detection"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#pfail#flag","text":"A node flags another node with the PFAIL flag when the node is not reachable for more than NODE_TIMEOUT time. Both master and slave nodes can flag another node as PFAIL , regardless of its type. The concept of non-reachability for a Redis Cluster node is that we have an active ping (a ping that we sent for which we have yet to get a reply) pending for longer than NODE_TIMEOUT . For this mechanism to work the NODE_TIMEOUT must be large compared to the network round trip time . In order to add reliability during normal operations, nodes will try to reconnect with other nodes in the cluster as soon as half of the NODE_TIMEOUT has elapsed without a reply to a ping. This mechanism ensures that connections are kept alive so broken connections usually won't result in false failure reports between nodes.","title":"PFAIL flag:"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#fail#flag","text":"The PFAIL flag alone is just local information every node has about other nodes, but it is not sufficient to trigger a slave promotion . For a node to be considered down\uff08down\u8868\u793afail\u4e86\uff09 the PFAIL condition needs to be escalated to a FAIL condition. As outlined in the node heartbeats section of this document, every node sends gossip messages to every other node including the state of a few random known nodes. Every node eventually receives a set of node flags for every other node. This way every node has a mechanism to signal other nodes about failure conditions they have detected. A PFAIL condition is escalated to a FAIL condition when the following set of conditions are met: Some node, that we'll call A, has another node B flagged as PFAIL . Node A collected, via gossip sections, information about the state of B from the point of view of the majority of masters in the cluster. The majority of masters signaled the PFAIL or FAIL condition within NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT time. (The validity factor is set to 2 in the current implementation, so this is just two times the NODE_TIMEOUT time). If all the above conditions are true, Node A will: Mark the node as FAIL . Send a FAIL message to all the reachable nodes. The FAIL message will force every receiving node to mark the node in FAIL state, whether or not it already flagged the node in PFAIL state. Note that the FAIL flag is mostly one way . That is, a node can go from PFAIL to FAIL , but a FAIL flag can only be cleared in the following situations: The node is already reachable and is a slave. In this case the FAIL flag can be cleared as slaves are not failed over. The node is already reachable and is a master not serving any slot. In this case the FAIL flag can be cleared as masters without slots do not really participate in the cluster and are waiting to be configured in order to join the cluster. The node is already reachable and is a master, but a long time (N times the NODE_TIMEOUT ) has elapsed without any detectable slave promotion. It's better for it to rejoin the cluster and continue in this case. It is useful to note that while the PFAIL -> FAIL transition uses a form of agreement , the agreement used is weak : Nodes collect views of other nodes over some time period, so even if the majority of master nodes need to \"agree\", actually this is just state that we collected from different nodes at different times and we are not sure, nor we require, that at a given moment the majority of masters agreed. However we discard failure reports which are old , so the failure was signaled by the majority of masters within a window of time . While every node detecting the FAIL condition will force that condition on other nodes in the cluster using the FAIL message, there is no way to ensure the message will reach all the nodes. For instance a node may detect the FAIL condition and because of a partition will not be able to reach any other node. However the Redis Cluster failure detection has a liveness requirement : eventually all the nodes should agree about the state of a given node. There are two cases that can originate from split brain conditions. Either some minority of nodes believe the node is in FAIL state, or a minority of nodes believe the node is not in FAIL state. In both the cases eventually the cluster will have a single view of the state of a given node: Case 1 : If a majority of masters have flagged a node as FAIL , because of failure detection and the chain effect it generates, every other node will eventually flag the master as FAIL , since in the specified window of time enough failures will be reported. Case 2 : When only a minority of masters have flagged a node as FAIL , the slave promotion will not happen (as it uses a more formal algorithm that makes sure everybody knows about the promotion eventually) and every node will clear the FAIL state as per the FAIL state clearing rules above (i.e. no promotion after N times the NODE_TIMEOUT has elapsed). The FAIL flag is only used as a trigger to run the safe part of the algorithm for the slave promotion . In theory a slave may act independently and start a slave promotion when its master is not reachable, and wait for the masters to refuse to provide the acknowledgment if the master is actually reachable by the majority. However the added complexity of the PFAIL -> FAIL state, the weak agreement, and the FAIL message forcing the propagation of the state in the shortest amount of time in the reachable part of the cluster, have practical advantages. Because of these mechanisms, usually all the nodes will stop accepting writes at about the same time if the cluster is in an error state. This is a desirable feature from the point of view of applications using Redis Cluster. Also erroneous election attempts initiated by slaves that can't reach its master due to local problems (the master is otherwise reachable by the majority of other master nodes) are avoided.","title":"FAIL flag:"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#configuration#handling#propagation#and#failovers","text":"NOTE: currentEpoch Cluster current epoch \u662f cluster \u4e2d\u6240\u6709\u7684node\u7684consensus\u5171\u8bc6 configEpoch Configuration epoch \u6bcf\u4e2anode\u7684configuration","title":"Configuration handling, propagation, and failovers"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#cluster#current#epoch","text":"Redis Cluster uses a concept similar to the Raft algorithm \"term\". In Redis Cluster the term is called epoch instead, and it is used in order to give incremental versioning to events. When multiple nodes provide conflicting information, it becomes possible for another node to understand which state is the most up to date. The currentEpoch is a 64 bit unsigned number. At node creation every Redis Cluster node, both slaves and master nodes, set the currentEpoch to 0. Every time a packet is received from another node, if the epoch of the sender (part of the cluster bus messages header) is greater than the local node epoch, the currentEpoch is updated to the sender epoch. Because of these semantics, eventually all the nodes will agree to the greatest configEpoch in the cluster. This information is used when the state of the cluster is changed and a node seeks agreement in order to perform some action. Currently this happens only during slave promotion , as described in the next section. Basically the epoch is a logical clock for the cluster and dictates(\u8868\u660e) that given information wins over one with a smaller epoch. NOTE: \u4e00\u3001raft\u7b97\u6cd5 \u4e8c\u3001\"epoch is a logical clock for the cluster\" \u4e09\u3001consensus\u5171\u8bc6","title":"Cluster current epoch"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#configuration#epoch","text":"Every master always advertises(\u5e7f\u4e3a\u544a\u77e5) its configEpoch in ping and pong packets along with a bitmap advertising the set of slots it serves. The configEpoch is set to zero in masters when a new node is created. A new configEpoch is created during slave election . Slaves trying to replace failing masters increment their epoch and try to get authorization from a majority of masters. When a slave is authorized, a new unique configEpoch is created and the slave turns into a master using the new configEpoch . As explained in the next sections, the configEpoch helps to resolve conflicts when different nodes claim divergent\uff08\u76f8\u5f02\u7684\uff09 configurations (a condition that may happen because of network partitions and node failures). Slave nodes also advertise(\u5e7f\u4e3a\u544a\u77e5) the configEpoch field in ping and pong packets, but in the case of slaves the field represents the configEpoch of its master as of the last time they exchanged packets. This allows other instances to detect when a slave has an old configuration that needs to be updated (master nodes will not grant votes to slaves with an old configuration). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u6bd4\u8f83\u4e0d\u597d\u7406\u89e3\u7684\uff0c\u5b83\u7684\u610f\u601d\u662f\uff1aSlave node\u5728\u5b83\u4eec\u7684ping and pong packets\u4e2d\u4e5f\u4f1a\u5e26\u4e0a configEpoch \u5b57\u6bb5\uff0c\u4f46\u662f\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e2a configEpoch \u5b57\u6bb5 Every time the configEpoch changes for some known node, it is permanently stored in the nodes.conf file by all the nodes that receive this information. The same also happens for the currentEpoch value. These two variables are guaranteed to be saved and fsync-ed to disk when updated before a node continues its operations. The configEpoch values generated using a simple algorithm during failovers are guaranteed to be new, incremental, and unique.","title":"Configuration epoch"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#slave#election#and#promotion","text":"Slave election and promotion is handled by slave nodes , with the help of master nodes that vote for the slave to promote. A slave election happens when a master is in FAIL state from the point of view of at least one of its slaves that has the prerequisites\uff08\u524d\u63d0\u6761\u4ef6\uff09 in order to become a master. In order for a slave to promote itself to master, it needs to start an election and win it. All the slaves for a given master can start an election if the master is in FAIL state, however only one slave will win the election and promote itself to master. A slave starts an election when the following conditions are met: The slave's master is in FAIL state. The master was serving a non-zero number of slots. The slave replication link was disconnected from the master for no longer than a given amount of time, in order to ensure the promoted slave's data is reasonably fresh. This time is user configurable. In order to be elected, the first step for a slave is to increment its currentEpoch counter, and request votes from master instances. Votes are requested by the slave by broadcasting a FAILOVER_AUTH_REQUEST packet to every master node of the cluster. Then it waits for a maximum time of two times the NODE_TIMEOUT for replies to arrive (but always for at least 2 seconds). Once a master has voted for a given slave, replying positively with a FAILOVER_AUTH_ACK , it can no longer vote for another slave of the same master for a period of NODE_TIMEOUT * 2 . In this period it will not be able to reply to other authorization requests for the same master. This is not needed to guarantee safety, but useful for preventing multiple slaves from getting elected (even if with a different configEpoch ) at around the same time, which is usually not wanted. A slave discards any AUTH_ACK replies with an epoch that is less than the currentEpoch at the time the vote request was sent. This ensures it doesn't count votes intended for a previous election. Once the slave receives ACKs from the majority of masters, it wins the election. Otherwise if the majority is not reached within the period of two times NODE_TIMEOUT (but always at least 2 seconds), the election is aborted and a new one will be tried again after NODE_TIMEOUT * 4 (and always at least 4 seconds).","title":"Slave election and promotion"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#slave#rank","text":"","title":"Slave rank"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#masters#reply#to#slave#vote#request","text":"","title":"Masters reply to slave vote request"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#practical#example#of#configuration#epoch#usefulness#during#partitions","text":"","title":"Practical example of configuration epoch usefulness during partitions"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#hash#slots#configuration#propagation","text":"","title":"Hash slots configuration propagation"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#update#messages#a#closer#look","text":"","title":"UPDATE messages, a closer look"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#how#nodes#rejoin#the#cluster","text":"","title":"How nodes rejoin the cluster"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#replica#migration","text":"","title":"Replica migration"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#replica#migration#algorithm","text":"","title":"Replica migration algorithm"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#configepoch#conflicts#resolution#algorithm","text":"","title":"configEpoch conflicts resolution algorithm"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#node#resets","text":"","title":"Node resets"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#removing#nodes#from#a#cluster","text":"","title":"Removing nodes from a cluster"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#publishsubscribe","text":"","title":"Publish/Subscribe"},{"location":"cluster/redis-doc-Redis-Cluster-Specification/#appendix","text":"","title":"Appendix"},{"location":"cluster/redis-doc-Redis-Cluster-Tutorial/","text":"redis doc Redis cluster tutorial This document is a gentle introduction to Redis Cluster, that does not use complex to understand distributed systems concepts. It provides instructions about how to setup a cluster, test, and operate it, without going into the details that are covered in the Redis Cluster specification but just describing how the system behaves from the point of view of the user. However this tutorial tries to provide information about the availability and consistency characteristics of Redis Cluster from the point of view of the final user, stated in a simple to understand way. Note this tutorial requires Redis version 3.0 or higher. If you plan to run a serious Redis Cluster deployment, the more formal specification is a suggested reading, even if not strictly required. However it is a good idea to start from this document, play with Redis Cluster some time, and only later read the specification. Redis Cluster 101 Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes . Redis Cluster also provides some degree of availability during partitions , that is in practical terms the ability to continue the operations when some nodes fail or are not able to communicate. However the cluster stops to operate in the event of larger failures (for example when the majority of masters are unavailable). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684partition\u7684\u542b\u4e49\u53c2\u89c1 Network partition \uff0c\u5173\u4e8eavailability\uff0c\u53c2\u89c1 High availability So in practical terms, what do you get with Redis Cluster? 1\u3001The ability to automatically split your dataset among multiple nodes . 2\u3001The ability to continue operations when a subset of the nodes are experiencing failures or are unable to communicate with the rest of the cluster. Redis Cluster TCP ports Every Redis Cluster node requires two TCP connections open. The normal Redis TCP port used to serve clients, for example 6379, plus the port obtained by adding 10000 to the data port , so 16379 in the example. This second high port is used for the Cluster bus , that is a node-to-node communication channel using a binary protocol . The Cluster bus is used by nodes for failure detection, configuration update, failover authorization and so forth. Clients should never try to communicate with the cluster bus port , but always with the normal Redis command port , however make sure you open both ports in your firewall, otherwise Redis cluster nodes will be not able to communicate. SUMMARY : struct redisServer \u7684\u6210\u5458 port \u5c31\u8868\u793a\u4e0a\u8ff0\u7684redis command port\uff0c\u6210\u5458 cluster_announce_port \u5c31\u8868\u793a\u4e0a\u8ff0\u7684cluster bus port\u3002 The command port and cluster bus port offset is fixed and is always 10000. Note that for a Redis Cluster to work properly you need, for each node: 1\u3001The normal client communication port (usually 6379) used to communicate with clients to be open to all the clients that need to reach the cluster, plus all the other cluster nodes (that use the client port for keys migrations). 2\u3001The cluster bus port (the client port + 10000) must be reachable from all the other cluster nodes. If you don't open both TCP ports, your cluster will not work as expected. The cluster bus uses a different, binary protocol, for node to node data exchange , which is more suited to exchange information between nodes using little bandwidth and processing time. Redis Cluster and Docker Currently Redis Cluster does not support NATted environments and in general environments where IP addresses or TCP ports are remapped. Docker uses a technique called port mapping : programs running inside Docker containers may be exposed with a different port compared to the one the program believes to be using. This is useful in order to run multiple containers using the same ports, at the same time, in the same server. In order to make Docker compatible with Redis Cluster you need to use the host networking mode of Docker. Please check the --net=host option in the Docker documentation for more information. Redis Cluster data sharding Redis Cluster does not use consistent hashing , but a different form of sharding where every key is conceptually part of what we call an hash slot . There are 16384 hash slots in Redis Cluster , and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384. Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where: Node A contains hash slots from 0 to 5500. Node B contains hash slots from 5501 to 11000. Node C contains hash slots from 11001 to 16383. This allows to add and remove nodes in the cluster easily. For example if I want to add a new node D, I need to move some hash slot from nodes A, B, C to D. Similarly if I want to remove node A from the cluster I can just move the hash slots served by A to B and C. When the node A will be empty I can remove it from the cluster completely. NOTE: hash slot\u662f\u79fb\u52a8\u7684\u5355\u4f4d Because moving hash slots from a node to another does not require to stop operations, adding and removing nodes, or changing the percentage of hash slots hold by nodes, does not require any downtime\uff08\u505c\u673a\u65f6\u95f4\uff09. NOTE: \u8fd9\u5c31\u662f\u5728 Redis Cluster Specification \u4e2d\u6240\u8ff0\u7684 \"Cluster live reconfiguration\" Redis Cluster supports multiple key operations as long as all the keys involved into a single command execution (or whole transaction, or Lua script execution) all belong to the same hash slot . The user can force multiple keys to be part of the same hash slot by using a concept called hash tags . Hash tags are documented in the Redis Cluster specification , but the gist\uff08\u8981\u70b9\u662f\uff09 is that if there is a substring between {} brackets in a key, only what is inside the string is hashed, so for example this{foo}key and another{foo}key are guaranteed to be in the same hash slot , and can be used together in a command with multiple keys as arguments. Redis Cluster master-slave model In order to remain available when a subset of master nodes are failing or are not able to communicate with the majority of nodes, Redis Cluster uses a master-slave model where every hash slot has from 1 (the master itself) to N replicas (N-1 additional slaves nodes). In our example cluster with nodes A, B, C, if node B fails the cluster is not able to continue, since we no longer have a way to serve hash slots in the range 5501-11000. However when the cluster is created (or at a later time) we add a slave node to every master , so that the final cluster is composed of A, B, C that are masters nodes , and A1, B1, C1 that are slaves nodes , the system is able to continue if node B fails. Node B1 replicates B, and B fails, the cluster will promote node B1 as the new master and will continue to operate correctly. However note that if nodes B and B1 fail at the same time Redis Cluster is not able to continue to operate. Redis Cluster consistency guarantees Redis Cluster is not able to guarantee strong consistency . In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client. The first reason why Redis Cluster can lose writes is because it uses asynchronous replication . This means that during writes the following happens: Your client writes to the master B. The master B replies OK to your client. The master B propagates the write to its slaves B1, B2 and B3. As you can see B does not wait for an acknowledge from B1, B2, B3 before replying to the client, since this would be a prohibitive\uff08\u7981\u6b62\u7684\uff09 latency penalty for Redis, so if your client writes something, B acknowledges the write, but crashes before being able to send the write to its slaves, one of the slaves (that did not receive the write) can be promoted to master, losing the write forever. This is very similar to what happens with most databases that are configured to flush data to disk every second, so it is a scenario you are already able to reason about because of past experiences with traditional database systems not involving distributed systems. Similarly you can improve consistency by forcing the database to flush data on disk before replying to the client, but this usually results into prohibitively low performance. That would be the equivalent of synchronous replication in the case of Redis Cluster. Basically there is a trade-off to take between performance and consistency . Redis Cluster has support for synchronous writes when absolutely needed, implemented via the WAIT command, this makes losing writes a lot less likely, however note that Redis Cluster does not implement strong consistency even when synchronous replication is used: it is always possible under more complex failure scenarios that a slave that was not able to receive the write is elected as master . NOTE: \u4e0a\u9762\u6ca1\u6709\u7ed9\u51fa\u4ed4\u7ec6\u7684\u5206\u6790 There is another notable scenario where Redis Cluster will lose writes, that happens during a network partition where a client is isolated with a minority of instances including at least a master. NOTE: \u5728 Redis Cluster Specification \u4e2d\u7684\"write safety\"\u7ae0\u8282\u4e2d\uff0c\u5bf9\u4e0a\u8ff0\u4e24\u79cd\u60c5\u51b5\u90fd\u8fdb\u884c\u4e86\u5206\u6790 Take as an example our 6 nodes cluster composed of A, B, C, A1, B1, C1, with 3 masters and 3 slaves. There is also a client, that we will call Z1. After a partition occurs, it is possible that in one side of the partition we have A, C, A1, B1, C1, and in the other side we have B and Z1. Z1 is still able to write to B, that will accept its writes. If the partition heals in a very short time, the cluster will continue normally. However if the partition lasts enough time for B1 to be promoted to master in the majority side of the partition, the writes that Z1 is sending to B will be lost. Note that there is a maximum window to the amount of writes Z1 will be able to send to B: if enough time has elapsed for the majority side of the partition to elect a slave as master, every master node in the minority side stops accepting writes. This amount of time is a very important configuration directive of Redis Cluster, and is called the node timeout . After node timeout has elapsed, a master node is considered to be failing, and can be replaced by one of its replicas. Similarly after node timeout has elapsed without a master node to be able to sense the majority of the other master nodes, it enters an error state and stops accepting writes. \u8fd8\u6709\u53e6\u4e00\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u60c5\u51b5\u662f\uff0cRedis\u7fa4\u96c6\u5c06\u4e22\u5931\u5199\u5165\uff0c\u8fd9\u79cd\u60c5\u51b5\u53d1\u751f\u5728\u7f51\u7edc\u5206\u533a\u4e2d\uff0c\u5176\u4e2d\u5ba2\u6237\u7aef\u4e0e\u5c11\u6570\u5b9e\u4f8b\uff08\u81f3\u5c11\u5305\u62ec\u4e3b\u670d\u52a1\u5668\uff09\u9694\u79bb\u3002 \u4ee56\u4e2a\u8282\u70b9\u7c07\u4e3a\u4f8b\uff0c\u5305\u62ecA\uff0cB\uff0cC\uff0cA1\uff0cB1\uff0cC1\uff0c3\u4e2a\u4e3b\u7ad9\u548c3\u4e2a\u4ece\u7ad9\u3002\u8fd8\u6709\u4e00\u4e2a\u5ba2\u6237\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aZ1\u3002 \u5728\u53d1\u751f\u5206\u533a\u4e4b\u540e\uff0c\u53ef\u80fd\u5728\u5206\u533a\u7684\u4e00\u4fa7\u6709A\uff0cC\uff0cA1\uff0cB1\uff0cC1\uff0c\u5728\u53e6\u4e00\u4fa7\u6709B\u548cZ1\u3002 Z1\u4ecd\u7136\u53ef\u4ee5\u5199\u5165B\uff0c\u5b83\u5c06\u63a5\u53d7\u5176\u5199\u5165\u3002\u5982\u679c\u5206\u533a\u5728\u5f88\u77ed\u7684\u65f6\u95f4\u5185\u6062\u590d\uff0c\u7fa4\u96c6\u5c06\u7ee7\u7eed\u6b63\u5e38\u8fd0\u884c\u3002\u4f46\u662f\uff0c\u5982\u679c\u5206\u533a\u6301\u7eed\u8db3\u591f\u7684\u65f6\u95f4\u4f7fB1\u5728\u5206\u533a\u7684\u591a\u6570\u4fa7\u88ab\u63d0\u5347\u4e3a\u4e3b\uff0c\u5219Z1\u53d1\u9001\u7ed9B\u7684\u5199\u5165\u5c06\u4e22\u5931\u3002 \u8bf7\u6ce8\u610f\uff0cZ1\u5c06\u80fd\u591f\u53d1\u9001\u5230B\u7684\u5199\u5165\u91cf\u5b58\u5728\u6700\u5927\u7a97\u53e3\uff1a\u5982\u679c\u5206\u533a\u7684\u591a\u6570\u65b9\u9762\u5df2\u7ecf\u6709\u8db3\u591f\u7684\u65f6\u95f4\u5c06\u4ece\u5c5e\u8bbe\u5907\u9009\u4e3a\u4e3b\u8bbe\u5907\uff0c\u5219\u5c11\u6570\u7aef\u7684\u6bcf\u4e2a\u4e3b\u8282\u70b9\u90fd\u4f1a\u505c\u6b62\u63a5\u53d7\u5199\u5165\u3002 \u8fd9\u6bb5\u65f6\u95f4\u662fRedis Cluster\u7684\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u914d\u7f6e\u6307\u4ee4\uff0c\u79f0\u4e3a\u8282\u70b9\u8d85\u65f6\u3002 \u8282\u70b9\u8d85\u65f6\u8fc7\u540e\uff0c\u4e3b\u8282\u70b9\u88ab\u89c6\u4e3a\u5931\u8d25\uff0c\u53ef\u4ee5\u7531\u5176\u4e2d\u4e00\u4e2a\u526f\u672c\u66ff\u6362\u3002\u7c7b\u4f3c\u5730\uff0c\u5728\u8282\u70b9\u8d85\u65f6\u5df2\u7ecf\u8fc7\u53bb\u800c\u4e3b\u8282\u70b9\u65e0\u6cd5\u611f\u77e5\u5927\u591a\u6570\u5176\u4ed6\u4e3b\u8282\u70b9\u4e4b\u540e\uff0c\u5b83\u8fdb\u5165\u9519\u8bef\u72b6\u6001\u5e76\u505c\u6b62\u63a5\u53d7\u5199\u5165\u3002","title":"Introduction"},{"location":"cluster/redis-doc-Redis-Cluster-Tutorial/#redis#doc#redis#cluster#tutorial","text":"This document is a gentle introduction to Redis Cluster, that does not use complex to understand distributed systems concepts. It provides instructions about how to setup a cluster, test, and operate it, without going into the details that are covered in the Redis Cluster specification but just describing how the system behaves from the point of view of the user. However this tutorial tries to provide information about the availability and consistency characteristics of Redis Cluster from the point of view of the final user, stated in a simple to understand way. Note this tutorial requires Redis version 3.0 or higher. If you plan to run a serious Redis Cluster deployment, the more formal specification is a suggested reading, even if not strictly required. However it is a good idea to start from this document, play with Redis Cluster some time, and only later read the specification.","title":"redis doc Redis cluster tutorial"},{"location":"cluster/redis-doc-Redis-Cluster-Tutorial/#redis#cluster#101","text":"Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes . Redis Cluster also provides some degree of availability during partitions , that is in practical terms the ability to continue the operations when some nodes fail or are not able to communicate. However the cluster stops to operate in the event of larger failures (for example when the majority of masters are unavailable). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684partition\u7684\u542b\u4e49\u53c2\u89c1 Network partition \uff0c\u5173\u4e8eavailability\uff0c\u53c2\u89c1 High availability So in practical terms, what do you get with Redis Cluster? 1\u3001The ability to automatically split your dataset among multiple nodes . 2\u3001The ability to continue operations when a subset of the nodes are experiencing failures or are unable to communicate with the rest of the cluster.","title":"Redis Cluster 101"},{"location":"cluster/redis-doc-Redis-Cluster-Tutorial/#redis#cluster#tcp#ports","text":"Every Redis Cluster node requires two TCP connections open. The normal Redis TCP port used to serve clients, for example 6379, plus the port obtained by adding 10000 to the data port , so 16379 in the example. This second high port is used for the Cluster bus , that is a node-to-node communication channel using a binary protocol . The Cluster bus is used by nodes for failure detection, configuration update, failover authorization and so forth. Clients should never try to communicate with the cluster bus port , but always with the normal Redis command port , however make sure you open both ports in your firewall, otherwise Redis cluster nodes will be not able to communicate. SUMMARY : struct redisServer \u7684\u6210\u5458 port \u5c31\u8868\u793a\u4e0a\u8ff0\u7684redis command port\uff0c\u6210\u5458 cluster_announce_port \u5c31\u8868\u793a\u4e0a\u8ff0\u7684cluster bus port\u3002 The command port and cluster bus port offset is fixed and is always 10000. Note that for a Redis Cluster to work properly you need, for each node: 1\u3001The normal client communication port (usually 6379) used to communicate with clients to be open to all the clients that need to reach the cluster, plus all the other cluster nodes (that use the client port for keys migrations). 2\u3001The cluster bus port (the client port + 10000) must be reachable from all the other cluster nodes. If you don't open both TCP ports, your cluster will not work as expected. The cluster bus uses a different, binary protocol, for node to node data exchange , which is more suited to exchange information between nodes using little bandwidth and processing time.","title":"Redis Cluster TCP ports"},{"location":"cluster/redis-doc-Redis-Cluster-Tutorial/#redis#cluster#and#docker","text":"Currently Redis Cluster does not support NATted environments and in general environments where IP addresses or TCP ports are remapped. Docker uses a technique called port mapping : programs running inside Docker containers may be exposed with a different port compared to the one the program believes to be using. This is useful in order to run multiple containers using the same ports, at the same time, in the same server. In order to make Docker compatible with Redis Cluster you need to use the host networking mode of Docker. Please check the --net=host option in the Docker documentation for more information.","title":"Redis Cluster and Docker"},{"location":"cluster/redis-doc-Redis-Cluster-Tutorial/#redis#cluster#data#sharding","text":"Redis Cluster does not use consistent hashing , but a different form of sharding where every key is conceptually part of what we call an hash slot . There are 16384 hash slots in Redis Cluster , and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384. Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where: Node A contains hash slots from 0 to 5500. Node B contains hash slots from 5501 to 11000. Node C contains hash slots from 11001 to 16383. This allows to add and remove nodes in the cluster easily. For example if I want to add a new node D, I need to move some hash slot from nodes A, B, C to D. Similarly if I want to remove node A from the cluster I can just move the hash slots served by A to B and C. When the node A will be empty I can remove it from the cluster completely. NOTE: hash slot\u662f\u79fb\u52a8\u7684\u5355\u4f4d Because moving hash slots from a node to another does not require to stop operations, adding and removing nodes, or changing the percentage of hash slots hold by nodes, does not require any downtime\uff08\u505c\u673a\u65f6\u95f4\uff09. NOTE: \u8fd9\u5c31\u662f\u5728 Redis Cluster Specification \u4e2d\u6240\u8ff0\u7684 \"Cluster live reconfiguration\" Redis Cluster supports multiple key operations as long as all the keys involved into a single command execution (or whole transaction, or Lua script execution) all belong to the same hash slot . The user can force multiple keys to be part of the same hash slot by using a concept called hash tags . Hash tags are documented in the Redis Cluster specification , but the gist\uff08\u8981\u70b9\u662f\uff09 is that if there is a substring between {} brackets in a key, only what is inside the string is hashed, so for example this{foo}key and another{foo}key are guaranteed to be in the same hash slot , and can be used together in a command with multiple keys as arguments.","title":"Redis Cluster data sharding"},{"location":"cluster/redis-doc-Redis-Cluster-Tutorial/#redis#cluster#master-slave#model","text":"In order to remain available when a subset of master nodes are failing or are not able to communicate with the majority of nodes, Redis Cluster uses a master-slave model where every hash slot has from 1 (the master itself) to N replicas (N-1 additional slaves nodes). In our example cluster with nodes A, B, C, if node B fails the cluster is not able to continue, since we no longer have a way to serve hash slots in the range 5501-11000. However when the cluster is created (or at a later time) we add a slave node to every master , so that the final cluster is composed of A, B, C that are masters nodes , and A1, B1, C1 that are slaves nodes , the system is able to continue if node B fails. Node B1 replicates B, and B fails, the cluster will promote node B1 as the new master and will continue to operate correctly. However note that if nodes B and B1 fail at the same time Redis Cluster is not able to continue to operate.","title":"Redis Cluster master-slave model"},{"location":"cluster/redis-doc-Redis-Cluster-Tutorial/#redis#cluster#consistency#guarantees","text":"Redis Cluster is not able to guarantee strong consistency . In practical terms this means that under certain conditions it is possible that Redis Cluster will lose writes that were acknowledged by the system to the client. The first reason why Redis Cluster can lose writes is because it uses asynchronous replication . This means that during writes the following happens: Your client writes to the master B. The master B replies OK to your client. The master B propagates the write to its slaves B1, B2 and B3. As you can see B does not wait for an acknowledge from B1, B2, B3 before replying to the client, since this would be a prohibitive\uff08\u7981\u6b62\u7684\uff09 latency penalty for Redis, so if your client writes something, B acknowledges the write, but crashes before being able to send the write to its slaves, one of the slaves (that did not receive the write) can be promoted to master, losing the write forever. This is very similar to what happens with most databases that are configured to flush data to disk every second, so it is a scenario you are already able to reason about because of past experiences with traditional database systems not involving distributed systems. Similarly you can improve consistency by forcing the database to flush data on disk before replying to the client, but this usually results into prohibitively low performance. That would be the equivalent of synchronous replication in the case of Redis Cluster. Basically there is a trade-off to take between performance and consistency . Redis Cluster has support for synchronous writes when absolutely needed, implemented via the WAIT command, this makes losing writes a lot less likely, however note that Redis Cluster does not implement strong consistency even when synchronous replication is used: it is always possible under more complex failure scenarios that a slave that was not able to receive the write is elected as master . NOTE: \u4e0a\u9762\u6ca1\u6709\u7ed9\u51fa\u4ed4\u7ec6\u7684\u5206\u6790 There is another notable scenario where Redis Cluster will lose writes, that happens during a network partition where a client is isolated with a minority of instances including at least a master. NOTE: \u5728 Redis Cluster Specification \u4e2d\u7684\"write safety\"\u7ae0\u8282\u4e2d\uff0c\u5bf9\u4e0a\u8ff0\u4e24\u79cd\u60c5\u51b5\u90fd\u8fdb\u884c\u4e86\u5206\u6790 Take as an example our 6 nodes cluster composed of A, B, C, A1, B1, C1, with 3 masters and 3 slaves. There is also a client, that we will call Z1. After a partition occurs, it is possible that in one side of the partition we have A, C, A1, B1, C1, and in the other side we have B and Z1. Z1 is still able to write to B, that will accept its writes. If the partition heals in a very short time, the cluster will continue normally. However if the partition lasts enough time for B1 to be promoted to master in the majority side of the partition, the writes that Z1 is sending to B will be lost. Note that there is a maximum window to the amount of writes Z1 will be able to send to B: if enough time has elapsed for the majority side of the partition to elect a slave as master, every master node in the minority side stops accepting writes. This amount of time is a very important configuration directive of Redis Cluster, and is called the node timeout . After node timeout has elapsed, a master node is considered to be failing, and can be replaced by one of its replicas. Similarly after node timeout has elapsed without a master node to be able to sense the majority of the other master nodes, it enters an error state and stops accepting writes. \u8fd8\u6709\u53e6\u4e00\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u60c5\u51b5\u662f\uff0cRedis\u7fa4\u96c6\u5c06\u4e22\u5931\u5199\u5165\uff0c\u8fd9\u79cd\u60c5\u51b5\u53d1\u751f\u5728\u7f51\u7edc\u5206\u533a\u4e2d\uff0c\u5176\u4e2d\u5ba2\u6237\u7aef\u4e0e\u5c11\u6570\u5b9e\u4f8b\uff08\u81f3\u5c11\u5305\u62ec\u4e3b\u670d\u52a1\u5668\uff09\u9694\u79bb\u3002 \u4ee56\u4e2a\u8282\u70b9\u7c07\u4e3a\u4f8b\uff0c\u5305\u62ecA\uff0cB\uff0cC\uff0cA1\uff0cB1\uff0cC1\uff0c3\u4e2a\u4e3b\u7ad9\u548c3\u4e2a\u4ece\u7ad9\u3002\u8fd8\u6709\u4e00\u4e2a\u5ba2\u6237\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aZ1\u3002 \u5728\u53d1\u751f\u5206\u533a\u4e4b\u540e\uff0c\u53ef\u80fd\u5728\u5206\u533a\u7684\u4e00\u4fa7\u6709A\uff0cC\uff0cA1\uff0cB1\uff0cC1\uff0c\u5728\u53e6\u4e00\u4fa7\u6709B\u548cZ1\u3002 Z1\u4ecd\u7136\u53ef\u4ee5\u5199\u5165B\uff0c\u5b83\u5c06\u63a5\u53d7\u5176\u5199\u5165\u3002\u5982\u679c\u5206\u533a\u5728\u5f88\u77ed\u7684\u65f6\u95f4\u5185\u6062\u590d\uff0c\u7fa4\u96c6\u5c06\u7ee7\u7eed\u6b63\u5e38\u8fd0\u884c\u3002\u4f46\u662f\uff0c\u5982\u679c\u5206\u533a\u6301\u7eed\u8db3\u591f\u7684\u65f6\u95f4\u4f7fB1\u5728\u5206\u533a\u7684\u591a\u6570\u4fa7\u88ab\u63d0\u5347\u4e3a\u4e3b\uff0c\u5219Z1\u53d1\u9001\u7ed9B\u7684\u5199\u5165\u5c06\u4e22\u5931\u3002 \u8bf7\u6ce8\u610f\uff0cZ1\u5c06\u80fd\u591f\u53d1\u9001\u5230B\u7684\u5199\u5165\u91cf\u5b58\u5728\u6700\u5927\u7a97\u53e3\uff1a\u5982\u679c\u5206\u533a\u7684\u591a\u6570\u65b9\u9762\u5df2\u7ecf\u6709\u8db3\u591f\u7684\u65f6\u95f4\u5c06\u4ece\u5c5e\u8bbe\u5907\u9009\u4e3a\u4e3b\u8bbe\u5907\uff0c\u5219\u5c11\u6570\u7aef\u7684\u6bcf\u4e2a\u4e3b\u8282\u70b9\u90fd\u4f1a\u505c\u6b62\u63a5\u53d7\u5199\u5165\u3002 \u8fd9\u6bb5\u65f6\u95f4\u662fRedis Cluster\u7684\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u914d\u7f6e\u6307\u4ee4\uff0c\u79f0\u4e3a\u8282\u70b9\u8d85\u65f6\u3002 \u8282\u70b9\u8d85\u65f6\u8fc7\u540e\uff0c\u4e3b\u8282\u70b9\u88ab\u89c6\u4e3a\u5931\u8d25\uff0c\u53ef\u4ee5\u7531\u5176\u4e2d\u4e00\u4e2a\u526f\u672c\u66ff\u6362\u3002\u7c7b\u4f3c\u5730\uff0c\u5728\u8282\u70b9\u8d85\u65f6\u5df2\u7ecf\u8fc7\u53bb\u800c\u4e3b\u8282\u70b9\u65e0\u6cd5\u611f\u77e5\u5927\u591a\u6570\u5176\u4ed6\u4e3b\u8282\u70b9\u4e4b\u540e\uff0c\u5b83\u8fdb\u5165\u9519\u8bef\u72b6\u6001\u5e76\u505c\u6b62\u63a5\u53d7\u5199\u5165\u3002","title":"Redis Cluster consistency guarantees"},{"location":"cluster/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/","text":"\u8bfb\u5199\u5206\u79bb cnblogs Redis\u8bfb\u5199\u5206\u79bb\u6280\u672f\u89e3\u6790","title":"Introduction"},{"location":"cluster/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/#_1","text":"","title":"\u8bfb\u5199\u5206\u79bb"},{"location":"cluster/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/#cnblogs#redis","text":"","title":"cnblogs Redis\u8bfb\u5199\u5206\u79bb\u6280\u672f\u89e3\u6790"},{"location":"deploy-topology/","text":"Redis deploy and topology \u672c\u7ae0\u5bf9Redis\u7684deploy\u65b9\u5f0f\u4ee5\u53ca\u5bf9\u5e94\u7684topology\u8fdb\u884c\u5206\u6790\u3002 \u53c2\u8003\u5185\u5bb9: 1\u3001rtfm Redis: replication, part 1 \u2013 an overview. Replication vs Sharding. Sentinel vs Cluster. Redis topology. \u5176\u4e2d\u603b\u7ed3\u4e86Redis\u7684\u6240\u6709\u53ef\u80fd\u7684\u90e8\u7f72\u65b9\u5f0f\uff0c\u6bd4\u8f83\u5168\u9762 2\u3001amazon Amazon ElastiCache for Redis \u5176\u4e2d\u603b\u7ed3\u4e86Redis sentinel\u3001Redis cluster","title":"Introduction"},{"location":"deploy-topology/#redis#deploy#and#topology","text":"\u672c\u7ae0\u5bf9Redis\u7684deploy\u65b9\u5f0f\u4ee5\u53ca\u5bf9\u5e94\u7684topology\u8fdb\u884c\u5206\u6790\u3002 \u53c2\u8003\u5185\u5bb9: 1\u3001rtfm Redis: replication, part 1 \u2013 an overview. Replication vs Sharding. Sentinel vs Cluster. Redis topology. \u5176\u4e2d\u603b\u7ed3\u4e86Redis\u7684\u6240\u6709\u53ef\u80fd\u7684\u90e8\u7f72\u65b9\u5f0f\uff0c\u6bd4\u8f83\u5168\u9762 2\u3001amazon Amazon ElastiCache for Redis \u5176\u4e2d\u603b\u7ed3\u4e86Redis sentinel\u3001Redis cluster","title":"Redis deploy and topology"},{"location":"deploy-topology/rtfm-Redis/","text":"rtfm Redis: replication, part 1 \u2013 an overview. Replication vs Sharding. Sentinel vs Cluster. Redis topology. Initially, it was planned to write one small post with an example how to create a Redis replication but as I read more and more details \u2013 I wanted to describe more and more about it, so eventually I split this post into two parts. In this one \u2013 some quick overview, a brief explanation about differences in Redis data distribution types, topology examples. In short terms but with links to detailed documentation and other useful posts on other resources. In the second part \u2013 a couple of examples of how to configure a simple replication and replication with Redis Sentinel. In the third part \u2013 the redis-py library examples with Redis replication and Sentinel. Redis Replication vs Sharding Redis supports two data sharing types replication (also known as mirroring , a data duplication), and sharding (also known as partitioning , a data segmentation). In this \u2013 Redis Cluster can use both methods simultaneously. Replication Is a data coping overall Redis nodes in a cluster which allows to make requests to one or more slave nodes and making data persistence if some of those nodes will go down, providing a High Availability . Using this approach \u2013 the read requests will be faster. See Replication and Redis Cluster master-slave model . Sharding With the data segmentation \u2013 all data will be split into a few parts and this will improve each node\u2019s performance as it will store only some part of the data and will not serve all requests. Using this approach \u2013 the write requests will go faster. See Partitioning: how to split data among multiple Redis instances and Redis Cluster data sharding . Redis Sentinel vs Redis Cluster Redis Sentinel Was added to Redis v.2.4 and basically is a monitoring service for master and slaves. Also, can send notifications, automatically switch masters and slaves roles if a master is down and so on. Might have a sense to be used for a bare master-slave replication (see below) without full clustering. It works as a dedicated daemon using a sentinel binary or redis-server in the sentinel mode. Will perform nodes reconfiguration if the master went down \u2013 will choose a new master from the slaves left. Requires at least three Sentinel instances to have a quorum for a new master election and to decide if one of a Redis nodes goes down Redis Cluster Was added to Redis v.3.0 and represents a full clustering solution for segmentation, replication, and its nodes management. Will perform data synchronization, replication, manage nodes access persistence if some will go down. The Sentinel usage in the Redis Cluster case doesn\u2019t make sense as Cluster will do everything itself. See Redis Sentinel & Redis Cluster \u2013 what? and Redis Sentinel Documentation . Redis topology One Redis instance The simplest and mor classical case. Simple in running and configuration. Limited by a host\u2019s resources \u2013 its CPU and memory. In case of such Redis instance will go down \u2013 all dependent services will be broken as well as there is no any availability or fault tolerance mechanisms. Master-slave replication One master which has multitype slaves attached. Data will be updated on this master and then the master will push those changes to its replicas. Slaves can talk to the master only and can\u2019t communicate with other slaves, but still can have their own slaves Slaves are read-only nodes \u2013 no data changes can be performed there unless this wasn\u2019t configured explicitly (see the second part of this post). In case of any node will go down \u2013 all data still will be available for clients as data is replicated over all nodes. Simple in configuration but the write operations are limited by the master\u2019s resources. In case of the master will go down \u2013 you\u2019ll have to manually reconfigure slaves and change slave to master roles for one on them. Also, clients need to know which they have to use for writes operations. Redis Sentinel Already described above but a few more words here. Similarly to the Redis Replication \u2013 Sentinel has one Master instance which has a priority when deciding on a Redis master\u2019s elections. I.e. in case of one Redis Master and two slaves and if Sentinel Master works on the same host where Redis Master is running and this host will go down \u2013 Sentinel will choose Sentinel\u2019s new Master instance and those two Sentinels instances need to decide which Redis slave will have to become a new Redis Master. During this \u2013 a Sentinel\u2019s Master will have more weight in such an election. Keep in mind that not every Redis client able to work with Sentinel, all client can be found here>>> . Redis Cluster And the most powerful solution \u2013 the Redis Cluster. Has a few master instances and each can have one more \u2013 up to 1000 \u2013 slaves. Will take care of data sharding, replication, synchronization, and failover operations. Must have at least 6 Redis nodes \u2013 3 for masters and three for slaves. Can redirect clients requests to a necessary master or slave host \u2013 but the client must have an ability to work with Redis Cluster.","title":"Introduction"},{"location":"deploy-topology/rtfm-Redis/#rtfm#redis#replication#part#1#an#overview#replication#vs#sharding#sentinel#vs#cluster#redis#topology","text":"Initially, it was planned to write one small post with an example how to create a Redis replication but as I read more and more details \u2013 I wanted to describe more and more about it, so eventually I split this post into two parts. In this one \u2013 some quick overview, a brief explanation about differences in Redis data distribution types, topology examples. In short terms but with links to detailed documentation and other useful posts on other resources. In the second part \u2013 a couple of examples of how to configure a simple replication and replication with Redis Sentinel. In the third part \u2013 the redis-py library examples with Redis replication and Sentinel.","title":"rtfm Redis: replication, part 1 \u2013 an overview. Replication vs Sharding. Sentinel vs Cluster. Redis topology."},{"location":"deploy-topology/rtfm-Redis/#redis#replication#vs#sharding","text":"Redis supports two data sharing types replication (also known as mirroring , a data duplication), and sharding (also known as partitioning , a data segmentation). In this \u2013 Redis Cluster can use both methods simultaneously.","title":"Redis Replication vs Sharding"},{"location":"deploy-topology/rtfm-Redis/#replication","text":"Is a data coping overall Redis nodes in a cluster which allows to make requests to one or more slave nodes and making data persistence if some of those nodes will go down, providing a High Availability . Using this approach \u2013 the read requests will be faster. See Replication and Redis Cluster master-slave model .","title":"Replication"},{"location":"deploy-topology/rtfm-Redis/#sharding","text":"With the data segmentation \u2013 all data will be split into a few parts and this will improve each node\u2019s performance as it will store only some part of the data and will not serve all requests. Using this approach \u2013 the write requests will go faster. See Partitioning: how to split data among multiple Redis instances and Redis Cluster data sharding .","title":"Sharding"},{"location":"deploy-topology/rtfm-Redis/#redis#sentinel#vs#redis#cluster","text":"","title":"Redis Sentinel vs Redis Cluster"},{"location":"deploy-topology/rtfm-Redis/#redis#sentinel","text":"Was added to Redis v.2.4 and basically is a monitoring service for master and slaves. Also, can send notifications, automatically switch masters and slaves roles if a master is down and so on. Might have a sense to be used for a bare master-slave replication (see below) without full clustering. It works as a dedicated daemon using a sentinel binary or redis-server in the sentinel mode. Will perform nodes reconfiguration if the master went down \u2013 will choose a new master from the slaves left. Requires at least three Sentinel instances to have a quorum for a new master election and to decide if one of a Redis nodes goes down","title":"Redis Sentinel"},{"location":"deploy-topology/rtfm-Redis/#redis#cluster","text":"Was added to Redis v.3.0 and represents a full clustering solution for segmentation, replication, and its nodes management. Will perform data synchronization, replication, manage nodes access persistence if some will go down. The Sentinel usage in the Redis Cluster case doesn\u2019t make sense as Cluster will do everything itself. See Redis Sentinel & Redis Cluster \u2013 what? and Redis Sentinel Documentation .","title":"Redis Cluster"},{"location":"deploy-topology/rtfm-Redis/#redis#topology","text":"","title":"Redis topology"},{"location":"deploy-topology/rtfm-Redis/#one#redis#instance","text":"The simplest and mor classical case. Simple in running and configuration. Limited by a host\u2019s resources \u2013 its CPU and memory. In case of such Redis instance will go down \u2013 all dependent services will be broken as well as there is no any availability or fault tolerance mechanisms.","title":"One Redis instance"},{"location":"deploy-topology/rtfm-Redis/#master-slave#replication","text":"One master which has multitype slaves attached. Data will be updated on this master and then the master will push those changes to its replicas. Slaves can talk to the master only and can\u2019t communicate with other slaves, but still can have their own slaves Slaves are read-only nodes \u2013 no data changes can be performed there unless this wasn\u2019t configured explicitly (see the second part of this post). In case of any node will go down \u2013 all data still will be available for clients as data is replicated over all nodes. Simple in configuration but the write operations are limited by the master\u2019s resources. In case of the master will go down \u2013 you\u2019ll have to manually reconfigure slaves and change slave to master roles for one on them. Also, clients need to know which they have to use for writes operations.","title":"Master-slave replication"},{"location":"deploy-topology/rtfm-Redis/#redis#sentinel_1","text":"Already described above but a few more words here. Similarly to the Redis Replication \u2013 Sentinel has one Master instance which has a priority when deciding on a Redis master\u2019s elections. I.e. in case of one Redis Master and two slaves and if Sentinel Master works on the same host where Redis Master is running and this host will go down \u2013 Sentinel will choose Sentinel\u2019s new Master instance and those two Sentinels instances need to decide which Redis slave will have to become a new Redis Master. During this \u2013 a Sentinel\u2019s Master will have more weight in such an election. Keep in mind that not every Redis client able to work with Sentinel, all client can be found here>>> .","title":"Redis Sentinel"},{"location":"deploy-topology/rtfm-Redis/#redis#cluster_1","text":"And the most powerful solution \u2013 the Redis Cluster. Has a few master instances and each can have one more \u2013 up to 1000 \u2013 slaves. Will take care of data sharding, replication, synchronization, and failover operations. Must have at least 6 Redis nodes \u2013 3 for masters and three for slaves. Can redirect clients requests to a necessary master or slave host \u2013 but the client must have an ability to work with Redis Cluster.","title":"Redis Cluster"}]}